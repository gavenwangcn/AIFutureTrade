# This must be at the very top of the file, before any other imports
import eventlet
eventlet.monkey_patch()

"""
Flask application for AI Futures Trading System
"""
from flask import Flask, render_template, request, jsonify
from flask_cors import CORS
from flask_socketio import SocketIO, emit
import os
import time
import threading
import json
from datetime import datetime
from trading_engine import TradingEngine
from market_data import MarketDataFetcher
from ai_trader import AITrader
from database_basic import Database
from version import __version__
from prompt_defaults import DEFAULT_BUY_CONSTRAINTS, DEFAULT_SELL_CONSTRAINTS

import config as app_config
import logging
import sys

# ============ Application Initialization ============

app = Flask(__name__)
CORS(app)
# ä½¿ç”¨eventletä½œä¸ºå¼‚æ­¥æ¨¡å¼ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
# async_mode='eventlet' æä¾›æ›´å¥½çš„å¹¶å‘æ€§èƒ½
socketio = SocketIO(
    app, 
    cors_allowed_origins="*",
    async_mode='eventlet',  # ä½¿ç”¨eventletå¼‚æ­¥æ¨¡å¼
    logger=False,  # ç¦ç”¨SocketIOæ—¥å¿—ä»¥å‡å°‘å¼€é”€
    engineio_logger=False,  # ç¦ç”¨EngineIOæ—¥å¿—
    ping_timeout=60,  # WebSocket pingè¶…æ—¶æ—¶é—´
    ping_interval=25,  # WebSocket pingé—´éš”
    max_http_buffer_size=1e6,  # æœ€å¤§HTTPç¼“å†²åŒºå¤§å°
    allow_upgrades=True,  # å…è®¸åè®®å‡çº§
    transports=['websocket', 'polling']  # æ”¯æŒçš„ä¼ è¾“æ–¹å¼
)

# ============ Logging Configuration ============

def get_log_level():
    """ä»é…ç½®è·å–æ—¥å¿—çº§åˆ«ï¼Œé»˜è®¤ä¸º INFO"""
    log_level_str = getattr(app_config, 'LOG_LEVEL', 'INFO').upper()
    log_level_map = {
        'DEBUG': logging.DEBUG,
        'INFO': logging.INFO,
        'WARNING': logging.WARNING,
        'ERROR': logging.ERROR,
        'CRITICAL': logging.CRITICAL
    }
    return log_level_map.get(log_level_str, logging.INFO)

log_format = getattr(app_config, 'LOG_FORMAT', '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
log_date_format = getattr(app_config, 'LOG_DATE_FORMAT', '%Y-%m-%d %H:%M:%S')

logging.basicConfig(
    level=get_log_level(),
    format=log_format,
    datefmt=log_date_format,
    handlers=[logging.StreamHandler(sys.stdout)]
)

logging.getLogger('werkzeug').setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

# ============ Global Configuration ============

DEFAULT_DB_PATH = 'trading_bot.db'
env_db_path = os.getenv('DATABASE_PATH')
config_db_path = getattr(app_config, 'DATABASE_PATH', None)
db_path = env_db_path or config_db_path or DEFAULT_DB_PATH

db = Database(db_path)

# Initialize database tables immediately when the application starts
# This ensures tables are created even when running with gunicorn or other WSGI servers
with app.app_context():
    db.init_db()
    logger.info("Database tables initialized")

# åº”ç”¨å¯åŠ¨æ—¶ç«‹å³å¯åŠ¨ClickHouseæ¶¨è·Œå¹…æ¦œåŒæ­¥æœåŠ¡
# è¿™ç¡®ä¿æ— è®ºé€šè¿‡ä»€ä¹ˆæ–¹å¼å¯åŠ¨ï¼ˆç›´æ¥è¿è¡Œã€gunicornç­‰ï¼‰ï¼Œéƒ½ä¼šè‡ªåŠ¨å¯åŠ¨æœåŠ¡
def _init_background_services():
    """åˆå§‹åŒ–åå°æœåŠ¡ï¼ˆåœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ï¼‰"""
    global clickhouse_leaderboard_running
    
    logger.info("ğŸš€ åˆå§‹åŒ–åå°æœåŠ¡...")
    
    # å¯åŠ¨ClickHouseæ¶¨è·Œå¹…æ¦œåŒæ­¥çº¿ç¨‹ï¼ˆé»˜è®¤è¿è¡ŒçŠ¶æ€ï¼‰
    logger.info("ğŸ“Š å¯åŠ¨ClickHouseæ¶¨è·Œå¹…æ¦œåŒæ­¥æœåŠ¡...")
    start_clickhouse_leaderboard_sync()
    logger.info("âœ… ClickHouseæ¶¨è·Œå¹…æ¦œåŒæ­¥æœåŠ¡å·²å¯åŠ¨ï¼ˆé»˜è®¤è¿è¡ŒçŠ¶æ€ï¼‰")
    
    logger.info("âœ… åå°æœåŠ¡åˆå§‹åŒ–å®Œæˆ")

market_fetcher = MarketDataFetcher(db)
trading_engines = {}
auto_trading = getattr(app_config, 'AUTO_TRADING', True)
TRADE_FEE_RATE = getattr(app_config, 'TRADE_FEE_RATE', 0.001)
LEADERBOARD_REFRESH_INTERVAL = getattr(app_config, 'FUTURES_LEADERBOARD_REFRESH', 10)

leaderboard_thread = None
leaderboard_stop_event = threading.Event()

# ClickHouse leaderboard sync
clickhouse_leaderboard_thread = None
clickhouse_leaderboard_stop_event = threading.Event()
clickhouse_leaderboard_running = True  # é»˜è®¤çŠ¶æ€ä¸ºè¿è¡ŒçŠ¶æ€
# æ·»åŠ çº¿ç¨‹é”ä»¥é˜²æ­¢å¹¶å‘æ‰§è¡Œ
clickhouse_leaderboard_lock = threading.Lock()
# çº¿ç¨‹ç›‘æ§æ ‡å¿—ï¼Œç”¨äºè‡ªåŠ¨é‡å¯
clickhouse_leaderboard_monitor_thread = None
clickhouse_leaderboard_monitor_stop_event = threading.Event()

# ============ Helper Functions ============

def init_trading_engine_for_model(model_id: int):
    """Initialize trading engine for a model if possible."""
    model = db.get_model(model_id)
    if not model:
        return None, 'Model not found'

    provider = db.get_provider(model['provider_id'])
    if not provider:
        return None, 'Provider not found'

    trading_engines[model_id] = TradingEngine(
        model_id=model_id,
        db=db,
        market_fetcher=market_fetcher,
        ai_trader=AITrader(
            provider_type=provider.get('provider_type', 'openai'),
            api_key=provider['api_key'],
            api_url=provider['api_url'],
            model_name=model['model_name']
        ),
        trade_fee_rate=TRADE_FEE_RATE
    )
    return trading_engines[model_id], None

def get_tracked_symbols():
    """Get list of tracked future symbols"""
    symbols = db.get_future_symbols()
    if not symbols:
        logger.warning('No futures configured. Please add futures via /api/futures.')
    return symbols

def get_trading_interval_seconds() -> int:
    """Read trading frequency from settings (minutes) and return seconds."""
    default_interval_seconds = getattr(app_config, 'TRADING_INTERVAL', 3600)
    default_minutes = max(1, int(default_interval_seconds / 60))
    try:
        settings = db.get_settings()
        minutes = int(settings.get('trading_frequency_minutes', default_minutes))
    except Exception as e:
        logger.warning(f"Unable to load trading frequency setting: {e}")
        minutes = default_minutes

    minutes = max(1, min(1440, minutes))
    return minutes * 60

def init_trading_engines():
    """Initialize trading engines for all models"""
    try:
        models = db.get_all_models()

        if not models:
            logger.warning("No trading models found")
            return

        logger.info(f"\nINIT: Initializing trading engines...")
        for model in models:
            model_id = model['id']
            model_name = model['name']

            try:
                provider = db.get_provider(model['provider_id'])
                if not provider:
                    logger.warning(f"  Model {model_id} ({model_name}): Provider not found")
                    continue

                trading_engines[model_id] = TradingEngine(
                    model_id=model_id,
                    db=db,
                    market_fetcher=market_fetcher,
                    ai_trader=AITrader(
                        provider_type=provider.get('provider_type', 'openai'),
                        api_key=provider['api_key'],
                        api_url=provider['api_url'],
                        model_name=model['model_name']
                    ),
                    trade_fee_rate=TRADE_FEE_RATE
                )
                logger.info(f"  OK: Model {model_id} ({model_name})")
            except Exception as e:
                logger.error(f"  Model {model_id} ({model_name}): {e}")
                continue

        logger.info(f"Initialized {len(trading_engines)} engine(s)\n")

    except Exception as e:
        logger.error(f"Init engines failed: {e}\n")

# ============ Background Tasks ============

def _leaderboard_loop():
    """
    åå°å¾ªç¯ä»»åŠ¡ï¼šå®šæœŸåŒæ­¥æ¶¨è·Œå¹…æ¦œæ•°æ®å¹¶æ¨é€åˆ°å‰ç«¯
    
    æµç¨‹ï¼š
    1. å¯åŠ¨å¾ªç¯ï¼Œè®°å½•å¯åŠ¨ä¿¡æ¯
    2. å®šæœŸè°ƒç”¨ sync_leaderboard åŒæ­¥æ•°æ®
    3. å¦‚æœåŒæ­¥æˆåŠŸï¼Œé€šè¿‡ WebSocket æ¨é€åˆ°å‰ç«¯
    4. ç­‰å¾…æŒ‡å®šé—´éš”åç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯
    5. æ”¶åˆ°åœæ­¢ä¿¡å·æ—¶é€€å‡ºå¾ªç¯
    """
    thread_id = threading.current_thread().ident
    logger.info(f"[Leaderboard Worker-{thread_id}] ========== æ¶¨è·Œå¹…æ¦œåŒæ­¥å¾ªç¯å¯åŠ¨ ==========")
    logger.info(f"[Leaderboard Worker-{thread_id}] åˆ·æ–°é—´éš”: {LEADERBOARD_REFRESH_INTERVAL} ç§’")
    
    wait_seconds = max(5, LEADERBOARD_REFRESH_INTERVAL)
    cycle_count = 0
    
    while not leaderboard_stop_event.is_set():
        cycle_count += 1
        cycle_start_time = datetime.now()
        
        logger.info(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] ========== å¼€å§‹åŒæ­¥æ¶¨è·Œå¹…æ¦œ ==========")
        logger.info(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] åŒæ­¥æ—¶é—´: {cycle_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # è°ƒç”¨åŒæ­¥æ–¹æ³•ï¼ˆä¸å¼ºåˆ¶åˆ·æ–°ï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶ï¼‰
            logger.info(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] [æ­¥éª¤1] è°ƒç”¨ sync_leaderboard åŒæ­¥æ•°æ®...")
            sync_start_time = datetime.now()
            
            data = market_fetcher.sync_leaderboard(force=False)
            
            sync_duration = (datetime.now() - sync_start_time).total_seconds()
            logger.info(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] [æ­¥éª¤1] æ•°æ®åŒæ­¥å®Œæˆ, è€—æ—¶: {sync_duration:.2f} ç§’")
            
            # æ£€æŸ¥åŒæ­¥ç»“æœ
            if data:
                gainers = data.get('gainers', [])
                losers = data.get('losers', [])
                gainers_count = len(gainers) if gainers else 0
                losers_count = len(losers) if losers else 0
                
                logger.info(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] [æ­¥éª¤2] åŒæ­¥æ•°æ®ç»Ÿè®¡: "
                           f"æ¶¨å¹…æ¦œ={gainers_count} æ¡, è·Œå¹…æ¦œ={losers_count} æ¡")
                
                # è®°å½•æ¶¨å¹…æ¦œå‰3åï¼ˆå¦‚æœæœ‰ï¼‰
                if gainers_count > 0:
                    top_gainers = gainers[:3]
                    for idx, entry in enumerate(top_gainers):
                        logger.info(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] [æ­¥éª¤2.1] æ¶¨å¹…æ¦œ #{idx+1}: "
                                   f"{entry.get('symbol', 'N/A')} "
                                   f"ä»·æ ¼=${entry.get('price', 0):.4f} "
                                   f"æ¶¨è·Œå¹…={entry.get('change_percent', 0):.2f}% "
                                   f"æˆäº¤é‡=${entry.get('quote_volume', 0):.2f}")
                
                # è®°å½•è·Œå¹…æ¦œå‰3åï¼ˆå¦‚æœæœ‰ï¼‰
                if losers_count > 0:
                    top_losers = losers[:3]
                    for idx, entry in enumerate(top_losers):
                        logger.info(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] [æ­¥éª¤2.2] è·Œå¹…æ¦œ #{idx+1}: "
                                   f"{entry.get('symbol', 'N/A')} "
                                   f"ä»·æ ¼=${entry.get('price', 0):.4f} "
                                   f"æ¶¨è·Œå¹…={entry.get('change_percent', 0):.2f}% "
                                   f"æˆäº¤é‡=${entry.get('quote_volume', 0):.2f}")
                
                # é€šè¿‡ WebSocket æ¨é€åˆ°å‰ç«¯
                logger.info(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] [æ­¥éª¤3] é€šè¿‡ WebSocket æ¨é€æ•°æ®åˆ°å‰ç«¯...")
                emit_start_time = datetime.now()
                
                socketio.emit('leaderboard:update', data)
                
                emit_duration = (datetime.now() - emit_start_time).total_seconds()
                logger.info(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] [æ­¥éª¤3] WebSocket æ¨é€å®Œæˆ, è€—æ—¶: {emit_duration:.3f} ç§’")
                
            else:
                logger.warning(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] [æ­¥éª¤2] åŒæ­¥è¿”å›ç©ºæ•°æ®ï¼Œè·³è¿‡æ¨é€")
                
        except Exception as exc:
            cycle_duration = (datetime.now() - cycle_start_time).total_seconds()
            logger.error(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] ========== æ¶¨è·Œå¹…æ¦œåŒæ­¥å¤±è´¥ ==========")
            logger.error(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] é”™è¯¯ä¿¡æ¯: {exc}")
            logger.error(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] å¤±è´¥è€—æ—¶: {cycle_duration:.2f} ç§’")
            import traceback
            logger.error(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
        
        # è®¡ç®—æœ¬æ¬¡å¾ªç¯æ€»è€—æ—¶
        cycle_duration = (datetime.now() - cycle_start_time).total_seconds()
        logger.info(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] ========== åŒæ­¥å¾ªç¯å®Œæˆ ==========")
        logger.info(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] æœ¬æ¬¡å¾ªç¯è€—æ—¶: {cycle_duration:.2f} ç§’")
        logger.info(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] ç­‰å¾… {wait_seconds} ç§’åå¼€å§‹ä¸‹ä¸€æ¬¡å¾ªç¯...")
        
        # ç­‰å¾…æŒ‡å®šé—´éš”ï¼ˆå¯è¢«åœæ­¢äº‹ä»¶ä¸­æ–­ï¼‰
        leaderboard_stop_event.wait(wait_seconds)
    
    logger.info(f"[Leaderboard Worker-{thread_id}] ========== æ¶¨è·Œå¹…æ¦œåŒæ­¥å¾ªç¯åœæ­¢ ==========")
    logger.info(f"[Leaderboard Worker-{thread_id}] æ€»å¾ªç¯æ¬¡æ•°: {cycle_count}")

def _clickhouse_leaderboard_loop():
    """
    åå°å¾ªç¯ä»»åŠ¡ï¼šå®šæœŸä» ClickHouse 24_market_tickers è¡¨åŒæ­¥æ¶¨è·Œå¹…æ¦œæ•°æ®åˆ° futures_leaderboard è¡¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - å®šæœŸä»24_market_tickersè¡¨è·å–æœ€æ–°çš„å¸‚åœºæ•°æ®
    - è®¡ç®—æ¯ä¸ªåˆçº¦çš„æ¶¨è·Œå¹…
    - ç­›é€‰å‡ºæ¶¨å¹…å‰Nåå’Œè·Œå¹…å‰Nå
    - å°†ç»“æœä¿å­˜åˆ°futures_leaderboardè¡¨ä¸­
    - æ”¯æŒé…ç½®åŒæ­¥é—´éš”ã€æ—¶é—´çª—å£å’Œå‰Nåæ•°é‡
    
    æ‰§è¡Œæµç¨‹ï¼š
    1. åˆå§‹åŒ–ClickHouseè¿æ¥
    2. è·å–é…ç½®å‚æ•°
    3. è¿›å…¥ä¸»å¾ªç¯ï¼š
       a. æŸ¥è¯¢æœ€è¿‘æ—¶é—´çª—å£å†…çš„å¸‚åœºæ•°æ®
       b. è®¡ç®—æ¶¨è·Œå¹…å¹¶æ’åº
       c. ç­›é€‰å‰Nåæ¶¨å¹…å’Œè·Œå¹…
       d. åŸå­æ›´æ–°futures_leaderboardè¡¨
       e. ç­‰å¾…æŒ‡å®šé—´éš”åé‡å¤å¾ªç¯
    4. æ”¶åˆ°åœæ­¢ä¿¡å·æ—¶é€€å‡ºå¾ªç¯
    
    é…ç½®å‚æ•°ï¼š
    - CLICKHOUSE_LEADERBOARD_SYNC_INTERVAL: åŒæ­¥é—´éš”ï¼ˆç§’ï¼‰
    - CLICKHOUSE_LEADERBOARD_TIME_WINDOW: æŸ¥è¯¢æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
    - CLICKHOUSE_LEADERBOARD_TOP_N: æ¶¨è·Œå¹…å‰Nåæ•°é‡
    
    æ³¨æ„ï¼š
    - æ­¤å‡½æ•°åŒ…å«å¼‚å¸¸å¤„ç†ï¼Œç¡®ä¿å³ä½¿å‘ç”Ÿå¼‚å¸¸ä¹Ÿä¸ä¼šé€€å‡ºå¾ªç¯
    - åªæœ‰åœ¨æ”¶åˆ°æ˜ç¡®çš„åœæ­¢ä¿¡å·æ—¶æ‰ä¼šé€€å‡º
    """
    # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯å¯¼å…¥é—®é¢˜
    from database_clickhouse import ClickHouseDatabase
    
    # è·å–å½“å‰çº¿ç¨‹IDï¼Œç”¨äºæ—¥å¿—æ ‡è¯†
    thread_id = threading.current_thread().ident
    logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] ========== ClickHouse æ¶¨å¹…æ¦œåŒæ­¥å¾ªç¯å¯åŠ¨ ==========")
    
    # è·å–é…ç½®å‚æ•°ï¼Œå¸¦é»˜è®¤å€¼
    sync_interval = getattr(app_config, 'CLICKHOUSE_LEADERBOARD_SYNC_INTERVAL', 2)
    time_window = getattr(app_config, 'CLICKHOUSE_LEADERBOARD_TIME_WINDOW', 5)  # å·²åºŸå¼ƒï¼Œä¿ç•™ä»¥å…¼å®¹
    top_n = getattr(app_config, 'CLICKHOUSE_LEADERBOARD_TOP_N', 10)
    
    # è®°å½•é…ç½®ä¿¡æ¯
    logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] é…ç½®ä¿¡æ¯:")
    logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}]   åŒæ­¥é—´éš”: {sync_interval} ç§’")
    logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}]   æŸ¥è¯¢èŒƒå›´: æ‰€æœ‰æ•°æ®ï¼ˆå·²ç§»é™¤æ—¶é—´çª—å£é™åˆ¶ï¼‰")
    logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}]   å‰Nåæ•°é‡: {top_n}")
    
    # ç¡®ä¿ç­‰å¾…æ—¶é—´è‡³å°‘ä¸º1ç§’
    wait_seconds = max(1, sync_interval)
    cycle_count = 0
    db = None
    
    # åœ¨å¾ªç¯å¤–åˆ›å»ºClickHouseDatabaseå®ä¾‹ï¼Œé¿å…é¢‘ç¹åˆ›å»ºå’Œé”€æ¯è¿æ¥
    logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] æ­£åœ¨åˆå§‹åŒ–ClickHouseè¿æ¥...")
    try:
        db = ClickHouseDatabase(auto_init_tables=True)
        logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] âœ… ClickHouseè¿æ¥åˆå§‹åŒ–æˆåŠŸ")
    except Exception as exc:
        logger.error(f"[ClickHouse Leaderboard Worker-{thread_id}] âŒ åˆå§‹åŒ–ClickHouseè¿æ¥å¤±è´¥: {exc}")
        logger.error(f"[ClickHouse Leaderboard Worker-{thread_id}] âŒ å°†åœ¨å¾ªç¯ä¸­é‡è¯•åˆå§‹åŒ–")
        # ä¸ç›´æ¥è¿”å›ï¼Œè€Œæ˜¯åœ¨å¾ªç¯ä¸­é‡è¯•
    
    # ç«‹å³æ‰§è¡Œç¬¬ä¸€æ¬¡åŒæ­¥ï¼ˆå¯åŠ¨æ—¶ç«‹å³åˆ·æ–°æ•°æ®ï¼‰
    logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] ğŸš€ å¯åŠ¨æ—¶ç«‹å³æ‰§è¡Œç¬¬ä¸€æ¬¡åŒæ­¥...")
    cycle_count += 1
    cycle_start_time = datetime.now()
    
    logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] ğŸš€ å¼€å§‹åŒæ­¥...")
    logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] åŒæ­¥æ—¶é—´: {cycle_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # å¦‚æœæ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–ï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–
        if db is None:
            logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] é‡æ–°åˆå§‹åŒ–ClickHouseè¿æ¥...")
            db = ClickHouseDatabase(auto_init_tables=True)
            logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] âœ… ClickHouseè¿æ¥åˆå§‹åŒ–æˆåŠŸ")
        
        # æ‰§è¡ŒåŒæ­¥é€»è¾‘
        logger.debug(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] è°ƒç”¨db.sync_leaderboard()")
        db.sync_leaderboard(
            time_window_seconds=time_window,
            top_n=top_n
        )
        
        # è®¡ç®—åŒæ­¥è€—æ—¶
        cycle_duration = (datetime.now() - cycle_start_time).total_seconds()
        logger.info(
            f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] âœ… åŒæ­¥å®Œæˆ, è€—æ—¶: {cycle_duration:.3f} ç§’"
        )
        logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] âœ… å¯åŠ¨æ—¶é¦–æ¬¡åŒæ­¥æˆåŠŸ")
    except Exception as exc:
        # å¤„ç†åŒæ­¥å¤±è´¥çš„æƒ…å†µï¼Œä½†ä¸é€€å‡ºå¾ªç¯
        cycle_duration = (datetime.now() - cycle_start_time).total_seconds()
        logger.error(
            f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] âŒ å¯åŠ¨æ—¶é¦–æ¬¡åŒæ­¥å¤±è´¥: {exc}, è€—æ—¶: {cycle_duration:.3f} ç§’"
        )
        # è®°å½•è¯¦ç»†çš„é”™è¯¯å †æ ˆ
        import traceback
        error_stack = traceback.format_exc()
        logger.error(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] âŒ é”™è¯¯å †æ ˆ: {error_stack}")
        logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] âš ï¸  å°†ç»§ç»­é‡è¯•ï¼Œä¸ä¼šé€€å‡º")
    
    # ä¸»å¾ªç¯ï¼šå®šæœŸæ‰§è¡ŒåŒæ­¥ä»»åŠ¡ï¼ˆæ°¸ä¸é€€å‡ºï¼Œé™¤éæ”¶åˆ°åœæ­¢ä¿¡å·ï¼‰
    logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] è¿›å…¥ä¸»åŒæ­¥å¾ªç¯ï¼ˆå°†æŒç»­è¿è¡Œï¼Œä¸ä¼šè‡ªåŠ¨æš‚åœï¼‰")
    while not clickhouse_leaderboard_stop_event.is_set():
        cycle_count += 1
        cycle_start_time = datetime.now()
        
        logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] ğŸš€ å¼€å§‹åŒæ­¥...")
        logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] åŒæ­¥æ—¶é—´: {cycle_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # å¦‚æœæ•°æ®åº“è¿æ¥ä¸¢å¤±ï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–
            if db is None:
                logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] é‡æ–°åˆå§‹åŒ–ClickHouseè¿æ¥...")
                db = ClickHouseDatabase(auto_init_tables=True)
                logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] âœ… ClickHouseè¿æ¥åˆå§‹åŒ–æˆåŠŸ")
            
            # æ‰§è¡ŒåŒæ­¥é€»è¾‘
            logger.debug(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] è°ƒç”¨db.sync_leaderboard()")
            db.sync_leaderboard(
                time_window_seconds=time_window,
                top_n=top_n
            )
            
            # è®¡ç®—åŒæ­¥è€—æ—¶
            cycle_duration = (datetime.now() - cycle_start_time).total_seconds()
            logger.info(
                f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] âœ… åŒæ­¥å®Œæˆ, è€—æ—¶: {cycle_duration:.3f} ç§’"
            )
            logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] ğŸ’¤ ç­‰å¾… {wait_seconds} ç§’åå¼€å§‹ä¸‹ä¸€æ¬¡åŒæ­¥")
            
        except Exception as exc:
            # å¤„ç†åŒæ­¥å¤±è´¥çš„æƒ…å†µï¼Œä½†ä¸é€€å‡ºå¾ªç¯ï¼Œç»§ç»­é‡è¯•
            cycle_duration = (datetime.now() - cycle_start_time).total_seconds()
            logger.error(
                f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] âŒ åŒæ­¥å¤±è´¥: {exc}, è€—æ—¶: {cycle_duration:.3f} ç§’"
            )
            # è®°å½•è¯¦ç»†çš„é”™è¯¯å †æ ˆ
            import traceback
            error_stack = traceback.format_exc()
            logger.error(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] âŒ é”™è¯¯å †æ ˆ: {error_stack}")
            logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] âš ï¸  å°†ç»§ç»­é‡è¯•ï¼Œä¸ä¼šé€€å‡º")
            logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] ğŸ’¤ ç­‰å¾… {wait_seconds} ç§’åé‡è¯•")
            # æ ‡è®°æ•°æ®åº“è¿æ¥å¯èƒ½å·²å¤±æ•ˆï¼Œä¸‹æ¬¡å¾ªç¯æ—¶é‡æ–°åˆå§‹åŒ–
            db = None
        
        # ç­‰å¾…æŒ‡å®šé—´éš”åç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯
        # ä½¿ç”¨wait()æ–¹æ³•å¯ä»¥è¢«åœæ­¢äº‹ä»¶ä¸­æ–­
        # å¦‚æœç­‰å¾…æœŸé—´æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œå¾ªç¯ä¼šé€€å‡º
        if clickhouse_leaderboard_stop_event.wait(wait_seconds):
            # å¦‚æœwaitè¿”å›Trueï¼Œè¯´æ˜åœ¨ç­‰å¾…æœŸé—´æ”¶åˆ°äº†åœæ­¢ä¿¡å·
            break
    
    # å¾ªç¯ç»“æŸï¼Œè®°å½•åœæ­¢ä¿¡æ¯
    logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] ========== ClickHouse æ¶¨å¹…æ¦œåŒæ­¥å¾ªç¯åœæ­¢ ==========")
    logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] ğŸ“Š æ€»å¾ªç¯æ¬¡æ•°: {cycle_count}")
    logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] ğŸ‘‹ æ¶¨è·Œå¹…æ¦œåŒæ­¥çº¿ç¨‹å·²åœæ­¢")
    
    # æ›´æ–°è¿è¡ŒçŠ¶æ€
    global clickhouse_leaderboard_running
    with clickhouse_leaderboard_lock:
        clickhouse_leaderboard_running = False


def _clickhouse_leaderboard_monitor():
    """
    ç›‘æ§çº¿ç¨‹ï¼šç›‘æ§ClickHouseæ¶¨è·Œå¹…æ¦œåŒæ­¥çº¿ç¨‹ï¼Œå¦‚æœçº¿ç¨‹æ„å¤–é€€å‡ºåˆ™è‡ªåŠ¨é‡å¯
    
    æ­¤ç›‘æ§çº¿ç¨‹ç¡®ä¿åŒæ­¥æœåŠ¡æŒç»­è¿è¡Œï¼Œä¸ä¼šå› ä¸ºå¼‚å¸¸è€Œåœæ­¢
    """
    global clickhouse_leaderboard_thread, clickhouse_leaderboard_running
    
    logger.info("[ClickHouse Leaderboard Monitor] ğŸ›¡ï¸  ç›‘æ§çº¿ç¨‹å¯åŠ¨")
    
    while not clickhouse_leaderboard_monitor_stop_event.is_set():
        # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡çº¿ç¨‹çŠ¶æ€
        clickhouse_leaderboard_monitor_stop_event.wait(10)
        
        if clickhouse_leaderboard_monitor_stop_event.is_set():
            break
        
        with clickhouse_leaderboard_lock:
            # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
            if clickhouse_leaderboard_running:
                if clickhouse_leaderboard_thread and clickhouse_leaderboard_thread.is_alive():
                    # çº¿ç¨‹æ­£å¸¸è¿è¡Œï¼Œç»§ç»­ç›‘æ§
                    continue
                else:
                    # çº¿ç¨‹æ„å¤–é€€å‡ºï¼Œéœ€è¦é‡å¯
                    logger.warning("[ClickHouse Leaderboard Monitor] âš ï¸  æ£€æµ‹åˆ°åŒæ­¥çº¿ç¨‹æ„å¤–é€€å‡ºï¼Œå‡†å¤‡è‡ªåŠ¨é‡å¯...")
                    clickhouse_leaderboard_running = False
            
            # å¦‚æœè¿è¡ŒçŠ¶æ€ä¸ºFalseï¼Œä½†ç”¨æˆ·æ²¡æœ‰æ˜ç¡®åœæ­¢ï¼Œåˆ™è‡ªåŠ¨é‡å¯
            if not clickhouse_leaderboard_running and not clickhouse_leaderboard_stop_event.is_set():
                logger.info("[ClickHouse Leaderboard Monitor] ğŸ”„ è‡ªåŠ¨é‡å¯åŒæ­¥çº¿ç¨‹...")
                clickhouse_leaderboard_stop_event.clear()
                clickhouse_leaderboard_running = True
                
                clickhouse_leaderboard_thread = threading.Thread(
                    target=_clickhouse_leaderboard_loop,
                    daemon=True,
                    name="ClickHouseLeaderboardSync"
                )
                clickhouse_leaderboard_thread.start()
                logger.info("[ClickHouse Leaderboard Monitor] âœ… åŒæ­¥çº¿ç¨‹å·²è‡ªåŠ¨é‡å¯")
    
    logger.info("[ClickHouse Leaderboard Monitor] ğŸ›¡ï¸  ç›‘æ§çº¿ç¨‹åœæ­¢")


def start_clickhouse_leaderboard_sync():
    """
    å¯åŠ¨ ClickHouse æ¶¨å¹…æ¦œåŒæ­¥çº¿ç¨‹
    
    åŠŸèƒ½ï¼š
    - æ£€æŸ¥åŒæ­¥çº¿ç¨‹æ˜¯å¦å·²åœ¨è¿è¡Œ
    - åˆå§‹åŒ–åœæ­¢äº‹ä»¶
    - åˆ›å»ºå¹¶å¯åŠ¨åŒæ­¥çº¿ç¨‹
    - è®¾ç½®çº¿ç¨‹ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼Œç¡®ä¿ä¸»ç¨‹åºé€€å‡ºæ—¶è‡ªåŠ¨ç»ˆæ­¢
    - å¯åŠ¨ç›‘æ§çº¿ç¨‹ï¼Œç¡®ä¿çº¿ç¨‹æ„å¤–é€€å‡ºæ—¶è‡ªåŠ¨é‡å¯
    
    æ³¨æ„ï¼š
    - è¯¥å‡½æ•°æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå¯ä»¥å¤šæ¬¡è°ƒç”¨
    - å¤šæ¬¡è°ƒç”¨æ—¶ï¼Œåªæœ‰ç¬¬ä¸€æ¬¡ä¼šçœŸæ­£å¯åŠ¨çº¿ç¨‹
    - é»˜è®¤çŠ¶æ€ä¸ºè¿è¡ŒçŠ¶æ€ï¼Œåº”ç”¨å¯åŠ¨æ—¶è‡ªåŠ¨æ‰§è¡Œ
    """
    global clickhouse_leaderboard_thread, clickhouse_leaderboard_running
    global clickhouse_leaderboard_monitor_thread, clickhouse_leaderboard_monitor_stop_event
    
    # ä½¿ç”¨é”é˜²æ­¢å¹¶å‘æ‰§è¡Œ
    with clickhouse_leaderboard_lock:
        # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦å·²åœ¨è¿è¡Œ
        if clickhouse_leaderboard_thread and clickhouse_leaderboard_thread.is_alive():
            logger.warning("[ClickHouse Leaderboard] âš ï¸  åŒæ­¥çº¿ç¨‹å·²åœ¨è¿è¡Œï¼Œæ— éœ€é‡å¤å¯åŠ¨")
            return
        
        logger.info("[ClickHouse Leaderboard] ğŸš€ å‡†å¤‡å¯åŠ¨æ¶¨è·Œå¹…æ¦œåŒæ­¥çº¿ç¨‹...")
        
        # é‡ç½®åœæ­¢äº‹ä»¶å’Œè¿è¡ŒçŠ¶æ€
        clickhouse_leaderboard_stop_event.clear()
        clickhouse_leaderboard_running = True
        
        # åˆ›å»ºåŒæ­¥çº¿ç¨‹
        clickhouse_leaderboard_thread = threading.Thread(
            target=_clickhouse_leaderboard_loop,
            daemon=True,  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹
            name="ClickHouseLeaderboardSync"  # è®¾ç½®çº¿ç¨‹åç§°ï¼Œä¾¿äºè°ƒè¯•
        )
        
        # å¯åŠ¨çº¿ç¨‹
        clickhouse_leaderboard_thread.start()
        
        # è®°å½•å¯åŠ¨ä¿¡æ¯
        logger.info(f"[ClickHouse Leaderboard] âœ… æ¶¨è·Œå¹…æ¦œåŒæ­¥çº¿ç¨‹å·²å¯åŠ¨")
        logger.info(f"[ClickHouse Leaderboard] ğŸ“‹ çº¿ç¨‹ID: {clickhouse_leaderboard_thread.ident}")
        logger.info(f"[ClickHouse Leaderboard] ğŸ“‹ çº¿ç¨‹åç§°: {clickhouse_leaderboard_thread.name}")
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰å¯åŠ¨ï¼‰
        if not clickhouse_leaderboard_monitor_thread or not clickhouse_leaderboard_monitor_thread.is_alive():
            clickhouse_leaderboard_monitor_stop_event.clear()
            clickhouse_leaderboard_monitor_thread = threading.Thread(
                target=_clickhouse_leaderboard_monitor,
                daemon=True,
                name="ClickHouseLeaderboardMonitor"
            )
            clickhouse_leaderboard_monitor_thread.start()
            logger.info("[ClickHouse Leaderboard] ğŸ›¡ï¸  ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")


def stop_clickhouse_leaderboard_sync():
    """
    åœæ­¢ ClickHouse æ¶¨å¹…æ¦œåŒæ­¥çº¿ç¨‹
    
    åŠŸèƒ½ï¼š
    - æ£€æŸ¥åŒæ­¥çº¿ç¨‹æ˜¯å¦åœ¨è¿è¡Œ
    - è®¾ç½®åœæ­¢äº‹ä»¶ï¼Œé€šçŸ¥çº¿ç¨‹é€€å‡º
    - ç­‰å¾…çº¿ç¨‹ç»ˆæ­¢ï¼ˆæœ€å¤š5ç§’ï¼‰
    - æ›´æ–°è¿è¡ŒçŠ¶æ€
    - åœæ­¢ç›‘æ§çº¿ç¨‹
    
    æ³¨æ„ï¼š
    - è¯¥å‡½æ•°æ˜¯çº¿ç¨‹å®‰å…¨çš„
    - è°ƒç”¨åä¼šç«‹å³è¿”å›ï¼Œä¸ä¼šé˜»å¡ç­‰å¾…çº¿ç¨‹ç»ˆæ­¢
    - åªæœ‰ç”¨æˆ·æ˜ç¡®è°ƒç”¨æ­¤å‡½æ•°æ—¶æ‰ä¼šåœæ­¢ï¼Œä¸ä¼šè‡ªåŠ¨æš‚åœ
    """
    global clickhouse_leaderboard_running
    global clickhouse_leaderboard_monitor_thread, clickhouse_leaderboard_monitor_stop_event
    
    # ä½¿ç”¨é”é˜²æ­¢å¹¶å‘æ‰§è¡Œ
    with clickhouse_leaderboard_lock:
        # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦åœ¨è¿è¡Œ
        if not clickhouse_leaderboard_running:
            logger.warning("[ClickHouse Leaderboard] âš ï¸  åŒæ­¥çº¿ç¨‹æœªè¿è¡Œï¼Œæ— éœ€åœæ­¢")
            return
        
        logger.info("[ClickHouse Leaderboard] ğŸ›‘ å‡†å¤‡åœæ­¢æ¶¨è·Œå¹…æ¦œåŒæ­¥çº¿ç¨‹ï¼ˆç”¨æˆ·æ‰‹åŠ¨åœæ­¢ï¼‰...")
        
        # è®¾ç½®åœæ­¢çŠ¶æ€å’Œåœæ­¢äº‹ä»¶
        clickhouse_leaderboard_running = False
        clickhouse_leaderboard_stop_event.set()
        
        # åœæ­¢ç›‘æ§çº¿ç¨‹
        if clickhouse_leaderboard_monitor_thread and clickhouse_leaderboard_monitor_thread.is_alive():
            logger.info("[ClickHouse Leaderboard] ğŸ›‘ åœæ­¢ç›‘æ§çº¿ç¨‹...")
            clickhouse_leaderboard_monitor_stop_event.set()
            clickhouse_leaderboard_monitor_thread.join(timeout=2)
        
        # ç­‰å¾…çº¿ç¨‹ç»ˆæ­¢ï¼Œæœ€å¤š5ç§’
        if clickhouse_leaderboard_thread and clickhouse_leaderboard_thread.is_alive():
            logger.info("[ClickHouse Leaderboard] â³ ç­‰å¾…çº¿ç¨‹ç»ˆæ­¢...")
            clickhouse_leaderboard_thread.join(timeout=5)
            
            if clickhouse_leaderboard_thread.is_alive():
                logger.warning("[ClickHouse Leaderboard] âš ï¸  çº¿ç¨‹æœªèƒ½åœ¨5ç§’å†…ç»ˆæ­¢ï¼Œå¯èƒ½å·²å¼ºåˆ¶ç»ˆæ­¢")
            else:
                logger.info("[ClickHouse Leaderboard] âœ… çº¿ç¨‹å·²æˆåŠŸç»ˆæ­¢")
        else:
            logger.info("[ClickHouse Leaderboard] âœ… çº¿ç¨‹å·²åœæ­¢ï¼ˆæœªè¿è¡Œï¼‰")
        
        logger.info("[ClickHouse Leaderboard] ğŸ“‹ æ¶¨è·Œå¹…æ¦œåŒæ­¥çº¿ç¨‹åœæ­¢å®Œæˆ")

def start_leaderboard_worker():
    """Start background worker for leaderboard updates"""
    global leaderboard_thread
    if leaderboard_thread and leaderboard_thread.is_alive():
        return
    leaderboard_stop_event.clear()
    leaderboard_thread = threading.Thread(target=_leaderboard_loop, daemon=True)
    leaderboard_thread.start()

def trading_loop():
    """Main trading loop for automatic trading"""
    logger.info("Trading loop started")

    while auto_trading:
        try:
            if not trading_engines:
                time.sleep(30)
                continue

            logger.info(f"\n{'='*60}")
            logger.info(f"CYCLE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"Active models: {len(trading_engines)}")
            logger.info(f"{'='*60}")

            for model_id, engine in list(trading_engines.items()):
                try:
                    if not db.is_model_auto_trading_enabled(model_id):
                        logger.info(f"SKIP: Model {model_id} auto trading paused")
                        continue

                    logger.info(f"\nEXEC: Model {model_id}")
                    result = engine.execute_trading_cycle()

                    if result.get('success'):
                        logger.info(f"OK: Model {model_id} completed")
                        if result.get('executions'):
                            for exec_result in result['executions']:
                                signal = exec_result.get('signal', 'unknown')
                                symbol = exec_result.get('future', exec_result.get('symbol', 'unknown'))
                                msg = exec_result.get('message', '')
                                if signal != 'hold':
                                    logger.info(f"  TRADE: {symbol}: {msg}")
                    else:
                        error = result.get('error', 'Unknown error')
                        logger.warning(f"Model {model_id} failed: {error}")

                except Exception as e:
                    logger.error(f"Model {model_id} exception: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
                    continue

            interval_seconds = get_trading_interval_seconds()
            interval_minutes = interval_seconds / 60
            logger.info(f"\n{'='*60}")
            logger.info(f"SLEEP: Waiting {interval_minutes:.1f} minute(s) for next cycle")
            logger.info(f"{'='*60}\n")

            time.sleep(interval_seconds)

        except Exception as e:
            logger.critical(f"\nTrading loop error: {e}")
            import traceback
            logger.critical(traceback.format_exc())
            logger.info("RETRY: Retrying in 60 seconds\n")
            time.sleep(60)

    logger.info("Trading loop stopped")

# ============ Page Routes ============

# åå°æœåŠ¡åˆå§‹åŒ–æ ‡å¿—ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œç¡®ä¿æ‰€æœ‰å‡½æ•°éƒ½å·²å®šä¹‰ï¼‰
_background_services_initialized = False

@app.before_request
def _ensure_background_services():
    """ç¡®ä¿åå°æœåŠ¡å·²å¯åŠ¨ï¼ˆåœ¨ç¬¬ä¸€æ¬¡è¯·æ±‚æ—¶è°ƒç”¨ï¼‰"""
    global _background_services_initialized
    if not _background_services_initialized:
        _init_background_services()
        _background_services_initialized = True

@app.route('/')
def index():
    """Main page route"""
    return render_template('index.html')

# ============ Provider API Endpoints ============

@app.route('/api/providers', methods=['GET'])
def get_providers():
    """Get all API providers"""
    providers = db.get_all_providers()
    return jsonify(providers)

@app.route('/api/providers', methods=['POST'])
def add_provider():
    """Add new API provider"""
    data = request.json
    try:
        provider_id = db.add_provider(
            name=data['name'],
            api_url=data['api_url'],
            api_key=data['api_key'],
            models=data.get('models', ''),
            provider_type=data.get('provider_type', 'openai')
        )
        return jsonify({'id': provider_id, 'message': 'Provider added successfully'})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/providers/<int:provider_id>', methods=['DELETE'])
def delete_provider(provider_id):
    """Delete API provider"""
    try:
        db.delete_provider(provider_id)
        return jsonify({'message': 'Provider deleted successfully'})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/providers/models', methods=['POST'])
def fetch_provider_models():
    """Fetch available models from provider's API"""
    data = request.json
    api_url = data.get('api_url')
    api_key = data.get('api_key')

    if not api_url or not api_key:
        return jsonify({'error': 'API URL and key are required'}), 400

    try:
        models = []

        # Try to detect provider type and call appropriate API
        if 'openai.com' in api_url.lower():
            import requests
            headers = {
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            }
            response = requests.get(f'{api_url}/models', headers=headers, timeout=10)
            if response.status_code == 200:
                result = response.json()
                models = [m['id'] for m in result.get('data', []) if 'gpt' in m['id'].lower()]
        elif 'deepseek' in api_url.lower():
            import requests
            headers = {
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            }
            response = requests.get(f'{api_url}/models', headers=headers, timeout=10)
            if response.status_code == 200:
                result = response.json()
                models = [m['id'] for m in result.get('data', [])]
        else:
            # Default: return common model names
            models = ['gpt-3.5-turbo', 'gpt-4', 'gpt-4-turbo']

        return jsonify({'models': models})
    except Exception as e:
        logger.error(f"Fetch models failed: {e}")
        return jsonify({'error': f'Failed to fetch models: {str(e)}'}), 500

# ============ Futures Configuration API Endpoints ============

@app.route('/api/futures', methods=['GET'])
def list_futures():
    """Get all futures configurations"""
    try:
        futures = db.get_futures()
        return jsonify(futures)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/futures', methods=['POST'])
def add_future_config():
    """Add new future configuration"""
    data = request.json or {}
    symbol = data.get('symbol', '').strip().upper()
    contract_symbol = data.get('contract_symbol', '').strip().upper()
    name = data.get('name', '').strip()
    exchange = data.get('exchange', 'BINANCE_FUTURES').strip().upper()
    link = (data.get('link') or '').strip()
    sort_order = data.get('sort_order')

    if not all([symbol, contract_symbol, name]):
        return jsonify({'error': 'symbol, contract_symbol, name are required'}), 400

    try:
        sort_order = int(sort_order)
    except (TypeError, ValueError):
        sort_order = 0

    try:
        future_id = db.add_future(
            symbol=symbol,
            contract_symbol=contract_symbol,
            name=name,
            exchange=exchange,
            link=link or None,
            sort_order=sort_order
        )
        return jsonify({'id': future_id, 'message': 'Future added successfully'})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/futures/<int:future_id>', methods=['DELETE'])
def delete_future_config(future_id):
    """Delete future configuration"""
    try:
        db.delete_future(future_id)
        return jsonify({'message': 'Future deleted successfully'})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ============ Model API Endpoints ============

@app.route('/api/models', methods=['GET'])
def get_models():
    """Get all trading models"""
    models = db.get_all_models()
    return jsonify(models)

@app.route('/api/models', methods=['POST'])
def add_model():
    """Add new trading model"""
    data = request.json or {}
    try:
        provider = db.get_provider(data['provider_id'])
        if not provider:
            return jsonify({'error': 'Provider not found'}), 404

        model_id = db.add_model(
            name=data['name'],
            provider_id=data['provider_id'],
            model_name=data['model_name'],
            initial_capital=float(data.get('initial_capital', 100000)),
            leverage=int(data.get('leverage', 10))
        )

        model = db.get_model(model_id)
        provider = db.get_provider(model['provider_id'])
        if not provider:
            return jsonify({'error': 'Provider not found'}), 404

        trading_engines[model_id] = TradingEngine(
            model_id=model_id,
            db=db,
            market_fetcher=market_fetcher,
            ai_trader=AITrader(
                provider_type=provider['provider_type'],
                api_key=provider['api_key'],
                api_url=provider['api_url'],
                model_name=model['model_name']
            ),
            trade_fee_rate=TRADE_FEE_RATE
        )
        logger.info(f"Model {model_id} ({data['name']}) initialized")

        return jsonify({'id': model_id, 'message': 'Model added successfully'})

    except Exception as e:
        logger.error(f"Failed to add model: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/models/<int:model_id>', methods=['DELETE'])
def delete_model(model_id):
    """Delete trading model"""
    try:
        model = db.get_model(model_id)
        model_name = model['name'] if model else f"ID-{model_id}"

        db.delete_model(model_id)
        if model_id in trading_engines:
            del trading_engines[model_id]

        logger.info(f"Model {model_id} ({model_name}) deleted")
        return jsonify({'message': 'Model deleted successfully'})
    except Exception as e:
        logger.error(f"Delete model {model_id} failed: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/models/<int:model_id>/portfolio', methods=['GET'])
def get_portfolio(model_id):
    """Get model portfolio data"""
    model = db.get_model(model_id)
    if not model:
        return jsonify({'error': f'Model {model_id} not found'}), 404

    symbols = get_tracked_symbols()
    prices_data = market_fetcher.get_prices(symbols)
    current_prices = {symbol: data['price'] for symbol, data in prices_data.items()}

    try:
        portfolio = db.get_portfolio(model_id, current_prices)
    except ValueError as exc:
        return jsonify({'error': str(exc)}), 404

    account_value = db.get_account_value_history(model_id, limit=100)

    return jsonify({
        'portfolio': portfolio,
        'account_value_history': account_value,
        'auto_trading_enabled': bool(model.get('auto_trading_enabled', 1)),
        'leverage': model.get('leverage', 10)
    })

@app.route('/api/models/<int:model_id>/trades', methods=['GET'])
def get_trades(model_id):
    """Get model trade history"""
    limit = request.args.get('limit', 50, type=int)
    trades = db.get_trades(model_id, limit=limit)
    return jsonify(trades)

@app.route('/api/models/<int:model_id>/conversations', methods=['GET'])
def get_conversations(model_id):
    """Get model conversation history"""
    limit = request.args.get('limit', 20, type=int)
    conversations = db.get_conversations(model_id, limit=limit)
    return jsonify(conversations)

@app.route('/api/models/<int:model_id>/prompts', methods=['GET'])
def get_model_prompts(model_id):
    """Get model prompt configuration"""
    model = db.get_model(model_id)
    if not model:
        return jsonify({'error': 'Model not found'}), 404

    prompt_config = db.get_model_prompt(model_id) or {}
    buy_prompt = prompt_config.get('buy_prompt') or DEFAULT_BUY_CONSTRAINTS
    sell_prompt = prompt_config.get('sell_prompt') or DEFAULT_SELL_CONSTRAINTS

    return jsonify({
        'model_id': model_id,
        'model_name': model.get('name'),
        'buy_prompt': buy_prompt,
        'sell_prompt': sell_prompt,
        'has_custom': bool(prompt_config),
        'updated_at': prompt_config.get('updated_at') if prompt_config else None
    })

@app.route('/api/models/<int:model_id>/prompts', methods=['PUT'])
def update_model_prompts(model_id):
    """Update model prompt configuration"""
    model = db.get_model(model_id)
    if not model:
        return jsonify({'error': 'Model not found'}), 404

    data = request.json or {}
    buy_prompt = data.get('buy_prompt')
    sell_prompt = data.get('sell_prompt')

    success = db.upsert_model_prompt(model_id, buy_prompt, sell_prompt)
    if not success:
        return jsonify({'error': 'Failed to update prompts'}), 500

    return jsonify({'success': True, 'message': 'Prompts updated successfully'})

@app.route('/api/models/<int:model_id>/leverage', methods=['POST'])
def update_model_leverage(model_id):
    """Update model leverage"""
    data = request.json or {}
    if 'leverage' not in data:
        return jsonify({'error': 'leverage is required'}), 400

    model = db.get_model(model_id)
    if not model:
        return jsonify({'error': 'Model not found'}), 404

    leverage = int(data.get('leverage', 0))
    leverage = max(0, leverage)
    if not db.set_model_leverage(model_id, leverage):
        return jsonify({'error': 'Failed to update leverage'}), 500

    return jsonify({'model_id': model_id, 'leverage': leverage})

@app.route('/api/models/<int:model_id>/execute', methods=['POST'])
def execute_trading(model_id):
    """Execute trading cycle for a model"""
    if model_id not in trading_engines:
        engine, error = init_trading_engine_for_model(model_id)
        if error:
            return jsonify({'error': error}), 404
    else:
        engine = trading_engines[model_id]

    # Manual execution enables auto trading
    db.set_model_auto_trading(model_id, True)

    try:
        result = engine.execute_trading_cycle()
        result['auto_trading_enabled'] = True
        return jsonify(result)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/models/<int:model_id>/auto-trading', methods=['POST'])
def set_model_auto_trading(model_id):
    """Enable or disable auto trading for a model"""
    data = request.json or {}
    if 'enabled' not in data:
        return jsonify({'error': 'enabled flag is required'}), 400

    model = db.get_model(model_id)
    if not model:
        return jsonify({'error': 'Model not found'}), 404

    enabled = bool(data.get('enabled'))
    success = db.set_model_auto_trading(model_id, enabled)
    if not success:
        return jsonify({'error': 'Failed to update model status'}), 500

    if enabled and model_id not in trading_engines:
        init_trading_engine_for_model(model_id)

    return jsonify({'model_id': model_id, 'auto_trading_enabled': enabled})

@app.route('/api/aggregated/portfolio', methods=['GET'])
def get_aggregated_portfolio():
    """Get aggregated portfolio data across all models"""
    symbols = get_tracked_symbols()
    prices_data = market_fetcher.get_current_prices(symbols)
    current_prices = {symbol: data['price'] for symbol, data in prices_data.items()}

    models = db.get_all_models()
    total_portfolio = {
        'total_value': 0,
        'cash': 0,
        'positions_value': 0,
        'realized_pnl': 0,
        'unrealized_pnl': 0,
        'initial_capital': 0,
        'positions': []
    }

    all_positions = {}

    for model in models:
        portfolio = db.get_portfolio(model['id'], current_prices)
        if portfolio:
            total_portfolio['total_value'] += portfolio.get('total_value', 0)
            total_portfolio['cash'] += portfolio.get('cash', 0)
            total_portfolio['positions_value'] += portfolio.get('positions_value', 0)
            total_portfolio['realized_pnl'] += portfolio.get('realized_pnl', 0)
            total_portfolio['unrealized_pnl'] += portfolio.get('unrealized_pnl', 0)
            total_portfolio['initial_capital'] += portfolio.get('initial_capital', 0)

            # Aggregate positions by future and side
            for pos in portfolio.get('positions', []):
                key = f"{pos['future']}_{pos['side']}"
                if key not in all_positions:
                    all_positions[key] = {
                        'future': pos['future'],
                        'side': pos['side'],
                        'quantity': 0,
                        'avg_price': 0,
                        'total_cost': 0,
                        'leverage': pos['leverage'],
                        'current_price': pos['current_price'],
                        'pnl': 0
                    }

                # Weighted average calculation
                current_pos = all_positions[key]
                current_cost = current_pos['quantity'] * current_pos['avg_price']
                new_cost = pos['quantity'] * pos['avg_price']
                total_quantity = current_pos['quantity'] + pos['quantity']

                if total_quantity > 0:
                    current_pos['avg_price'] = (current_cost + new_cost) / total_quantity
                    current_pos['quantity'] = total_quantity
                    current_pos['total_cost'] = current_cost + new_cost
                    current_pos['pnl'] = (pos['current_price'] - current_pos['avg_price']) * total_quantity

    total_portfolio['positions'] = list(all_positions.values())
    chart_data = db.get_multi_model_chart_data(limit=100)

    return jsonify({
        'portfolio': total_portfolio,
        'chart_data': chart_data,
        'model_count': len(models)
    })

# ============ Market Data API Endpoints ============

@app.route('/api/market/prices', methods=['GET'])
def get_market_prices():
    """Get current market prices for both configured futures and model positions"""
    # è·å–é…ç½®çš„åˆçº¦
    configured_symbols = get_tracked_symbols()
    configured_prices = market_fetcher.get_prices(configured_symbols)
    
    # ä¸ºé…ç½®çš„åˆçº¦æ·»åŠ æ¥æºæ ‡è®°
    for symbol in configured_prices:
        configured_prices[symbol]['source'] = 'configured'
    
    # è·å–æ‰€æœ‰æ¨¡å‹çš„æŒä»“åˆçº¦
    models = db.get_all_models()
    position_symbols = set()
    for model in models:
        try:
            portfolio = db.get_portfolio(model['id'], {})
            for pos in portfolio.get('positions', []):
                if pos.get('future'):
                    position_symbols.add(pos['future'])
        except Exception:
            continue
    
    # è·å–æŒä»“åˆçº¦çš„ä»·æ ¼æ•°æ®ï¼ˆæ’é™¤å·²é…ç½®çš„åˆçº¦ï¼Œé¿å…é‡å¤ï¼‰
    position_symbols = [s for s in position_symbols if s not in configured_symbols]
    if position_symbols:
        position_prices = market_fetcher.get_prices(position_symbols)
        # ä¸ºæŒä»“åˆçº¦æ·»åŠ æ¥æºæ ‡è®°
        for symbol in position_prices:
            position_prices[symbol]['source'] = 'position'
        # åˆå¹¶æ•°æ®
        configured_prices.update(position_prices)
    
    return jsonify(configured_prices)

@app.route('/api/market/indicators/<symbol>', methods=['GET'])
def get_market_indicators(symbol):
    """Get technical indicators for a specific symbol
    
    ä»å¸å®‰APIå®æ—¶è·å–å¹¶è®¡ç®—æŠ€æœ¯æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
    - Kçº¿æ•°æ®ï¼ˆå¼€é«˜ä½æ”¶ã€æˆäº¤é‡ï¼‰
    - MAå‡çº¿ï¼ˆ5ã€20ã€60ã€99å‘¨æœŸï¼‰
    - MACDæŒ‡æ ‡ï¼ˆDIFã€DEAã€BARï¼‰
    - RSIæŒ‡æ ‡ï¼ˆRSI6ã€RSI9ï¼‰
    - æˆäº¤é‡ï¼ˆVOLï¼‰
    
    æ—¶é—´æ¡†æ¶ï¼š1å‘¨ã€1å¤©ã€4å°æ—¶ã€1å°æ—¶ã€15åˆ†é’Ÿã€5åˆ†é’Ÿã€1åˆ†é’Ÿ
    
    Args:
        symbol: äº¤æ˜“å¯¹ç¬¦å·ï¼ˆå¦‚ 'BTC'ï¼‰
        
    Returns:
        æŠ€æœ¯æŒ‡æ ‡æ•°æ®å­—å…¸ï¼Œæ ¼å¼ï¼š{'timeframes': {1w: {...}, 1d: {...}, ...}}
    """
    try:
        indicators = market_fetcher.calculate_technical_indicators(symbol)
        if not indicators:
            return jsonify({
                'symbol': symbol,
                'timeframes': {},
                'error': 'æ— æ³•è·å–æŠ€æœ¯æŒ‡æ ‡æ•°æ®'
            }), 200
        
        return jsonify({
            'symbol': symbol,
            **indicators
        })
    except Exception as e:
        logger.error(f"[API] Failed to get indicators for {symbol}: {e}", exc_info=True)
        return jsonify({
            'symbol': symbol,
            'timeframes': {},
            'error': str(e)
        }), 500

@app.route('/api/market/leaderboard', methods=['GET'])
def get_market_leaderboard():
    """Get market leaderboard data"""
    limit = request.args.get('limit', type=int)
    force = request.args.get('force', default=0, type=int)
    try:
        data = market_fetcher.sync_leaderboard(force=bool(force), limit=limit)
        return jsonify(data)
    except Exception as exc:
        logger.error(f"Failed to load leaderboard: {exc}")
        return jsonify({'error': str(exc)}), 500

@app.route('/api/clickhouse/leaderboard/status', methods=['GET'])
def get_clickhouse_leaderboard_status():
    """Get ClickHouse leaderboard sync status
    
    è¿”å›çŠ¶æ€ä¿¡æ¯ï¼š
    - running: è¿è¡ŒçŠ¶æ€ï¼ˆTrueè¡¨ç¤ºè¿è¡Œä¸­ï¼ŒFalseè¡¨ç¤ºå·²åœæ­¢ï¼‰
    - thread_alive: çº¿ç¨‹æ˜¯å¦å­˜æ´»
    - é»˜è®¤çŠ¶æ€ä¸ºè¿è¡ŒçŠ¶æ€ï¼ˆrunning=Trueï¼‰
    """
    global clickhouse_leaderboard_running, clickhouse_leaderboard_thread
    
    # æ£€æŸ¥çº¿ç¨‹å®é™…çŠ¶æ€ï¼Œå¦‚æœçº¿ç¨‹ä¸å­˜åœ¨æˆ–å·²æ­»äº¡ï¼Œä½†ç”¨æˆ·æ²¡æœ‰æ˜ç¡®åœæ­¢ï¼Œåˆ™è®¤ä¸ºæ˜¯è¿è¡ŒçŠ¶æ€
    thread_alive = clickhouse_leaderboard_thread.is_alive() if clickhouse_leaderboard_thread else False
    
    # å¦‚æœçº¿ç¨‹å·²æ­»äº¡ä½†è¿è¡ŒçŠ¶æ€ä¸ºTrueï¼Œè¯´æ˜çº¿ç¨‹æ„å¤–é€€å‡ºï¼Œä½†ç”¨æˆ·æœŸæœ›è¿è¡Œ
    # è¿™ç§æƒ…å†µä¸‹ï¼Œè¿”å›running=Trueï¼Œè®©å‰ç«¯æ˜¾ç¤ºè¿è¡ŒçŠ¶æ€ï¼Œç›‘æ§çº¿ç¨‹ä¼šè‡ªåŠ¨é‡å¯
    actual_running = clickhouse_leaderboard_running or (not clickhouse_leaderboard_stop_event.is_set() and thread_alive)
    
    return jsonify({
        'running': actual_running,
        'thread_alive': thread_alive
    })

@app.route('/api/clickhouse/leaderboard/control', methods=['POST'])
def control_clickhouse_leaderboard():
    """Control ClickHouse leaderboard sync (start/stop)"""
    data = request.json or {}
    action = data.get('action', '').lower()
    
    if action == 'start':
        start_clickhouse_leaderboard_sync()
        return jsonify({'message': 'ClickHouse leaderboard sync started', 'running': True})
    elif action == 'stop':
        stop_clickhouse_leaderboard_sync()
        return jsonify({'message': 'ClickHouse leaderboard sync stopped', 'running': False})
    else:
        return jsonify({'error': 'Invalid action. Use "start" or "stop"'}), 400

@socketio.on('leaderboard:request')
def handle_leaderboard_request(payload=None):
    """WebSocket handler for leaderboard requests"""
    payload = payload or {}
    limit = payload.get('limit')
    try:
        data = market_fetcher.get_leaderboard(limit=limit)
        emit('leaderboard:update', data)
    except Exception as exc:
        emit('leaderboard:error', {'message': str(exc)})

# ============ Settings API Endpoints ============

@app.route('/api/settings', methods=['GET'])
def get_settings():
    """Get system settings"""
    try:
        settings = db.get_settings()
        return jsonify(settings)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/settings', methods=['PUT'])
def update_settings():
    """Update system settings"""
    try:
        data = request.json or {}
        trading_frequency_minutes = int(data.get('trading_frequency_minutes', 60))
        trading_fee_rate = float(data.get('trading_fee_rate', 0.001))
        show_system_prompt = 1 if data.get('show_system_prompt') in (True, 1, '1', 'true', 'True') else 0

        success = db.update_settings(
            trading_frequency_minutes,
            trading_fee_rate,
            show_system_prompt
        )

        if success:
            return jsonify({'success': True, 'message': 'Settings updated successfully'})
        else:
            return jsonify({'success': False, 'error': 'Failed to update settings'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ============ Version API Endpoints ============

@app.route('/api/version', methods=['GET'])
def get_version():
    """Get current version information"""
    return jsonify({
        'current_version': __version__
    })

@app.route('/api/check-update', methods=['GET'])
def check_update():
    """Check for application updates"""
    try:
        return jsonify({
            'update_available': False,
            'current_version': __version__,
            'latest_version': __version__,
            'error': None
        })
    except Exception as e:
        logger.error(f"Check update failed: {e}")
        return jsonify({
            'update_available': False,
            'current_version': __version__,
            'latest_version': __version__,
            'error': str(e)
        }), 500

# ============ Main Entry Point ============

if __name__ == '__main__':
    import webbrowser

    logger.info("\n" + "=" * 60)
    logger.info("AICoinTrade - Starting...")
    logger.info("=" * 60)
    logger.info("Initializing database...")

    # Initialize database and trading engines within application context
    with app.app_context():
        db.init_db()
        logger.info("Database initialized")
        logger.info("Initializing trading engines...")
        init_trading_engines()
        logger.info("Trading engines initialized")

    # Start background threads
    if auto_trading:
        trading_thread = threading.Thread(target=trading_loop, daemon=True)
        trading_thread.start()
        logger.info("Auto-trading enabled")

    # Start leaderboard workers
    logger.info("ğŸš€ å‡†å¤‡å¯åŠ¨æ¶¨è·Œå¹…æ¦œç›¸å…³å·¥ä½œçº¿ç¨‹...")
    
    # å¯åŠ¨å‰ç«¯æ¨é€å·¥ä½œçº¿ç¨‹
    logger.info("ğŸ“¡ å¯åŠ¨æ¶¨è·Œå¹…æ¦œå‰ç«¯æ¨é€çº¿ç¨‹...")
    start_leaderboard_worker()
    logger.info("âœ… æ¶¨è·Œå¹…æ¦œå‰ç«¯æ¨é€çº¿ç¨‹å·²å¯åŠ¨")
    
    # åˆå§‹åŒ–åå°æœåŠ¡ï¼ˆåŒ…æ‹¬ClickHouseæ¶¨è·Œå¹…æ¦œåŒæ­¥çº¿ç¨‹ï¼‰
    logger.info("ğŸ“Š åˆå§‹åŒ–åå°æœåŠ¡...")
    _init_background_services()
    
    logger.info("âœ… æ‰€æœ‰æ¶¨è·Œå¹…æ¦œç›¸å…³å·¥ä½œçº¿ç¨‹å·²å¯åŠ¨å®Œæˆ")

    logger.info("\n" + "=" * 60)
    logger.info("AICoinTrade is running!")
    logger.info("Server: http://localhost:5002")
    logger.info("Press Ctrl+C to stop")
    logger.info("=" * 60 + "\n")

    def open_browser():
        """Open browser after server starts"""
        time.sleep(1.5)
        url = "http://localhost:5002"
        try:
            webbrowser.open(url)
            logger.info(f"Browser opened: {url}")
        except Exception as e:
            logger.warning(f"Could not open browser: {e}")

    browser_thread = threading.Thread(target=open_browser, daemon=True)
    browser_thread.start()

    # å¼€å‘ç¯å¢ƒï¼šä½¿ç”¨WerkzeugæœåŠ¡å™¨
    # ç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨gunicorn + eventletï¼ˆè§Dockerfileå’Œgunicorn_config.pyï¼‰
    # é€šè¿‡ç¯å¢ƒå˜é‡USE_GUNICORN=trueæ¥ä½¿ç”¨gunicornå¯åŠ¨
    if os.getenv('USE_GUNICORN') == 'true':
        logger.info("Production mode: Use 'gunicorn --config gunicorn_config.py app:app' to start")
        # ç”Ÿäº§ç¯å¢ƒåº”è¯¥ä½¿ç”¨gunicornå¯åŠ¨ï¼Œè¿™é‡Œåªæ˜¯æç¤º
        socketio.run(
            app, 
            debug=False, 
            host='0.0.0.0', 
            port=5002, 
            use_reloader=False,
            allow_unsafe_werkzeug=True
        )
    else:
        # å¼€å‘ç¯å¢ƒ
        socketio.run(
            app, 
            debug=False, 
            host='0.0.0.0', 
            port=5002, 
            use_reloader=False,
            allow_unsafe_werkzeug=True
        )
