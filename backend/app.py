# This must be at the very top of the file, before any other imports
import eventlet
eventlet.monkey_patch()

"""
Flask application for AI Futures Trading System
"""
from flask import Flask, render_template, request, jsonify
from flask_cors import CORS
from flask_socketio import SocketIO, emit
import os
import time
import threading
import json
from datetime import datetime
from trade.trading_engine import TradingEngine
from market.market_data import MarketDataFetcher
from trade.ai_trader import AITrader
from common.database_basic import Database
from common.version import __version__
from trade.prompt_defaults import DEFAULT_BUY_CONSTRAINTS, DEFAULT_SELL_CONSTRAINTS

import common.config as app_config
import logging
import sys

# ============ Application Initialization ============

app = Flask(__name__)
# CORSé…ç½®ï¼šå…è®¸å‰ç«¯æœåŠ¡è®¿é—®
# æ³¨æ„ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œåº”è¯¥é™åˆ¶å…·ä½“çš„åŸŸåï¼Œè€Œä¸æ˜¯ä½¿ç”¨é€šé…ç¬¦
CORS(app, resources={
    r"/api/*": {
        "origins": "*",  # å…è®¸æ‰€æœ‰æ¥æºï¼ˆç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶ï¼‰
        "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],  # æ˜ç¡®æ”¯æŒæ‰€æœ‰HTTPæ–¹æ³•
        "allow_headers": ["Content-Type", "Authorization"],  # å…è®¸çš„è¯·æ±‚å¤´
        "expose_headers": ["Content-Type"],  # æš´éœ²çš„å“åº”å¤´
        "supports_credentials": False  # ä¸æ”¯æŒå‡­è¯
    },
    r"/socket.io/*": {"origins": "*"}  # å…è®¸æ‰€æœ‰æ¥æºï¼ˆç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶ï¼‰
})
# ä½¿ç”¨eventletä½œä¸ºå¼‚æ­¥æ¨¡å¼ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
# async_mode='eventlet' æä¾›æ›´å¥½çš„å¹¶å‘æ€§èƒ½
socketio = SocketIO(
    app, 
    cors_allowed_origins="*",  # Socket.IOå…è®¸æ‰€æœ‰æ¥æº
    async_mode='eventlet',  # ä½¿ç”¨eventletå¼‚æ­¥æ¨¡å¼
    logger=False,  # ç¦ç”¨SocketIOæ—¥å¿—ä»¥å‡å°‘å¼€é”€
    engineio_logger=False,  # ç¦ç”¨EngineIOæ—¥å¿—
    ping_timeout=60,  # WebSocket pingè¶…æ—¶æ—¶é—´
    ping_interval=25,  # WebSocket pingé—´éš”
    max_http_buffer_size=1e6,  # æœ€å¤§HTTPç¼“å†²åŒºå¤§å°
    allow_upgrades=True,  # å…è®¸åè®®å‡çº§
    transports=['websocket', 'polling']  # æ”¯æŒçš„ä¼ è¾“æ–¹å¼
)

# ============ Logging Configuration ============

def get_log_level():
    """ä»é…ç½®è·å–æ—¥å¿—çº§åˆ«ï¼Œé»˜è®¤ä¸º INFO"""
    log_level_str = getattr(app_config, 'LOG_LEVEL', 'INFO').upper()
    log_level_map = {
        'DEBUG': logging.DEBUG,
        'INFO': logging.INFO,
        'WARNING': logging.WARNING,
        'ERROR': logging.ERROR,
        'CRITICAL': logging.CRITICAL
    }
    return log_level_map.get(log_level_str, logging.INFO)

log_format = getattr(app_config, 'LOG_FORMAT', '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
log_date_format = getattr(app_config, 'LOG_DATE_FORMAT', '%Y-%m-%d %H:%M:%S')

logging.basicConfig(
    level=get_log_level(),
    format=log_format,
    datefmt=log_date_format,
    handlers=[logging.StreamHandler(sys.stdout)]
)

logging.getLogger('werkzeug').setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

# ============ Global Configuration ============

DEFAULT_DB_PATH = 'trading_bot.db'
env_db_path = os.getenv('DATABASE_PATH')
config_db_path = getattr(app_config, 'DATABASE_PATH', None)
db_path = env_db_path or config_db_path or DEFAULT_DB_PATH

db = Database(db_path)

# Initialize database tables immediately when the application starts
# This ensures tables are created even when running with gunicorn or other WSGI servers
with app.app_context():
    db.init_db()
    logger.info("Database tables initialized")

# åº”ç”¨å¯åŠ¨æ—¶ç«‹å³å¯åŠ¨ClickHouseæ¶¨è·Œå¹…æ¦œåŒæ­¥æœåŠ¡
# è¿™ç¡®ä¿æ— è®ºé€šè¿‡ä»€ä¹ˆæ–¹å¼å¯åŠ¨ï¼ˆç›´æ¥è¿è¡Œã€gunicornç­‰ï¼‰ï¼Œéƒ½ä¼šè‡ªåŠ¨å¯åŠ¨æœåŠ¡
def _init_background_services():
    """åˆå§‹åŒ–åå°æœåŠ¡ï¼ˆåœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ï¼‰"""
    global clickhouse_leaderboard_running
    
    logger.info("ğŸš€ åˆå§‹åŒ–åå°æœåŠ¡...")
    
    # å¯åŠ¨ClickHouseæ¶¨è·Œå¹…æ¦œåŒæ­¥çº¿ç¨‹ï¼ˆé»˜è®¤è¿è¡ŒçŠ¶æ€ï¼‰
    logger.info("ğŸ“Š å¯åŠ¨ClickHouseæ¶¨è·Œå¹…æ¦œåŒæ­¥æœåŠ¡...")
    start_clickhouse_leaderboard_sync()
    logger.info("âœ… ClickHouseæ¶¨è·Œå¹…æ¦œåŒæ­¥æœåŠ¡å·²å¯åŠ¨ï¼ˆé»˜è®¤è¿è¡ŒçŠ¶æ€ï¼‰")
    
    logger.info("âœ… åå°æœåŠ¡åˆå§‹åŒ–å®Œæˆ")

market_fetcher = MarketDataFetcher(db)
trading_engines = {}
auto_trading = getattr(app_config, 'AUTO_TRADING', True)
TRADE_FEE_RATE = getattr(app_config, 'TRADE_FEE_RATE', 0.001)
LEADERBOARD_REFRESH_INTERVAL = getattr(app_config, 'FUTURES_LEADERBOARD_REFRESH', 10)

leaderboard_thread = None
leaderboard_stop_event = threading.Event()

# ClickHouse leaderboard sync
clickhouse_leaderboard_thread = None
clickhouse_leaderboard_stop_event = threading.Event()
clickhouse_leaderboard_running = True  # é»˜è®¤çŠ¶æ€ä¸ºè¿è¡ŒçŠ¶æ€
# æ·»åŠ çº¿ç¨‹é”ä»¥é˜²æ­¢å¹¶å‘æ‰§è¡Œ
clickhouse_leaderboard_lock = threading.Lock()
# çº¿ç¨‹ç›‘æ§æ ‡å¿—ï¼Œç”¨äºè‡ªåŠ¨é‡å¯
clickhouse_leaderboard_monitor_thread = None
clickhouse_leaderboard_monitor_stop_event = threading.Event()

# ============ Helper Functions ============

def init_trading_engine_for_model(model_id: int):
    """Initialize trading engine for a model if possible."""
    model = db.get_model(model_id)
    if not model:
        return None, 'Model not found'

    provider = db.get_provider(model['provider_id'])
    if not provider:
        return None, 'Provider not found'

    trading_engines[model_id] = TradingEngine(
        model_id=model_id,
        db=db,
        market_fetcher=market_fetcher,
        ai_trader=AITrader(
            provider_type=provider.get('provider_type', 'openai'),
            api_key=provider['api_key'],
            api_url=provider['api_url'],
            model_name=model['model_name'],
            market_fetcher=market_fetcher
        ),
        trade_fee_rate=TRADE_FEE_RATE
    )
    return trading_engines[model_id], None

def get_tracked_symbols():
    """Get list of tracked future symbols"""
    symbols = db.get_future_symbols()
    if not symbols:
        logger.warning('No futures configured. Please add futures via /api/futures.')
    return symbols

def get_trading_interval_seconds() -> int:
    """Read trading frequency from settings (minutes) and return seconds."""
    default_interval_seconds = getattr(app_config, 'TRADING_INTERVAL', 3600)
    default_minutes = max(1, int(default_interval_seconds / 60))
    try:
        settings = db.get_settings()
        minutes = int(settings.get('trading_frequency_minutes', default_minutes))
    except Exception as e:
        logger.warning(f"Unable to load trading frequency setting: {e}")
        minutes = default_minutes

    minutes = max(1, min(1440, minutes))
    return minutes * 60

def init_trading_engines():
    """Initialize trading engines for all models"""
    try:
        models = db.get_all_models()

        if not models:
            logger.warning("No trading models found")
            return

        logger.info(f"\nINIT: Initializing trading engines...")
        for model in models:
            model_id = model['id']
            model_name = model['name']

            try:
                provider = db.get_provider(model['provider_id'])
                if not provider:
                    logger.warning(f"  Model {model_id} ({model_name}): Provider not found")
                    continue

                trading_engines[model_id] = TradingEngine(
                    model_id=model_id,
                    db=db,
                    market_fetcher=market_fetcher,
                    ai_trader=AITrader(
                        provider_type=provider.get('provider_type', 'openai'),
                        api_key=provider['api_key'],
                        api_url=provider['api_url'],
                        model_name=model['model_name'],
                        market_fetcher=market_fetcher
                    ),
                    trade_fee_rate=TRADE_FEE_RATE
                )
                logger.info(f"  OK: Model {model_id} ({model_name})")
            except Exception as e:
                logger.error(f"  Model {model_id} ({model_name}): {e}")
                continue

        logger.info(f"Initialized {len(trading_engines)} engine(s)\n")

    except Exception as e:
        logger.error(f"Init engines failed: {e}\n")

# ============ Background Tasks ============

def _leaderboard_loop():
    """
    åå°å¾ªç¯ä»»åŠ¡ï¼šå®šæœŸåŒæ­¥æ¶¨è·Œå¹…æ¦œæ•°æ®åˆ°ClickHouseï¼ˆä¸å†é€šè¿‡WebSocketæ¨é€åˆ°å‰ç«¯ï¼‰
    
    æµç¨‹ï¼š
    1. å¯åŠ¨å¾ªç¯ï¼Œè®°å½•å¯åŠ¨ä¿¡æ¯
    2. å®šæœŸè°ƒç”¨ sync_leaderboard åŒæ­¥æ•°æ®åˆ°ClickHouse
    3. ä¸å†é€šè¿‡ WebSocket æ¨é€åˆ°å‰ç«¯ï¼ˆå‰ç«¯æ”¹ä¸ºè½®è¯¢æ–¹å¼è·å–æ•°æ®ï¼‰
    4. ç­‰å¾…æŒ‡å®šé—´éš”åç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯
    5. æ”¶åˆ°åœæ­¢ä¿¡å·æ—¶é€€å‡ºå¾ªç¯
    
    æ³¨æ„ï¼šå‰ç«¯å·²æ”¹ä¸ºè½®è¯¢æ–¹å¼è·å–æ•°æ®ï¼Œä¸å†ä½¿ç”¨WebSocketæ¨é€
    """
    thread_id = threading.current_thread().ident
    logger.info(f"[Leaderboard Worker-{thread_id}] æ¶¨è·Œå¹…æ¦œåŒæ­¥å¾ªç¯å¯åŠ¨ï¼Œåˆ·æ–°é—´éš”: {LEADERBOARD_REFRESH_INTERVAL} ç§’ï¼ˆä»…åŒæ­¥åˆ°ClickHouseï¼Œä¸æ¨é€å‰ç«¯ï¼‰")
    
    wait_seconds = max(5, LEADERBOARD_REFRESH_INTERVAL)
    cycle_count = 0
    
    while not leaderboard_stop_event.is_set():
        cycle_count += 1
        cycle_start_time = datetime.now()
        
        try:
            # è°ƒç”¨åŒæ­¥æ–¹æ³•ï¼ˆä¸å¼ºåˆ¶åˆ·æ–°ï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶ï¼‰
            # ä»…åŒæ­¥æ•°æ®åˆ°ClickHouseï¼Œä¸å†é€šè¿‡WebSocketæ¨é€
            data = market_fetcher.sync_leaderboard(force=False)
            
            # æ£€æŸ¥åŒæ­¥ç»“æœï¼ˆä»…è®°å½•æ—¥å¿—ï¼Œä¸æ¨é€ï¼‰
            if data:
                gainers_count = len(data.get('gainers', [])) if data.get('gainers') else 0
                losers_count = len(data.get('losers', [])) if data.get('losers') else 0
                logger.debug(
                    f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] åŒæ­¥å®Œæˆ: "
                    f"æ¶¨å¹…æ¦œ {gainers_count} æ¡, è·Œå¹…æ¦œ {losers_count} æ¡ "
                    f"ï¼ˆå·²åŒæ­¥åˆ°ClickHouseï¼Œå‰ç«¯é€šè¿‡è½®è¯¢è·å–ï¼‰"
                )
            else:
                logger.warning(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] åŒæ­¥è¿”å›ç©ºæ•°æ®")
                
        except Exception as exc:
            cycle_duration = (datetime.now() - cycle_start_time).total_seconds()
            logger.error(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] æ¶¨è·Œå¹…æ¦œåŒæ­¥å¤±è´¥: {exc}, è€—æ—¶: {cycle_duration:.2f} ç§’")
            import traceback
            logger.error(f"[Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
        
        # ç­‰å¾…æŒ‡å®šé—´éš”ï¼ˆå¯è¢«åœæ­¢äº‹ä»¶ä¸­æ–­ï¼‰
        leaderboard_stop_event.wait(wait_seconds)
    
    logger.info(f"[Leaderboard Worker-{thread_id}] æ¶¨è·Œå¹…æ¦œåŒæ­¥å¾ªç¯åœæ­¢ï¼Œæ€»å¾ªç¯æ¬¡æ•°: {cycle_count}")

def _clickhouse_leaderboard_loop():
    """
    åå°å¾ªç¯ä»»åŠ¡ï¼šå®šæœŸä» ClickHouse 24_market_tickers è¡¨åŒæ­¥æ¶¨è·Œå¹…æ¦œæ•°æ®åˆ° futures_leaderboard è¡¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - å®šæœŸä»24_market_tickersè¡¨è·å–æœ€æ–°çš„å¸‚åœºæ•°æ®
    - è®¡ç®—æ¯ä¸ªåˆçº¦çš„æ¶¨è·Œå¹…
    - ç­›é€‰å‡ºæ¶¨å¹…å‰Nåå’Œè·Œå¹…å‰Nå
    - å°†ç»“æœä¿å­˜åˆ°futures_leaderboardè¡¨ä¸­
    - æ”¯æŒé…ç½®åŒæ­¥é—´éš”ã€æ—¶é—´çª—å£å’Œå‰Nåæ•°é‡
    
    æ‰§è¡Œæµç¨‹ï¼š
    1. åˆå§‹åŒ–ClickHouseè¿æ¥
    2. è·å–é…ç½®å‚æ•°
    3. è¿›å…¥ä¸»å¾ªç¯ï¼š
       a. æŸ¥è¯¢æœ€è¿‘æ—¶é—´çª—å£å†…çš„å¸‚åœºæ•°æ®
       b. è®¡ç®—æ¶¨è·Œå¹…å¹¶æ’åº
       c. ç­›é€‰å‰Nåæ¶¨å¹…å’Œè·Œå¹…
       d. åŸå­æ›´æ–°futures_leaderboardè¡¨
       e. ç­‰å¾…æŒ‡å®šé—´éš”åé‡å¤å¾ªç¯
    4. æ”¶åˆ°åœæ­¢ä¿¡å·æ—¶é€€å‡ºå¾ªç¯
    
    é…ç½®å‚æ•°ï¼š
    - CLICKHOUSE_LEADERBOARD_SYNC_INTERVAL: åŒæ­¥é—´éš”ï¼ˆç§’ï¼‰
    - CLICKHOUSE_LEADERBOARD_TIME_WINDOW: æŸ¥è¯¢æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
    - CLICKHOUSE_LEADERBOARD_TOP_N: æ¶¨è·Œå¹…å‰Nåæ•°é‡
    
    æ³¨æ„ï¼š
    - æ­¤å‡½æ•°åŒ…å«å¼‚å¸¸å¤„ç†ï¼Œç¡®ä¿å³ä½¿å‘ç”Ÿå¼‚å¸¸ä¹Ÿä¸ä¼šé€€å‡ºå¾ªç¯
    - åªæœ‰åœ¨æ”¶åˆ°æ˜ç¡®çš„åœæ­¢ä¿¡å·æ—¶æ‰ä¼šé€€å‡º
    """
    # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯å¯¼å…¥é—®é¢˜
    from common.database_clickhouse import ClickHouseDatabase
    
    # è·å–å½“å‰çº¿ç¨‹IDï¼Œç”¨äºæ—¥å¿—æ ‡è¯†
    thread_id = threading.current_thread().ident
    
    # è·å–é…ç½®å‚æ•°ï¼Œå¸¦é»˜è®¤å€¼
    sync_interval = getattr(app_config, 'CLICKHOUSE_LEADERBOARD_SYNC_INTERVAL', 2)
    time_window = getattr(app_config, 'CLICKHOUSE_LEADERBOARD_TIME_WINDOW', 5)  # å·²åºŸå¼ƒï¼Œä¿ç•™ä»¥å…¼å®¹
    top_n = getattr(app_config, 'CLICKHOUSE_LEADERBOARD_TOP_N', 10)
    
    logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] ClickHouse æ¶¨å¹…æ¦œåŒæ­¥å¾ªç¯å¯åŠ¨ï¼ŒåŒæ­¥é—´éš”: {sync_interval} ç§’ï¼Œå‰Nåæ•°é‡: {top_n}")
    
    # ç¡®ä¿ç­‰å¾…æ—¶é—´è‡³å°‘ä¸º1ç§’
    wait_seconds = max(1, sync_interval)
    cycle_count = 0
    db = None
    
    # åœ¨å¾ªç¯å¤–åˆ›å»ºClickHouseDatabaseå®ä¾‹ï¼Œé¿å…é¢‘ç¹åˆ›å»ºå’Œé”€æ¯è¿æ¥
    try:
        db = ClickHouseDatabase(auto_init_tables=True)
    except Exception as exc:
        logger.error(f"[ClickHouse Leaderboard Worker-{thread_id}] åˆå§‹åŒ–ClickHouseè¿æ¥å¤±è´¥: {exc}ï¼Œå°†åœ¨å¾ªç¯ä¸­é‡è¯•åˆå§‹åŒ–")
        # ä¸ç›´æ¥è¿”å›ï¼Œè€Œæ˜¯åœ¨å¾ªç¯ä¸­é‡è¯•
    
    # ç«‹å³æ‰§è¡Œç¬¬ä¸€æ¬¡åŒæ­¥ï¼ˆå¯åŠ¨æ—¶ç«‹å³åˆ·æ–°æ•°æ®ï¼‰
    cycle_count += 1
    cycle_start_time = datetime.now()
    
    try:
        # å¦‚æœæ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–ï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–
        if db is None:
            db = ClickHouseDatabase(auto_init_tables=True)
        
        # æ‰§è¡ŒåŒæ­¥é€»è¾‘
        db.sync_leaderboard(
            time_window_seconds=time_window,
            top_n=top_n
        )
    except Exception as exc:
        # å¤„ç†åŒæ­¥å¤±è´¥çš„æƒ…å†µï¼Œä½†ä¸é€€å‡ºå¾ªç¯
        cycle_duration = (datetime.now() - cycle_start_time).total_seconds()
        logger.error(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] å¯åŠ¨æ—¶é¦–æ¬¡åŒæ­¥å¤±è´¥: {exc}, è€—æ—¶: {cycle_duration:.3f} ç§’")
        import traceback
        logger.error(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
    
    # ä¸»å¾ªç¯ï¼šå®šæœŸæ‰§è¡ŒåŒæ­¥ä»»åŠ¡ï¼ˆæ°¸ä¸é€€å‡ºï¼Œé™¤éæ”¶åˆ°åœæ­¢ä¿¡å·ï¼‰
    while not clickhouse_leaderboard_stop_event.is_set():
        cycle_count += 1
        cycle_start_time = datetime.now()
        
        try:
            # å¦‚æœæ•°æ®åº“è¿æ¥ä¸¢å¤±ï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–
            if db is None:
                db = ClickHouseDatabase(auto_init_tables=True)
            
            # æ‰§è¡ŒåŒæ­¥é€»è¾‘
            db.sync_leaderboard(
                time_window_seconds=time_window,
                top_n=top_n
            )
            
        except Exception as exc:
            # å¤„ç†åŒæ­¥å¤±è´¥çš„æƒ…å†µï¼Œä½†ä¸é€€å‡ºå¾ªç¯ï¼Œç»§ç»­é‡è¯•
            cycle_duration = (datetime.now() - cycle_start_time).total_seconds()
            logger.error(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] åŒæ­¥å¤±è´¥: {exc}, è€—æ—¶: {cycle_duration:.3f} ç§’")
            import traceback
            error_stack = traceback.format_exc()
            logger.error(f"[ClickHouse Leaderboard Worker-{thread_id}] [å¾ªç¯ #{cycle_count}] é”™è¯¯å †æ ˆ: {error_stack}")
            # æ ‡è®°æ•°æ®åº“è¿æ¥å¯èƒ½å·²å¤±æ•ˆï¼Œä¸‹æ¬¡å¾ªç¯æ—¶é‡æ–°åˆå§‹åŒ–
            db = None
        
        # ç­‰å¾…æŒ‡å®šé—´éš”åç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯
        # ä½¿ç”¨wait()æ–¹æ³•å¯ä»¥è¢«åœæ­¢äº‹ä»¶ä¸­æ–­
        # å¦‚æœç­‰å¾…æœŸé—´æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œå¾ªç¯ä¼šé€€å‡º
        if clickhouse_leaderboard_stop_event.wait(wait_seconds):
            # å¦‚æœwaitè¿”å›Trueï¼Œè¯´æ˜åœ¨ç­‰å¾…æœŸé—´æ”¶åˆ°äº†åœæ­¢ä¿¡å·
            break
    
    # å¾ªç¯ç»“æŸï¼Œè®°å½•åœæ­¢ä¿¡æ¯
    logger.info(f"[ClickHouse Leaderboard Worker-{thread_id}] ClickHouse æ¶¨å¹…æ¦œåŒæ­¥å¾ªç¯åœæ­¢ï¼Œæ€»å¾ªç¯æ¬¡æ•°: {cycle_count}")
    
    # æ›´æ–°è¿è¡ŒçŠ¶æ€
    global clickhouse_leaderboard_running
    with clickhouse_leaderboard_lock:
        clickhouse_leaderboard_running = False


def _clickhouse_leaderboard_monitor():
    """
    ç›‘æ§çº¿ç¨‹ï¼šç›‘æ§ClickHouseæ¶¨è·Œå¹…æ¦œåŒæ­¥çº¿ç¨‹ï¼Œå¦‚æœçº¿ç¨‹æ„å¤–é€€å‡ºåˆ™è‡ªåŠ¨é‡å¯
    
    æ­¤ç›‘æ§çº¿ç¨‹ç¡®ä¿åŒæ­¥æœåŠ¡æŒç»­è¿è¡Œï¼Œä¸ä¼šå› ä¸ºå¼‚å¸¸è€Œåœæ­¢
    """
    global clickhouse_leaderboard_thread, clickhouse_leaderboard_running
    
    logger.info("[ClickHouse Leaderboard Monitor] ğŸ›¡ï¸  ç›‘æ§çº¿ç¨‹å¯åŠ¨")
    
    while not clickhouse_leaderboard_monitor_stop_event.is_set():
        # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡çº¿ç¨‹çŠ¶æ€
        clickhouse_leaderboard_monitor_stop_event.wait(10)
        
        if clickhouse_leaderboard_monitor_stop_event.is_set():
            break
        
        with clickhouse_leaderboard_lock:
            # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
            if clickhouse_leaderboard_running:
                if clickhouse_leaderboard_thread and clickhouse_leaderboard_thread.is_alive():
                    # çº¿ç¨‹æ­£å¸¸è¿è¡Œï¼Œç»§ç»­ç›‘æ§
                    continue
                else:
                    # çº¿ç¨‹æ„å¤–é€€å‡ºï¼Œéœ€è¦é‡å¯
                    logger.warning("[ClickHouse Leaderboard Monitor] âš ï¸  æ£€æµ‹åˆ°åŒæ­¥çº¿ç¨‹æ„å¤–é€€å‡ºï¼Œå‡†å¤‡è‡ªåŠ¨é‡å¯...")
                    clickhouse_leaderboard_running = False
            
            # å¦‚æœè¿è¡ŒçŠ¶æ€ä¸ºFalseï¼Œä½†ç”¨æˆ·æ²¡æœ‰æ˜ç¡®åœæ­¢ï¼Œåˆ™è‡ªåŠ¨é‡å¯
            if not clickhouse_leaderboard_running and not clickhouse_leaderboard_stop_event.is_set():
                logger.info("[ClickHouse Leaderboard Monitor] ğŸ”„ è‡ªåŠ¨é‡å¯åŒæ­¥çº¿ç¨‹...")
                clickhouse_leaderboard_stop_event.clear()
                clickhouse_leaderboard_running = True
                
                clickhouse_leaderboard_thread = threading.Thread(
                    target=_clickhouse_leaderboard_loop,
                    daemon=True,
                    name="ClickHouseLeaderboardSync"
                )
                clickhouse_leaderboard_thread.start()
                logger.info("[ClickHouse Leaderboard Monitor] âœ… åŒæ­¥çº¿ç¨‹å·²è‡ªåŠ¨é‡å¯")
    
    logger.info("[ClickHouse Leaderboard Monitor] ğŸ›¡ï¸  ç›‘æ§çº¿ç¨‹åœæ­¢")


def start_clickhouse_leaderboard_sync():
    """
    å¯åŠ¨ ClickHouse æ¶¨å¹…æ¦œåŒæ­¥çº¿ç¨‹
    
    åŠŸèƒ½ï¼š
    - æ£€æŸ¥åŒæ­¥çº¿ç¨‹æ˜¯å¦å·²åœ¨è¿è¡Œ
    - åˆå§‹åŒ–åœæ­¢äº‹ä»¶
    - åˆ›å»ºå¹¶å¯åŠ¨åŒæ­¥çº¿ç¨‹
    - è®¾ç½®çº¿ç¨‹ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼Œç¡®ä¿ä¸»ç¨‹åºé€€å‡ºæ—¶è‡ªåŠ¨ç»ˆæ­¢
    - å¯åŠ¨ç›‘æ§çº¿ç¨‹ï¼Œç¡®ä¿çº¿ç¨‹æ„å¤–é€€å‡ºæ—¶è‡ªåŠ¨é‡å¯
    
    æ³¨æ„ï¼š
    - è¯¥å‡½æ•°æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå¯ä»¥å¤šæ¬¡è°ƒç”¨
    - å¤šæ¬¡è°ƒç”¨æ—¶ï¼Œåªæœ‰ç¬¬ä¸€æ¬¡ä¼šçœŸæ­£å¯åŠ¨çº¿ç¨‹
    - é»˜è®¤çŠ¶æ€ä¸ºè¿è¡ŒçŠ¶æ€ï¼Œåº”ç”¨å¯åŠ¨æ—¶è‡ªåŠ¨æ‰§è¡Œ
    """
    global clickhouse_leaderboard_thread, clickhouse_leaderboard_running
    global clickhouse_leaderboard_monitor_thread, clickhouse_leaderboard_monitor_stop_event
    
    # ä½¿ç”¨é”é˜²æ­¢å¹¶å‘æ‰§è¡Œ
    with clickhouse_leaderboard_lock:
        # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦å·²åœ¨è¿è¡Œ
        if clickhouse_leaderboard_thread and clickhouse_leaderboard_thread.is_alive():
            logger.warning("[ClickHouse Leaderboard] âš ï¸  åŒæ­¥çº¿ç¨‹å·²åœ¨è¿è¡Œï¼Œæ— éœ€é‡å¤å¯åŠ¨")
            return
        
        logger.info("[ClickHouse Leaderboard] ğŸš€ å‡†å¤‡å¯åŠ¨æ¶¨è·Œå¹…æ¦œåŒæ­¥çº¿ç¨‹...")
        
        # é‡ç½®åœæ­¢äº‹ä»¶å’Œè¿è¡ŒçŠ¶æ€
        clickhouse_leaderboard_stop_event.clear()
        clickhouse_leaderboard_running = True
        
        # åˆ›å»ºåŒæ­¥çº¿ç¨‹
        clickhouse_leaderboard_thread = threading.Thread(
            target=_clickhouse_leaderboard_loop,
            daemon=True,  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹
            name="ClickHouseLeaderboardSync"  # è®¾ç½®çº¿ç¨‹åç§°ï¼Œä¾¿äºè°ƒè¯•
        )
        
        # å¯åŠ¨çº¿ç¨‹
        clickhouse_leaderboard_thread.start()
        
        # è®°å½•å¯åŠ¨ä¿¡æ¯
        logger.info(f"[ClickHouse Leaderboard] âœ… æ¶¨è·Œå¹…æ¦œåŒæ­¥çº¿ç¨‹å·²å¯åŠ¨")
        logger.info(f"[ClickHouse Leaderboard] ğŸ“‹ çº¿ç¨‹ID: {clickhouse_leaderboard_thread.ident}")
        logger.info(f"[ClickHouse Leaderboard] ğŸ“‹ çº¿ç¨‹åç§°: {clickhouse_leaderboard_thread.name}")
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰å¯åŠ¨ï¼‰
        if not clickhouse_leaderboard_monitor_thread or not clickhouse_leaderboard_monitor_thread.is_alive():
            clickhouse_leaderboard_monitor_stop_event.clear()
            clickhouse_leaderboard_monitor_thread = threading.Thread(
                target=_clickhouse_leaderboard_monitor,
                daemon=True,
                name="ClickHouseLeaderboardMonitor"
            )
            clickhouse_leaderboard_monitor_thread.start()
            logger.info("[ClickHouse Leaderboard] ğŸ›¡ï¸  ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")


def stop_clickhouse_leaderboard_sync():
    """
    åœæ­¢ ClickHouse æ¶¨å¹…æ¦œåŒæ­¥çº¿ç¨‹
    
    åŠŸèƒ½ï¼š
    - æ£€æŸ¥åŒæ­¥çº¿ç¨‹æ˜¯å¦åœ¨è¿è¡Œ
    - è®¾ç½®åœæ­¢äº‹ä»¶ï¼Œé€šçŸ¥çº¿ç¨‹é€€å‡º
    - ç­‰å¾…çº¿ç¨‹ç»ˆæ­¢ï¼ˆæœ€å¤š5ç§’ï¼‰
    - æ›´æ–°è¿è¡ŒçŠ¶æ€
    - åœæ­¢ç›‘æ§çº¿ç¨‹
    
    æ³¨æ„ï¼š
    - è¯¥å‡½æ•°æ˜¯çº¿ç¨‹å®‰å…¨çš„
    - è°ƒç”¨åä¼šç«‹å³è¿”å›ï¼Œä¸ä¼šé˜»å¡ç­‰å¾…çº¿ç¨‹ç»ˆæ­¢
    - åªæœ‰ç”¨æˆ·æ˜ç¡®è°ƒç”¨æ­¤å‡½æ•°æ—¶æ‰ä¼šåœæ­¢ï¼Œä¸ä¼šè‡ªåŠ¨æš‚åœ
    """
    global clickhouse_leaderboard_running
    global clickhouse_leaderboard_monitor_thread, clickhouse_leaderboard_monitor_stop_event
    
    # ä½¿ç”¨é”é˜²æ­¢å¹¶å‘æ‰§è¡Œ
    with clickhouse_leaderboard_lock:
        # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦åœ¨è¿è¡Œ
        if not clickhouse_leaderboard_running:
            logger.warning("[ClickHouse Leaderboard] âš ï¸  åŒæ­¥çº¿ç¨‹æœªè¿è¡Œï¼Œæ— éœ€åœæ­¢")
            return
        
        logger.info("[ClickHouse Leaderboard] ğŸ›‘ å‡†å¤‡åœæ­¢æ¶¨è·Œå¹…æ¦œåŒæ­¥çº¿ç¨‹ï¼ˆç”¨æˆ·æ‰‹åŠ¨åœæ­¢ï¼‰...")
        
        # è®¾ç½®åœæ­¢çŠ¶æ€å’Œåœæ­¢äº‹ä»¶
        clickhouse_leaderboard_running = False
        clickhouse_leaderboard_stop_event.set()
        
        # åœæ­¢ç›‘æ§çº¿ç¨‹
        if clickhouse_leaderboard_monitor_thread and clickhouse_leaderboard_monitor_thread.is_alive():
            logger.info("[ClickHouse Leaderboard] ğŸ›‘ åœæ­¢ç›‘æ§çº¿ç¨‹...")
            clickhouse_leaderboard_monitor_stop_event.set()
            clickhouse_leaderboard_monitor_thread.join(timeout=2)
        
        # ç­‰å¾…çº¿ç¨‹ç»ˆæ­¢ï¼Œæœ€å¤š5ç§’
        if clickhouse_leaderboard_thread and clickhouse_leaderboard_thread.is_alive():
            logger.info("[ClickHouse Leaderboard] â³ ç­‰å¾…çº¿ç¨‹ç»ˆæ­¢...")
            clickhouse_leaderboard_thread.join(timeout=5)
            
            if clickhouse_leaderboard_thread.is_alive():
                logger.warning("[ClickHouse Leaderboard] âš ï¸  çº¿ç¨‹æœªèƒ½åœ¨5ç§’å†…ç»ˆæ­¢ï¼Œå¯èƒ½å·²å¼ºåˆ¶ç»ˆæ­¢")
            else:
                logger.info("[ClickHouse Leaderboard] âœ… çº¿ç¨‹å·²æˆåŠŸç»ˆæ­¢")
        else:
            logger.info("[ClickHouse Leaderboard] âœ… çº¿ç¨‹å·²åœæ­¢ï¼ˆæœªè¿è¡Œï¼‰")
        
        logger.info("[ClickHouse Leaderboard] ğŸ“‹ æ¶¨è·Œå¹…æ¦œåŒæ­¥çº¿ç¨‹åœæ­¢å®Œæˆ")

def start_leaderboard_worker():
    """Start background worker for leaderboard updates"""
    global leaderboard_thread
    if leaderboard_thread and leaderboard_thread.is_alive():
        return
    leaderboard_stop_event.clear()
    leaderboard_thread = threading.Thread(target=_leaderboard_loop, daemon=True)
    leaderboard_thread.start()

def trading_loop():
    """Main trading loop for automatic trading"""
    logger.info("Trading loop started")

    while auto_trading:
        try:
            if not trading_engines:
                time.sleep(30)
                continue

            logger.info(f"\n{'='*60}")
            logger.info(f"CYCLE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"Active models: {len(trading_engines)}")
            logger.info(f"{'='*60}")

            for model_id, engine in list(trading_engines.items()):
                try:
                    if not db.is_model_auto_trading_enabled(model_id):
                        logger.info(f"SKIP: Model {model_id} auto trading paused")
                        continue

                    logger.info(f"\nEXEC: Model {model_id}")
                    result = engine.execute_trading_cycle()

                    if result.get('success'):
                        logger.info(f"OK: Model {model_id} completed")
                        if result.get('executions'):
                            for exec_result in result['executions']:
                                signal = exec_result.get('signal', 'unknown')
                                symbol = exec_result.get('future', exec_result.get('symbol', 'unknown'))
                                msg = exec_result.get('message', '')
                                if signal != 'hold':
                                    logger.info(f"  TRADE: {symbol}: {msg}")
                    else:
                        error = result.get('error', 'Unknown error')
                        logger.warning(f"Model {model_id} failed: {error}")

                except Exception as e:
                    logger.error(f"Model {model_id} exception: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
                    continue

            interval_seconds = get_trading_interval_seconds()
            interval_minutes = interval_seconds / 60
            logger.info(f"\n{'='*60}")
            logger.info(f"SLEEP: Waiting {interval_minutes:.1f} minute(s) for next cycle")
            logger.info(f"{'='*60}\n")

            time.sleep(interval_seconds)

        except Exception as e:
            logger.critical(f"\nTrading loop error: {e}")
            import traceback
            logger.critical(traceback.format_exc())
            logger.info("RETRY: Retrying in 60 seconds\n")
            time.sleep(60)

    logger.info("Trading loop stopped")

# ============ Page Routes ============

# åå°æœåŠ¡åˆå§‹åŒ–æ ‡å¿—ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œç¡®ä¿æ‰€æœ‰å‡½æ•°éƒ½å·²å®šä¹‰ï¼‰
_background_services_initialized = False

@app.before_request
def _ensure_background_services():
    """ç¡®ä¿åå°æœåŠ¡å·²å¯åŠ¨ï¼ˆåœ¨ç¬¬ä¸€æ¬¡è¯·æ±‚æ—¶è°ƒç”¨ï¼‰"""
    global _background_services_initialized
    if not _background_services_initialized:
        _init_background_services()
        _background_services_initialized = True

@app.after_request
def after_request(response):
    """æ·»åŠ  CORS å“åº”å¤´ï¼Œç¡®ä¿æ‰€æœ‰è¯·æ±‚éƒ½èƒ½æ­£ç¡®å¤„ç†"""
    # å¯¹äºæ‰€æœ‰ API è¯·æ±‚ï¼Œæ·»åŠ  CORS å¤´
    if request.path.startswith('/api/'):
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Methods', 'GET, POST, PUT, DELETE, OPTIONS')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type, Authorization')
        response.headers.add('Access-Control-Max-Age', '3600')
    return response

@app.route('/')
def index():
    """Main page route - è¿”å›ç®€å•çš„çŠ¶æ€ä¿¡æ¯ï¼Œä¸æ¸²æŸ“æ¨¡æ¿"""
    return jsonify({
        'status': 'running',
        'message': 'AI Future Trade Backend API',
        'version': __version__,
        'frontend_url': 'http://localhost:3000',
        'api_endpoint': '/api/'
    })

@app.route('/lib/<path:filename>')
def serve_lib_file(filename):
    """Serve files from static/lib/ directory"""
    from flask import send_from_directory
    import os
    lib_path = os.path.join(app.root_path, 'static', 'lib')
    return send_from_directory(lib_path, filename)

# ============ Provider API Endpoints ============

@app.route('/api/providers', methods=['GET'])
def get_providers():
    """Get all API providers"""
    providers = db.get_all_providers()
    return jsonify(providers)

@app.route('/api/providers', methods=['POST'])
def add_provider():
    """Add new API provider"""
    data = request.json
    try:
        provider_id = db.add_provider(
            name=data['name'],
            api_url=data['api_url'],
            api_key=data['api_key'],
            models=data.get('models', ''),
            provider_type=data.get('provider_type', 'openai')
        )
        return jsonify({'id': provider_id, 'message': 'Provider added successfully'})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/providers/<int:provider_id>', methods=['DELETE', 'OPTIONS'])
def delete_provider(provider_id):
    """Delete API provider"""
    # å¤„ç† OPTIONS é¢„æ£€è¯·æ±‚
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Methods', 'DELETE, OPTIONS')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        return response
    
    try:
        db.delete_provider(provider_id)
        logger.info(f"Provider {provider_id} deleted successfully")
        return jsonify({'success': True, 'message': 'Provider deleted successfully'})
    except Exception as e:
        logger.error(f"Failed to delete provider {provider_id}: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/providers/models', methods=['POST'])
def fetch_provider_models():
    """Fetch available models from provider's API"""
    data = request.json
    api_url = data.get('api_url')
    api_key = data.get('api_key')

    if not api_url or not api_key:
        return jsonify({'error': 'API URL and key are required'}), 400

    try:
        models = []

        # Try to detect provider type and call appropriate API
        if 'openai.com' in api_url.lower():
            import requests
            headers = {
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            }
            response = requests.get(f'{api_url}/models', headers=headers, timeout=10)
            if response.status_code == 200:
                result = response.json()
                models = [m['id'] for m in result.get('data', []) if 'gpt' in m['id'].lower()]
        elif 'deepseek' in api_url.lower():
            import requests
            headers = {
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            }
            response = requests.get(f'{api_url}/models', headers=headers, timeout=10)
            if response.status_code == 200:
                result = response.json()
                models = [m['id'] for m in result.get('data', [])]
        else:
            # Default: return common model names
            models = ['gpt-3.5-turbo', 'gpt-4', 'gpt-4-turbo']

        return jsonify({'models': models})
    except Exception as e:
        logger.error(f"Fetch models failed: {e}")
        return jsonify({'error': f'Failed to fetch models: {str(e)}'}), 500

# ============ Futures Configuration API Endpoints ============

@app.route('/api/futures', methods=['GET'])
def list_futures():
    """Get all futures configurations"""
    try:
        futures = db.get_futures()
        return jsonify(futures)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/futures', methods=['POST'])
def add_future_config():
    """Add new future configuration"""
    data = request.json or {}
    symbol = data.get('symbol', '').strip().upper()
    contract_symbol = data.get('contract_symbol', '').strip().upper()
    name = data.get('name', '').strip()
    exchange = data.get('exchange', 'BINANCE_FUTURES').strip().upper()
    link = (data.get('link') or '').strip()
    sort_order = data.get('sort_order')

    if not all([symbol, contract_symbol, name]):
        return jsonify({'error': 'symbol, contract_symbol, name are required'}), 400

    try:
        sort_order = int(sort_order)
    except (TypeError, ValueError):
        sort_order = 0

    try:
        future_id = db.add_future(
            symbol=symbol,
            contract_symbol=contract_symbol,
            name=name,
            exchange=exchange,
            link=link or None,
            sort_order=sort_order
        )
        return jsonify({'id': future_id, 'message': 'Future added successfully'})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/futures/<int:future_id>', methods=['DELETE', 'OPTIONS'])
def delete_future_config(future_id):
    """Delete future configuration"""
    # å¤„ç† OPTIONS é¢„æ£€è¯·æ±‚
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Methods', 'DELETE, OPTIONS')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        return response
    
    try:
        db.delete_future(future_id)
        logger.info(f"Future {future_id} deleted successfully")
        return jsonify({'success': True, 'message': 'Future deleted successfully'})
    except Exception as e:
        logger.error(f"Failed to delete future {future_id}: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500

# ============ Model API Endpoints ============

@app.route('/api/models', methods=['GET'])
def get_models():
    """Get all trading models"""
    models = db.get_all_models()
    return jsonify(models)

@app.route('/api/models', methods=['POST'])
def add_model():
    """Add new trading model"""
    data = request.json or {}
    try:
        provider = db.get_provider(data['provider_id'])
        if not provider:
            return jsonify({'error': 'Provider not found'}), 404

        model_id = db.add_model(
            name=data['name'],
            provider_id=data['provider_id'],
            model_name=data['model_name'],
            initial_capital=float(data.get('initial_capital', 100000)),
            leverage=int(data.get('leverage', 10))
        )

        model = db.get_model(model_id)
        provider = db.get_provider(model['provider_id'])
        if not provider:
            return jsonify({'error': 'Provider not found'}), 404

        trading_engines[model_id] = TradingEngine(
            model_id=model_id,
            db=db,
            market_fetcher=market_fetcher,
            ai_trader=AITrader(
                provider_type=provider['provider_type'],
                api_key=provider['api_key'],
                api_url=provider['api_url'],
                model_name=model['model_name']
            ),
            trade_fee_rate=TRADE_FEE_RATE
        )
        logger.info(f"Model {model_id} ({data['name']}) initialized")

        return jsonify({'id': model_id, 'message': 'Model added successfully'})

    except Exception as e:
        logger.error(f"Failed to add model: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/models/<int:model_id>', methods=['DELETE', 'OPTIONS'])
def delete_model(model_id):
    """Delete trading model"""
    # å¤„ç† OPTIONS é¢„æ£€è¯·æ±‚
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Methods', 'DELETE, OPTIONS')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        return response
    
    try:
        model = db.get_model(model_id)
        model_name = model['name'] if model else f"ID-{model_id}"

        db.delete_model(model_id)
        if model_id in trading_engines:
            del trading_engines[model_id]

        logger.info(f"Model {model_id} ({model_name}) deleted")
        return jsonify({'success': True, 'message': 'Model deleted successfully'})
    except Exception as e:
        logger.error(f"Delete model {model_id} failed: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/models/<int:model_id>/portfolio', methods=['GET'])
def get_portfolio(model_id):
    """Get model portfolio data"""
    model = db.get_model(model_id)
    if not model:
        return jsonify({'error': f'Model {model_id} not found'}), 404

    symbols = get_tracked_symbols()
    prices_data = market_fetcher.get_prices(symbols)
    current_prices = {symbol: data['price'] for symbol, data in prices_data.items()}

    try:
        portfolio = db.get_portfolio(model_id, current_prices)
    except ValueError as exc:
        return jsonify({'error': str(exc)}), 404

    account_value = db.get_account_value_history(model_id, limit=100)

    return jsonify({
        'portfolio': portfolio,
        'account_value_history': account_value,
        'auto_trading_enabled': bool(model.get('auto_trading_enabled', 1)),
        'leverage': model.get('leverage', 10)
    })

@app.route('/api/models/<int:model_id>/trades', methods=['GET'])
def get_trades(model_id):
    """Get model trade history"""
    limit = request.args.get('limit', 50, type=int)
    trades = db.get_trades(model_id, limit=limit)
    return jsonify(trades)

@app.route('/api/models/<int:model_id>/conversations', methods=['GET'])
def get_conversations(model_id):
    """Get model conversation history"""
    limit = request.args.get('limit', 20, type=int)
    conversations = db.get_conversations(model_id, limit=limit)
    return jsonify(conversations)

@app.route('/api/models/<int:model_id>/prompts', methods=['GET'])
def get_model_prompts(model_id):
    """Get model prompt configuration"""
    model = db.get_model(model_id)
    if not model:
        return jsonify({'error': 'Model not found'}), 404

    prompt_config = db.get_model_prompt(model_id) or {}
    buy_prompt = prompt_config.get('buy_prompt') or DEFAULT_BUY_CONSTRAINTS
    sell_prompt = prompt_config.get('sell_prompt') or DEFAULT_SELL_CONSTRAINTS

    return jsonify({
        'model_id': model_id,
        'model_name': model.get('name'),
        'buy_prompt': buy_prompt,
        'sell_prompt': sell_prompt,
        'has_custom': bool(prompt_config),
        'updated_at': prompt_config.get('updated_at') if prompt_config else None
    })

@app.route('/api/models/<int:model_id>/prompts', methods=['PUT'])
def update_model_prompts(model_id):
    """Update model prompt configuration"""
    model = db.get_model(model_id)
    if not model:
        return jsonify({'error': 'Model not found'}), 404

    data = request.json or {}
    buy_prompt = data.get('buy_prompt')
    sell_prompt = data.get('sell_prompt')

    success = db.upsert_model_prompt(model_id, buy_prompt, sell_prompt)
    if not success:
        return jsonify({'error': 'Failed to update prompts'}), 500

    return jsonify({'success': True, 'message': 'Prompts updated successfully'})

@app.route('/api/models/<int:model_id>/leverage', methods=['POST'])
def update_model_leverage(model_id):
    """Update model leverage"""
    data = request.json or {}
    if 'leverage' not in data:
        return jsonify({'error': 'leverage is required'}), 400

    model = db.get_model(model_id)
    if not model:
        return jsonify({'error': 'Model not found'}), 404

    leverage = int(data.get('leverage', 0))
    leverage = max(0, leverage)
    if not db.set_model_leverage(model_id, leverage):
        return jsonify({'error': 'Failed to update leverage'}), 500

    return jsonify({'model_id': model_id, 'leverage': leverage})

@app.route('/api/models/<int:model_id>/execute', methods=['POST'])
def execute_trading(model_id):
    """Execute trading cycle for a model"""
    if model_id not in trading_engines:
        engine, error = init_trading_engine_for_model(model_id)
        if error:
            return jsonify({'error': error}), 404
    else:
        engine = trading_engines[model_id]

    # Manual execution enables auto trading
    db.set_model_auto_trading(model_id, True)

    try:
        result = engine.execute_trading_cycle()
        result['auto_trading_enabled'] = True
        return jsonify(result)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/models/<int:model_id>/auto-trading', methods=['POST'])
def set_model_auto_trading(model_id):
    """Enable or disable auto trading for a model"""
    data = request.json or {}
    if 'enabled' not in data:
        return jsonify({'error': 'enabled flag is required'}), 400

    model = db.get_model(model_id)
    if not model:
        return jsonify({'error': 'Model not found'}), 404

    enabled = bool(data.get('enabled'))
    success = db.set_model_auto_trading(model_id, enabled)
    if not success:
        return jsonify({'error': 'Failed to update model status'}), 500

    if enabled and model_id not in trading_engines:
        init_trading_engine_for_model(model_id)

    return jsonify({'model_id': model_id, 'auto_trading_enabled': enabled})

@app.route('/api/aggregated/portfolio', methods=['GET'])
def get_aggregated_portfolio():
    """Get aggregated portfolio data across all models"""
    symbols = get_tracked_symbols()
    prices_data = market_fetcher.get_current_prices(symbols)
    current_prices = {symbol: data['price'] for symbol, data in prices_data.items()}

    models = db.get_all_models()
    total_portfolio = {
        'total_value': 0,
        'cash': 0,
        'positions_value': 0,
        'realized_pnl': 0,
        'unrealized_pnl': 0,
        'initial_capital': 0,
        'positions': []
    }

    all_positions = {}

    for model in models:
        portfolio = db.get_portfolio(model['id'], current_prices)
        if portfolio:
            total_portfolio['total_value'] += portfolio.get('total_value', 0)
            total_portfolio['cash'] += portfolio.get('cash', 0)
            total_portfolio['positions_value'] += portfolio.get('positions_value', 0)
            total_portfolio['realized_pnl'] += portfolio.get('realized_pnl', 0)
            total_portfolio['unrealized_pnl'] += portfolio.get('unrealized_pnl', 0)
            total_portfolio['initial_capital'] += portfolio.get('initial_capital', 0)

            # Aggregate positions by future and side
            for pos in portfolio.get('positions', []):
                key = f"{pos['future']}_{pos['side']}"
                if key not in all_positions:
                    all_positions[key] = {
                        'future': pos['future'],
                        'side': pos['side'],
                        'quantity': 0,
                        'avg_price': 0,
                        'total_cost': 0,
                        'leverage': pos['leverage'],
                        'current_price': pos['current_price'],
                        'pnl': 0
                    }

                # Weighted average calculation
                current_pos = all_positions[key]
                current_cost = current_pos['quantity'] * current_pos['avg_price']
                new_cost = pos['quantity'] * pos['avg_price']
                total_quantity = current_pos['quantity'] + pos['quantity']

                if total_quantity > 0:
                    current_pos['avg_price'] = (current_cost + new_cost) / total_quantity
                    current_pos['quantity'] = total_quantity
                    current_pos['total_cost'] = current_cost + new_cost
                    current_pos['pnl'] = (pos['current_price'] - current_pos['avg_price']) * total_quantity

    total_portfolio['positions'] = list(all_positions.values())
    chart_data = db.get_multi_model_chart_data(limit=100)

    return jsonify({
        'portfolio': total_portfolio,
        'chart_data': chart_data,
        'model_count': len(models)
    })

# ============ Market Data API Endpoints ============

@app.route('/api/market/prices', methods=['GET'])
def get_market_prices():
    """Get current market prices for both configured futures and model positions"""
    # è·å–é…ç½®çš„åˆçº¦
    configured_symbols = get_tracked_symbols()
    configured_prices = market_fetcher.get_prices(configured_symbols)
    
    # ä¸ºé…ç½®çš„åˆçº¦æ·»åŠ æ¥æºæ ‡è®°
    for symbol in configured_prices:
        configured_prices[symbol]['source'] = 'configured'
    
    # è·å–æ‰€æœ‰æ¨¡å‹çš„æŒä»“åˆçº¦
    models = db.get_all_models()
    position_symbols = set()
    for model in models:
        try:
            portfolio = db.get_portfolio(model['id'], {})
            for pos in portfolio.get('positions', []):
                if pos.get('future'):
                    position_symbols.add(pos['future'])
        except Exception:
            continue
    
    # è·å–æŒä»“åˆçº¦çš„ä»·æ ¼æ•°æ®ï¼ˆæ’é™¤å·²é…ç½®çš„åˆçº¦ï¼Œé¿å…é‡å¤ï¼‰
    position_symbols = [s for s in position_symbols if s not in configured_symbols]
    if position_symbols:
        position_prices = market_fetcher.get_prices(position_symbols)
        # ä¸ºæŒä»“åˆçº¦æ·»åŠ æ¥æºæ ‡è®°
        for symbol in position_prices:
            position_prices[symbol]['source'] = 'position'
        # åˆå¹¶æ•°æ®
        configured_prices.update(position_prices)
    
    return jsonify(configured_prices)

@app.route('/api/market/indicators/<symbol>', methods=['GET'])
def get_market_indicators(symbol):
    """Get technical indicators for a specific symbol
    
    ä»å¸å®‰APIå®æ—¶è·å–å¹¶è®¡ç®—æŠ€æœ¯æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
    - Kçº¿æ•°æ®ï¼ˆå¼€é«˜ä½æ”¶ã€æˆäº¤é‡ï¼‰
    - MAå‡çº¿ï¼ˆ5ã€20ã€60ã€99å‘¨æœŸï¼‰
    - MACDæŒ‡æ ‡ï¼ˆDIFã€DEAã€BARï¼‰
    - RSIæŒ‡æ ‡ï¼ˆRSI6ã€RSI9ï¼‰
    - æˆäº¤é‡ï¼ˆVOLï¼‰
    
    æ—¶é—´æ¡†æ¶ï¼š1å‘¨ã€1å¤©ã€4å°æ—¶ã€1å°æ—¶ã€15åˆ†é’Ÿã€5åˆ†é’Ÿã€1åˆ†é’Ÿ
    
    Args:
        symbol: äº¤æ˜“å¯¹ç¬¦å·ï¼ˆå¦‚ 'BTC'ï¼‰
        
    Returns:
        æŠ€æœ¯æŒ‡æ ‡æ•°æ®å­—å…¸ï¼Œæ ¼å¼ï¼š{'timeframes': {1w: {...}, 1d: {...}, ...}}
    """
    try:
        indicators = market_fetcher.calculate_technical_indicators(symbol)
        if not indicators:
            return jsonify({
                'symbol': symbol,
                'timeframes': {},
                'error': 'æ— æ³•è·å–æŠ€æœ¯æŒ‡æ ‡æ•°æ®'
            }), 200
        
        return jsonify({
            'symbol': symbol,
            **indicators
        })
    except Exception as e:
        logger.error(f"[API] Failed to get indicators for {symbol}: {e}", exc_info=True)
        return jsonify({
            'symbol': symbol,
            'timeframes': {},
            'error': str(e)
        }), 500

@app.route('/api/market/leaderboard', methods=['GET'])
def get_market_leaderboard():
    """Get market leaderboard data
    
    è¿”å›å®Œæ•´çš„æ¶¨è·Œå¹…æ¦œæ•°æ®ï¼š
    - gainers: æ¶¨å¹…æ¦œTOP 10ï¼ˆæŒ‰æ¶¨å¹…ä»é«˜åˆ°ä½æ’åºï¼‰
    - losers: è·Œå¹…æ¦œTOP 10ï¼ˆæŒ‰è·Œå¹…ä»ä½åˆ°é«˜æ’åºï¼Œè·Œå¹…ä¸ºè´Ÿå€¼ï¼‰
    
    å‰ç«¯é€šè¿‡è½®è¯¢æ­¤æ¥å£è·å–æ•°æ®ï¼Œæ•´ä½“åˆ·æ–°æ¸²æŸ“
    """
    limit = request.args.get('limit', type=int) or 10  # é»˜è®¤10æ¡ï¼Œæ¶¨10ä¸ªï¼Œè·Œ10ä¸ª
    force = request.args.get('force', default=0, type=int)
    
    try:
        # è·å–æ¶¨è·Œå¹…æ¦œæ•°æ®ï¼ˆæ¶¨10ä¸ªï¼Œè·Œ10ä¸ªï¼‰
        data = market_fetcher.sync_leaderboard(force=bool(force), limit=limit)
        
        # ç¡®ä¿è¿”å›å®Œæ•´æ•°æ®æ ¼å¼
        result = {
            'gainers': data.get('gainers', [])[:limit],  # ç¡®ä¿æœ€å¤šè¿”å›limitæ¡
            'losers': data.get('losers', [])[:limit],   # ç¡®ä¿æœ€å¤šè¿”å›limitæ¡
            'timestamp': int(datetime.now().timestamp() * 1000)  # æ·»åŠ æ—¶é—´æˆ³ï¼Œä¾¿äºå‰ç«¯åˆ¤æ–­æ•°æ®æ–°é²œåº¦
        }
        
        gainers_count = len(result['gainers'])
        losers_count = len(result['losers'])
        logger.debug(f"[API] æ¶¨è·Œå¹…æ¦œæ•°æ®è¿”å›: æ¶¨å¹…æ¦œ {gainers_count} æ¡, è·Œå¹…æ¦œ {losers_count} æ¡")
        
        return jsonify(result)
    except Exception as exc:
        logger.error(f"Failed to load leaderboard: {exc}", exc_info=True)
        return jsonify({'error': str(exc), 'gainers': [], 'losers': []}), 500

@app.route('/api/clickhouse/leaderboard/status', methods=['GET'])
def get_clickhouse_leaderboard_status():
    """Get ClickHouse leaderboard sync status
    
    è¿”å›çŠ¶æ€ä¿¡æ¯ï¼š
    - running: è¿è¡ŒçŠ¶æ€ï¼ˆTrueè¡¨ç¤ºè¿è¡Œä¸­ï¼ŒFalseè¡¨ç¤ºå·²åœæ­¢ï¼‰
    - thread_alive: çº¿ç¨‹æ˜¯å¦å­˜æ´»
    - é»˜è®¤çŠ¶æ€ä¸ºè¿è¡ŒçŠ¶æ€ï¼ˆrunning=Trueï¼‰
    """
    global clickhouse_leaderboard_running, clickhouse_leaderboard_thread
    
    # æ£€æŸ¥çº¿ç¨‹å®é™…çŠ¶æ€ï¼Œå¦‚æœçº¿ç¨‹ä¸å­˜åœ¨æˆ–å·²æ­»äº¡ï¼Œä½†ç”¨æˆ·æ²¡æœ‰æ˜ç¡®åœæ­¢ï¼Œåˆ™è®¤ä¸ºæ˜¯è¿è¡ŒçŠ¶æ€
    thread_alive = clickhouse_leaderboard_thread.is_alive() if clickhouse_leaderboard_thread else False
    
    # å¦‚æœçº¿ç¨‹å·²æ­»äº¡ä½†è¿è¡ŒçŠ¶æ€ä¸ºTrueï¼Œè¯´æ˜çº¿ç¨‹æ„å¤–é€€å‡ºï¼Œä½†ç”¨æˆ·æœŸæœ›è¿è¡Œ
    # è¿™ç§æƒ…å†µä¸‹ï¼Œè¿”å›running=Trueï¼Œè®©å‰ç«¯æ˜¾ç¤ºè¿è¡ŒçŠ¶æ€ï¼Œç›‘æ§çº¿ç¨‹ä¼šè‡ªåŠ¨é‡å¯
    actual_running = clickhouse_leaderboard_running or (not clickhouse_leaderboard_stop_event.is_set() and thread_alive)
    
    return jsonify({
        'running': actual_running,
        'thread_alive': thread_alive
    })

@app.route('/api/clickhouse/leaderboard/control', methods=['POST'])
def control_clickhouse_leaderboard():
    """Control ClickHouse leaderboard sync (start/stop)"""
    data = request.json or {}
    action = data.get('action', '').lower()
    
    if action == 'start':
        start_clickhouse_leaderboard_sync()
        return jsonify({'message': 'ClickHouse leaderboard sync started', 'running': True})
    elif action == 'stop':
        stop_clickhouse_leaderboard_sync()
        return jsonify({'message': 'ClickHouse leaderboard sync stopped', 'running': False})

@app.route('/api/market/klines', methods=['GET'])
def get_market_klines():
    """è·å–Kçº¿å†å²æ•°æ®
    
    å‚æ•°:
        symbol: äº¤æ˜“å¯¹ç¬¦å·ï¼ˆå¦‚ 'BTCUSDT'ï¼‰
        interval: æ—¶é—´é—´éš”ï¼ˆ'1m', '5m', '15m', '1h', '4h', '1d', '1w'ï¼‰
        limit: è¿”å›çš„æœ€å¤§è®°å½•æ•°ï¼Œé»˜è®¤å€¼æ ¹æ®intervalä¸åŒï¼š
               - 1dï¼ˆ1å¤©ï¼‰ï¼šé»˜è®¤120æ¡ï¼Œæœ€å¤§120æ¡
               - 1wï¼ˆ1å‘¨ï¼‰ï¼šé»˜è®¤20æ¡ï¼Œæœ€å¤§20æ¡
               - å…¶ä»–intervalï¼šé»˜è®¤500æ¡ï¼Œæœ€å¤§500æ¡
        start_time: å¼€å§‹æ—¶é—´ï¼ˆå¯é€‰ï¼ŒISOæ ¼å¼å­—ç¬¦ä¸²ï¼‰
        end_time: ç»“æŸæ—¶é—´ï¼ˆå¯é€‰ï¼ŒISOæ ¼å¼å­—ç¬¦ä¸²ï¼‰
    """
    try:
        from datetime import datetime
        from common.config import KLINE_DATA_SOURCE
        
        symbol = request.args.get('symbol', '').upper()
        interval = request.args.get('interval', '5m')
        # æ ¹æ®æ•°æ®æºè®¾ç½®ä¸åŒçš„é»˜è®¤limit
        source = KLINE_DATA_SOURCE  # ä»é…ç½®æ–‡ä»¶è·å–æ•°æ®æºï¼Œä¸å†ä»è¯·æ±‚å‚æ•°è·å–
        
        # æ ¹æ®ä¸åŒçš„intervalè®¾ç½®ä¸åŒçš„é»˜è®¤limit
        # 1dï¼ˆ1å¤©ï¼‰ï¼š120æ¡ï¼ˆçº¦4ä¸ªæœˆå†å²æ•°æ®ï¼‰
        # 1wï¼ˆ1å‘¨ï¼‰ï¼š20æ¡ï¼ˆçº¦5ä¸ªæœˆå†å²æ•°æ®ï¼‰
        # å…¶ä»–intervalï¼š500æ¡
        interval_default_limits = {
            '1d': 120,  # 1å¤©å‘¨æœŸï¼Œé»˜è®¤120æ¡
            '1w': 20,   # 1å‘¨å‘¨æœŸï¼Œé»˜è®¤20æ¡
        }
        default_limit = interval_default_limits.get(interval, 500)  # å…¶ä»–å‘¨æœŸé»˜è®¤500æ¡
        
        limit = request.args.get('limit', type=int) or default_limit
        start_time_str = request.args.get('start_time')
        end_time_str = request.args.get('end_time')
        
        if not symbol:
            return jsonify({'error': 'symbol parameter is required'}), 400
        
        # éªŒè¯interval
        valid_intervals = ['1m', '5m', '15m', '1h', '4h', '1d', '1w']
        if interval not in valid_intervals:
            return jsonify({'error': f'invalid interval. Must be one of: {valid_intervals}'}), 400
        
        # è§£ææ—¶é—´å‚æ•°
        start_time = None
        end_time = None
        start_timestamp = None
        end_timestamp = None
        
        if start_time_str:
            try:
                start_time = datetime.fromisoformat(start_time_str.replace('Z', '+00:00'))
                start_timestamp = int(start_time.timestamp() * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
            except ValueError:
                return jsonify({'error': 'invalid start_time format. Use ISO format'}), 400
        
        if end_time_str:
            try:
                end_time = datetime.fromisoformat(end_time_str.replace('Z', '+00:00'))
                end_timestamp = int(end_time.timestamp() * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
            except ValueError:
                return jsonify({'error': 'invalid end_time format. Use ISO format'}), 400
        
        # è·å–å®¢æˆ·ç«¯IPåœ°å€
        client_ip = request.remote_addr
        
        # æŸ¥è¯¢Kçº¿æ•°æ®ï¼Œæ·»åŠ å®¢æˆ·ç«¯IPä¿¡æ¯
        logger.info(f"[API] è·å–Kçº¿å†å²æ•°æ®è¯·æ±‚: symbol={symbol}, interval={interval}, limit={limit}, source={source}, start_time={start_time_str}, end_time={end_time_str}, client_ip={client_ip}")
        
        klines = []
        
        if source == 'db':
            # ä»æ•°æ®åº“è·å–æ•°æ®
            from common.database_clickhouse import ClickHouseDatabase
            logger.info(f"[API] ä»æ•°æ®åº“è·å–Kçº¿æ•°æ®: symbol={symbol}, interval={interval}")
            clickhouse_db = ClickHouseDatabase(auto_init_tables=False)
            klines = clickhouse_db.get_market_klines(
                symbol=symbol,
                interval=interval,
                limit=limit,
                start_time=start_time,
                end_time=end_time
            )
        else:
            # ä»SDKè·å–æ•°æ®ï¼ˆé»˜è®¤ï¼‰
            # ä½¿ç”¨å…¨å±€market_fetcherå˜é‡ï¼Œè€Œéé‡æ–°å¯¼å…¥
            
            # SDKæ¨¡å¼ä¸‹æ ¹æ®ä¸åŒçš„intervalè®¾ç½®ä¸åŒçš„æœ€å¤§limit
            # 1dï¼ˆ1å¤©ï¼‰ï¼šæœ€å¤§120æ¡
            # 1wï¼ˆ1å‘¨ï¼‰ï¼šæœ€å¤§20æ¡
            # å…¶ä»–intervalï¼šæœ€å¤§500æ¡
            interval_max_limits = {
                '1d': 120,  # 1å¤©å‘¨æœŸï¼Œæœ€å¤§120æ¡
                '1w': 20,   # 1å‘¨å‘¨æœŸï¼Œæœ€å¤§20æ¡
            }
            max_limit = interval_max_limits.get(interval, 500)  # å…¶ä»–å‘¨æœŸæœ€å¤§500æ¡
            
            sdk_limit = limit
            if sdk_limit > max_limit:
                sdk_limit = max_limit
                logger.debug(f"[API] SDKæ¨¡å¼ä¸‹é™åˆ¶limitä¸º{max_limit}ï¼ˆinterval={interval}ï¼‰ï¼ŒåŸè¯·æ±‚limit={limit}")
            
            logger.info(f"[API] ä»SDKè·å–Kçº¿æ•°æ®: symbol={symbol}, interval={interval}, limit={sdk_limit}")
            
            # è°ƒç”¨SDKè·å–Kçº¿æ•°æ®ï¼ˆåªä¼ å…¥endTimeï¼Œä¸ä¼ å…¥startTimeï¼‰
            klines_raw = market_fetcher._futures_client.get_klines(
                symbol=symbol,
                interval=interval,
                limit=sdk_limit,
                startTime=start_timestamp,  # å¦‚æœæä¾›äº†startTimeï¼Œä¹Ÿä¼ å…¥
                endTime=end_timestamp  # åªä¼ å…¥endTimeï¼ˆæˆ–ä¼ å…¥çš„endTimeï¼‰
            )
            
            if not klines_raw or len(klines_raw) == 0:
                logger.warning(f"[API] SDKæœªè¿”å›Kçº¿æ•°æ®: symbol={symbol}, interval={interval}")
                klines = []
            else:
                # SDKè¿”å›çš„æ•°æ®æ˜¯å€’åºçš„ï¼ˆä»æ–°åˆ°æ—§ï¼‰ï¼Œæ•°ç»„[0]æ˜¯æœ€æ–°çš„Kçº¿ï¼Œæ•°ç»„[-1]æ˜¯æœ€æ—§çš„Kçº¿
                # æ³¨æ„ï¼šKçº¿é¡µé¢å·²æ”¹ä¸ºä»…ä½¿ç”¨å†å²æ•°æ®ï¼Œä¸å†è®¢é˜…å®æ—¶Kçº¿æ›´æ–°ï¼Œå› æ­¤ä¿ç•™æ‰€æœ‰æ•°æ®ï¼ˆåŒ…æ‹¬æœ€æ–°Kçº¿ï¼‰
                logger.debug(f"[API] SDKè¿”å›{len(klines_raw)}æ¡Kçº¿æ•°æ®ï¼ˆå€’åºï¼šæœ€æ–°â†’æœ€æ—§ï¼‰ï¼Œä¿ç•™æ‰€æœ‰æ•°æ®ï¼ˆåŒ…æ‹¬æœ€æ–°Kçº¿ï¼‰")
                
                # è½¬æ¢SDKè¿”å›æ•°æ®ä¸ºç»Ÿä¸€æ ¼å¼ï¼Œä»·æ ¼ä¿ç•™6ä½å°æ•°
                formatted_klines = []
                for kline in klines_raw:
                    # è·å–åŸå§‹ä»·æ ¼æ•°æ®ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–æ•°å­—ï¼‰
                    raw_open = kline.get('open', 0)
                    raw_high = kline.get('high', 0)
                    raw_low = kline.get('low', 0)
                    raw_close = kline.get('close', 0)
                    
                    # è½¬æ¢ä¸ºæµ®ç‚¹æ•°å¹¶ä¿ç•™6ä½å°æ•°
                    formatted_open = round(float(raw_open) if raw_open else 0.0, 6)
                    formatted_high = round(float(raw_high) if raw_high else 0.0, 6)
                    formatted_low = round(float(raw_low) if raw_low else 0.0, 6)
                    formatted_close = round(float(raw_close) if raw_close else 0.0, 6)
                    
                    formatted_klines.append({
                        'timestamp': kline.get('open_time', 0),
                        'open': formatted_open,
                        'high': formatted_high,
                        'low': formatted_low,
                        'close': formatted_close,
                        'volume': float(kline.get('volume', 0)),
                        'turnover': float(kline.get('quote_asset_volume', 0))
                    })
                
                # ç”±äºSDKè¿”å›çš„æ•°æ®æ˜¯å€’åºçš„ï¼ˆä»æ–°åˆ°æ—§ï¼‰ï¼Œéœ€è¦æŒ‰timestampå‡åºæ’åºï¼ˆä»æ—§åˆ°æ–°ï¼‰
                # ç¡®ä¿ä¸æ•°æ®åº“æ¨¡å¼å’Œå‰ç«¯æœŸæœ›çš„æ•°æ®é¡ºåºä¸€è‡´
                # å‰ç«¯Kçº¿å›¾è¡¨ä»å·¦åˆ°å³æ˜¾ç¤ºï¼Œå·¦è¾¹æ˜¯æœ€æ—§çš„æ•°æ®ï¼Œå³è¾¹æ˜¯æœ€æ–°çš„æ•°æ®ï¼Œæ‰€ä»¥éœ€è¦ä»æ—§åˆ°æ–°çš„é¡ºåº
                formatted_klines.sort(key=lambda x: x.get('timestamp', 0))
                klines = formatted_klines
                
                logger.info(f"[API] SDKæŸ¥è¯¢å®Œæˆï¼Œå…±è·å– {len(klines)} æ¡Kçº¿æ•°æ®ï¼ˆå·²æ’åºä¸ºä»æ—§åˆ°æ–°ï¼ŒåŒ…å«æœ€æ–°Kçº¿ï¼‰")
                
                # éªŒè¯æ•°æ®é¡ºåºï¼šç¡®ä¿ç¬¬ä¸€æ¡æ—¶é—´æˆ³å°äºæœ€åä¸€æ¡æ—¶é—´æˆ³ï¼ˆä»æ—§åˆ°æ–°ï¼Œtimestampå‡åºï¼‰
                # å‰ç«¯Kçº¿å›¾è¡¨ä»å·¦åˆ°å³æ˜¾ç¤ºï¼Œå·¦è¾¹æ˜¯æœ€æ—§çš„æ•°æ®ï¼ˆç¬¬ä¸€æ¡ï¼‰ï¼Œå³è¾¹æ˜¯æœ€æ–°çš„æ•°æ®ï¼ˆæœ€åä¸€æ¡ï¼‰
                # æ‰€ä»¥æ•°æ®é¡ºåºåº”è¯¥æ˜¯ï¼šç¬¬ä¸€æ¡ï¼ˆæœ€æ—§ï¼‰< æœ€åä¸€æ¡ï¼ˆæœ€æ–°ï¼‰
                if len(klines) > 1:
                    first_timestamp = klines[0].get('timestamp', 0)
                    last_timestamp = klines[-1].get('timestamp', 0)
                    
                    # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºdatetimeæ ¼å¼ä¾¿äºæ’æŸ¥
                    def format_timestamp_for_validation(ts):
                        """å°†timestampï¼ˆæ¯«ç§’ï¼‰è½¬æ¢ä¸ºdatetimeå­—ç¬¦ä¸²ç”¨äºéªŒè¯"""
                        if ts == 0 or ts is None:
                            return 'N/A'
                        try:
                            from datetime import timezone as tz
                            dt = datetime.fromtimestamp(ts / 1000, tz=tz.utc)
                            return dt.strftime('%Y-%m-%d %H:%M:%S UTC')
                        except (ValueError, TypeError, OSError) as e:
                            return f'{ts} (è½¬æ¢å¤±è´¥: {e})'
                    
                    first_timestamp_dt = format_timestamp_for_validation(first_timestamp)
                    last_timestamp_dt = format_timestamp_for_validation(last_timestamp)
                    
                    if first_timestamp >= last_timestamp:
                        logger.warning(
                            f"[API] âš ï¸ æ•°æ®é¡ºåºå¼‚å¸¸ï¼šç¬¬ä¸€æ¡æ—¶é—´æˆ³({first_timestamp}, {first_timestamp_dt}) >= "
                            f"æœ€åä¸€æ¡({last_timestamp}, {last_timestamp_dt})ï¼Œ"
                            f"é‡æ–°æ’åºä»¥ç¡®ä¿ä»æ—§åˆ°æ–°çš„é¡ºåºï¼ˆä¸å‰ç«¯Kçº¿å›¾è¡¨ä»å·¦åˆ°å³çš„è¦æ±‚ä¸€è‡´ï¼‰"
                        )
                        klines.sort(key=lambda x: x.get('timestamp', 0))
                        # é‡æ–°éªŒè¯
                        first_timestamp = klines[0].get('timestamp', 0)
                        last_timestamp = klines[-1].get('timestamp', 0)
                        first_timestamp_dt = format_timestamp_for_validation(first_timestamp)
                        last_timestamp_dt = format_timestamp_for_validation(last_timestamp)
                        logger.debug(
                            f"[API] âœ“ é‡æ–°æ’åºåï¼šç¬¬ä¸€æ¡æ—¶é—´æˆ³={first_timestamp} ({first_timestamp_dt}), "
                            f"æœ€åä¸€æ¡æ—¶é—´æˆ³={last_timestamp} ({last_timestamp_dt})"
                        )
                    else:
                        logger.debug(
                            f"[API] âœ“ æ•°æ®é¡ºåºéªŒè¯é€šè¿‡ï¼šç¬¬ä¸€æ¡æ—¶é—´æˆ³={first_timestamp} ({first_timestamp_dt}) < "
                            f"æœ€åä¸€æ¡æ—¶é—´æˆ³={last_timestamp} ({last_timestamp_dt}) "
                            f"ï¼ˆä»æ—§åˆ°æ–°ï¼Œç¬¦åˆå‰ç«¯Kçº¿å›¾è¡¨ä»å·¦åˆ°å³çš„æ˜¾ç¤ºè¦æ±‚ï¼‰"
                        )
        
        # è®°å½•è¿”å›æ•°æ®ä¿¡æ¯ï¼Œæ·»åŠ å®¢æˆ·ç«¯IP
        klines_count = len(klines) if klines else 0
        logger.info(f"[API] è·å–Kçº¿å†å²æ•°æ®æŸ¥è¯¢å®Œæˆ: symbol={symbol}, interval={interval}, source={source}, è¿”å›æ•°æ®æ¡æ•°={klines_count}, client_ip={client_ip}")
        
        if klines_count > 0:
            # è®°å½•ç¬¬ä¸€æ¡å’Œæœ€åä¸€æ¡æ•°æ®çš„æ—¶é—´æˆ³ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            first_kline = klines[0]
            last_kline = klines[-1]
            first_timestamp = first_kline.get('timestamp', 'N/A')
            last_timestamp = last_kline.get('timestamp', 'N/A')
            
            # å°†timestampè½¬æ¢ä¸ºdatetimeæ ¼å¼ä¾¿äºæ’æŸ¥
            def format_timestamp(ts):
                """å°†timestampï¼ˆæ¯«ç§’ï¼‰è½¬æ¢ä¸ºdatetimeå­—ç¬¦ä¸²"""
                if ts == 'N/A' or ts is None:
                    return 'N/A'
                try:
                    # timestampæ˜¯æ¯«ç§’æ—¶é—´æˆ³ï¼Œéœ€è¦é™¤ä»¥1000
                    from datetime import timezone as tz
                    dt = datetime.fromtimestamp(ts / 1000, tz=tz.utc)
                    return dt.strftime('%Y-%m-%d %H:%M:%S UTC')
                except (ValueError, TypeError, OSError) as e:
                    return f'{ts} (è½¬æ¢å¤±è´¥: {e})'
            
            first_timestamp_dt = format_timestamp(first_timestamp)
            last_timestamp_dt = format_timestamp(last_timestamp)
            
            logger.info(
                f"[API] è·å–Kçº¿å†å²æ•°æ®æ—¶é—´èŒƒå›´: "
                f"ç¬¬ä¸€æ¡timestamp={first_timestamp} ({first_timestamp_dt}), "
                f"æœ€åä¸€æ¡timestamp={last_timestamp} ({last_timestamp_dt}), "
                f"å…±è¿”å›{klines_count}æ¡æ•°æ®, client_ip={client_ip}"
            )
            
            # è®°å½•ç¬¬ä¸€æ¡æ•°æ®çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•æ•°æ®æ ¼å¼ï¼‰
            logger.debug(f"[API] è·å–Kçº¿å†å²æ•°æ®ç¤ºä¾‹ï¼ˆç¬¬ä¸€æ¡ï¼‰: {first_kline}")
            logger.debug(f"[API] è·å–Kçº¿å†å²æ•°æ®ç¤ºä¾‹ï¼ˆæœ€åä¸€æ¡ï¼‰: {last_kline}")
        else:
            logger.warning(f"[API]  æœªæ‰¾åˆ°Kçº¿å†å²æ•°æ®: symbol={symbol}, interval={interval}, client_ip={client_ip}")
        
        response_data = {
            'symbol': symbol,
            'interval': interval,
            'source': source,
            'data': klines,
            'count': klines_count  # æ·»åŠ æ•°æ®æ¡æ•°å­—æ®µï¼Œä¾¿äºå‰ç«¯è°ƒè¯•
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"[API] è·å–Kçº¿æ•°æ®å¤±è´¥: symbol={symbol}, interval={interval}, source={source}, error={e}", exc_info=True)
        return jsonify({'error': str(e)}), 500

@socketio.on('leaderboard:request')
def handle_leaderboard_request(payload=None):
    """WebSocket handler for leaderboard requests (å·²åºŸå¼ƒï¼Œå‰ç«¯å·²æ”¹ä¸ºè½®è¯¢æ–¹å¼)
    
    æ³¨æ„ï¼šæ¶¨è·Œå¹…æ¦œå·²æ”¹ä¸ºå‰ç«¯è½®è¯¢æ–¹å¼è·å–æ•°æ®ï¼Œä¸å†é€šè¿‡WebSocketæ¨é€ã€‚
    æ­¤handlerä¿ç•™ä»¥å…¼å®¹æ—§ç‰ˆæœ¬å‰ç«¯ï¼Œä½†å»ºè®®å‰ç«¯ä½¿ç”¨ /api/market/leaderboard APIæ¥å£ã€‚
    """
    payload = payload or {}
    limit = payload.get('limit', 10)
    
    logger.warning(f"[Leaderboard Request] WebSocket leaderboard:request å·²åºŸå¼ƒï¼Œå»ºè®®ä½¿ç”¨ /api/market/leaderboard APIæ¥å£ï¼ˆè½®è¯¢æ–¹å¼ï¼‰")
    
    try:
        # è·å–æ¶¨è·Œæ¦œæ•°æ®
        data = market_fetcher.sync_leaderboard(force=False, limit=limit)
        
        # å‘é€æ•°æ®æ›´æ–°äº‹ä»¶ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬å‰ç«¯ï¼‰
        emit('leaderboard:update', data)
        logger.debug(f"[Leaderboard Request] Leaderboard update emitted to client (å…¼å®¹æ¨¡å¼)")
        
    except Exception as exc:
        logger.error(f"[Leaderboard Request] Failed to fetch leaderboard data: limit={limit}, error={str(exc)}", exc_info=True)
        emit('leaderboard:error', {'message': str(exc)})

@socketio.on('klines:subscribe')
def handle_klines_subscribe(payload=None):
    """WebSocket handler for Kçº¿è®¢é˜…è¯·æ±‚
    
    å‚æ•°:
        symbol: äº¤æ˜“å¯¹ç¬¦å·ï¼ˆå¦‚ 'BTCUSDT'ï¼‰
        interval: æ—¶é—´é—´éš”ï¼ˆ'1m', '5m', '15m', '1h', '4h', '1d', '1w'ï¼‰
    """
    payload = payload or {}
    symbol = payload.get('symbol', '').upper()
    interval = payload.get('interval', '5m')
    
    # è®°å½•å‡½æ•°è°ƒç”¨ä¿¡æ¯
    logger.info(f"[KLine Subscribe] Received subscription request: payload={payload}, symbol={symbol}, interval={interval}")
    
    if not symbol:
        logger.warning(f"[KLine Subscribe] Subscription failed: symbol is required, payload={payload}")
        emit('klines:error', {'message': 'symbol is required'})
        return
    
    valid_intervals = ['1m', '5m', '15m', '1h', '4h', '1d', '1w']
    if interval not in valid_intervals:
        logger.warning(f"[KLine Subscribe] Subscription failed: invalid interval '{interval}', must be one of {valid_intervals}")
        emit('klines:error', {'message': f'invalid interval. Must be one of: {valid_intervals}'})
        return
    
    # åŠ å…¥æˆ¿é—´ï¼ˆæŒ‰symbolå’Œintervalåˆ†ç»„ï¼‰
    room = f'klines:{symbol}:{interval}'
    from flask_socketio import join_room
    join_room(room)
    logger.debug(f"[KLine Subscribe] Client joined room: {room}")
    
    # è®°å½•è®¢é˜…å‰åçš„è®¢é˜…æ•°é‡
    with kline_push_lock:
        previous_count = len(kline_subscriptions)
        kline_subscriptions[room] = {
            'symbol': symbol,
            'interval': interval,
            'last_update_time': datetime.now()
        }
        current_count = len(kline_subscriptions)
    
    logger.info(f"[KLine Subscribe] Subscription added: room={room}, symbol={symbol}, interval={interval}, " \
                f"subscriptions_count: {previous_count} â†’ {current_count}")
    
    # å¯åŠ¨æ¨é€å·¥ä½œçº¿ç¨‹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰å¯åŠ¨ï¼‰
    start_kline_push_worker()
    logger.debug(f"[KLine Subscribe] Push worker started/checked")
    
    # å‘é€è®¢é˜…æˆåŠŸäº‹ä»¶
    emit('klines:subscribed', {'symbol': symbol, 'interval': interval})
    logger.info(f"[KLine Subscribe] Subscription completed: symbol={symbol}, interval={interval}, room={room}, " \
                f"total_subscriptions={current_count}")

@socketio.on('klines:unsubscribe')
def handle_klines_unsubscribe(payload=None):
    """WebSocket handler for Kçº¿å–æ¶ˆè®¢é˜…"""
    payload = payload or {}
    symbol = payload.get('symbol', '').upper()
    interval = payload.get('interval', '5m')
    
    # è®°å½•å‡½æ•°è°ƒç”¨ä¿¡æ¯
    logger.info(f"[KLine Unsubscribe] Received unsubscribe request: payload={payload}, symbol={symbol}, interval={interval}")
    
    room = f'klines:{symbol}:{interval}'
    from flask_socketio import leave_room
    
    # è®°å½•å®¢æˆ·ç«¯ç¦»å¼€æˆ¿é—´ä¿¡æ¯
    logger.debug(f"[KLine Unsubscribe] Client leaving room: {room}")
    leave_room(room)
    
    # ç§»é™¤è®¢é˜…ä¿¡æ¯
    with kline_push_lock:
        previous_count = len(kline_subscriptions)
        was_subscribed = room in kline_subscriptions
        
        if was_subscribed:
            del kline_subscriptions[room]
            current_count = len(kline_subscriptions)
            logger.info(f"[KLine Unsubscribe] Subscription removed: room={room}, symbol={symbol}, interval={interval}, subscriptions_count: {previous_count} â†’ {current_count}")
        else:
            current_count = previous_count
            logger.warning(f"[KLine Unsubscribe] Room not found in subscriptions: room={room}, symbol={symbol}, interval={interval}")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ´»è·ƒè®¢é˜…ï¼Œå¦‚æœæ²¡æœ‰åˆ™å…³é—­æ¨é€çº¿ç¨‹
        if not kline_subscriptions:
            kline_push_stop_event.set()
            logger.info("[KLine Unsubscribe] No active KLine subscriptions, stopping push thread")
    
    # è®°å½•å–æ¶ˆè®¢é˜…å®Œæˆä¿¡æ¯
    logger.info(f"[KLine Unsubscribe] Client unsubscribed from klines: symbol={symbol}, interval={interval}, room={room}, was_subscribed={was_subscribed}")
    emit('klines:unsubscribed', {'symbol': symbol, 'interval': interval})

# Kçº¿å®æ—¶æ¨é€ç›¸å…³å˜é‡
kline_subscriptions = {}  # å­˜å‚¨è®¢é˜…ä¿¡æ¯: {room: {symbol, interval, last_update_time}}
kline_push_thread = None
kline_push_stop_event = threading.Event()
kline_push_lock = threading.Lock()

def push_realtime_kline(symbol: str, interval: str):
    """æ¨é€å®æ—¶Kçº¿æ•°æ®åˆ°è®¢é˜…çš„å®¢æˆ·ç«¯"""
    try:
        if not market_fetcher._futures_client:
            return
        
        # è·å–æœ€æ–°Kçº¿æ•°æ®
        contract_symbol = market_fetcher._futures_client.format_symbol(symbol)
        klines = market_fetcher._futures_client.get_klines(
            contract_symbol,
            interval,
            limit=1
        )
        
        if klines and len(klines) > 0:
            latest_kline = klines[-1]
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            kline_data = {
                'timestamp': int(latest_kline.get('close_time', latest_kline.get('open_time', 0))),
                'open': float(latest_kline.get('open', 0)),
                'high': float(latest_kline.get('high', 0)),
                'low': float(latest_kline.get('low', 0)),
                'close': float(latest_kline.get('close', 0)),
                'volume': float(latest_kline.get('volume', 0)),
                'turnover': float(latest_kline.get('quote_asset_volume', 0))
            }
            
            # æ¨é€åˆ°è®¢é˜…çš„æˆ¿é—´
            room = f'klines:{contract_symbol}:{interval}'
            socketio.emit('klines:update', {
                'symbol': contract_symbol,
                'interval': interval,
                'kline': kline_data
            }, room=room)
            
    except Exception as e:
        logger.error(f"Failed to push realtime kline for {symbol} {interval}: {e}", exc_info=True)

def _kline_push_loop():
    """åå°å¾ªç¯ä»»åŠ¡ï¼šå®šæœŸæ¨é€å®æ—¶Kçº¿æ•°æ®åˆ°è®¢é˜…çš„å®¢æˆ·ç«¯"""
    global kline_push_thread
    thread_id = threading.current_thread().ident
    logger.info(f"[KLine Push Worker-{thread_id}] Kçº¿å®æ—¶æ¨é€å¾ªç¯å¯åŠ¨")
    
    # æ ¹æ®æœ€å°å‘¨æœŸï¼ˆ1mï¼‰è®¾ç½®æ¨é€é—´éš”
    push_interval = 5  # æ¯5ç§’æ¨é€ä¸€æ¬¡
    
    while not kline_push_stop_event.is_set():
        try:
            with kline_push_lock:
                # è·å–æ‰€æœ‰æ´»è·ƒçš„è®¢é˜…
                active_subscriptions = dict(kline_subscriptions)
            
            if not active_subscriptions:
                # æ²¡æœ‰è®¢é˜…ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
                kline_push_stop_event.wait(push_interval)
                continue
            
            # éå†æ‰€æœ‰è®¢é˜…å¹¶æ¨é€æ•°æ®
            for room, subscription_info in active_subscriptions.items():
                try:
                    symbol = subscription_info.get('symbol')
                    interval = subscription_info.get('interval')
                    
                    if symbol and interval:
                        push_realtime_kline(symbol, interval)
                except Exception as e:
                    logger.error(f"[KLine Push Worker] Error pushing kline for {room}: {e}", exc_info=True)
            
            # ç­‰å¾…æŒ‡å®šé—´éš”
            kline_push_stop_event.wait(push_interval)
            
        except Exception as e:
            logger.error(f"[KLine Push Worker-{thread_id}] Error in push loop: {e}", exc_info=True)
            kline_push_stop_event.wait(push_interval)
    
    logger.info(f"[KLine Push Worker-{thread_id}] Kçº¿å®æ—¶æ¨é€å¾ªç¯åœæ­¢")
    
    # çº¿ç¨‹é€€å‡ºæ—¶é‡ç½®çŠ¶æ€ï¼Œç¡®ä¿ä¸‹æ¬¡èƒ½æ­£ç¡®å¯åŠ¨
    with kline_push_lock:
        kline_push_stop_event.clear()
        kline_push_thread = None

def start_kline_push_worker():
    """å¯åŠ¨Kçº¿å®æ—¶æ¨é€å·¥ä½œçº¿ç¨‹"""
    global kline_push_thread
    if kline_push_thread and kline_push_thread.is_alive():
        return
    kline_push_stop_event.clear()
    kline_push_thread = threading.Thread(target=_kline_push_loop, daemon=True, name="KLinePushWorker")
    kline_push_thread.start()
    logger.info("[KLine Push] Kçº¿å®æ—¶æ¨é€å·¥ä½œçº¿ç¨‹å·²å¯åŠ¨")

# ============ Settings API Endpoints ============

@app.route('/api/settings', methods=['GET'])
def get_settings():
    """Get system settings"""
    try:
        settings = db.get_settings()
        return jsonify(settings)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/settings', methods=['PUT'])
def update_settings():
    """Update system settings"""
    try:
        data = request.json or {}
        trading_frequency_minutes = int(data.get('trading_frequency_minutes', 60))
        trading_fee_rate = float(data.get('trading_fee_rate', 0.001))
        show_system_prompt = 1 if data.get('show_system_prompt') in (True, 1, '1', 'true', 'True') else 0

        success = db.update_settings(
            trading_frequency_minutes,
            trading_fee_rate,
            show_system_prompt
        )

        if success:
            return jsonify({'success': True, 'message': 'Settings updated successfully'})
        else:
            return jsonify({'success': False, 'error': 'Failed to update settings'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ============ Version API Endpoints ============

@app.route('/api/version', methods=['GET'])
def get_version():
    """Get current version information"""
    return jsonify({
        'current_version': __version__
    })

@app.route('/api/check-update', methods=['GET'])
def check_update():
    """Check for application updates"""
    try:
        return jsonify({
            'update_available': False,
            'current_version': __version__,
            'latest_version': __version__,
            'error': None
        })
    except Exception as e:
        logger.error(f"Check update failed: {e}")
        return jsonify({
            'update_available': False,
            'current_version': __version__,
            'latest_version': __version__,
            'error': str(e)
        }), 500

# ============ Main Entry Point ============

if __name__ == '__main__':
    logger.info("\n" + "=" * 60)
    logger.info("AIFutureTrade Backend Service - Starting...")
    logger.info("=" * 60)
    logger.info("Initializing database...")

    # Initialize database and trading engines within application context
    with app.app_context():
        db.init_db()
        logger.info("Database initialized")
        logger.info("Initializing trading engines...")
        init_trading_engines()
        logger.info("Trading engines initialized")

    # Start background threads
    if auto_trading:
        trading_thread = threading.Thread(target=trading_loop, daemon=True)
        trading_thread.start()
        logger.info("Auto-trading enabled")

    # Start leaderboard workers
    logger.info("ğŸš€ å‡†å¤‡å¯åŠ¨æ¶¨è·Œå¹…æ¦œç›¸å…³å·¥ä½œçº¿ç¨‹...")
    
    # å¯åŠ¨å‰ç«¯æ¨é€å·¥ä½œçº¿ç¨‹
    logger.info("ğŸ“¡ å¯åŠ¨æ¶¨è·Œå¹…æ¦œå‰ç«¯æ¨é€çº¿ç¨‹...")
    start_leaderboard_worker()
    logger.info("âœ… æ¶¨è·Œå¹…æ¦œå‰ç«¯æ¨é€çº¿ç¨‹å·²å¯åŠ¨")
    
    # åˆå§‹åŒ–åå°æœåŠ¡ï¼ˆåŒ…æ‹¬ClickHouseæ¶¨è·Œå¹…æ¦œåŒæ­¥çº¿ç¨‹ï¼‰
    logger.info("ğŸ“Š åˆå§‹åŒ–åå°æœåŠ¡...")
    _init_background_services()
    
    logger.info("âœ… æ‰€æœ‰æ¶¨è·Œå¹…æ¦œç›¸å…³å·¥ä½œçº¿ç¨‹å·²å¯åŠ¨å®Œæˆ")

    logger.info("\n" + "=" * 60)
    logger.info("AIFutureTrade Backend Service is running!")
    logger.info("API Server: http://0.0.0.0:5002")
    logger.info("WebSocket Server: ws://0.0.0.0:5002")
    logger.info("=" * 60 + "\n")

    # å¼€å‘ç¯å¢ƒï¼šä½¿ç”¨WerkzeugæœåŠ¡å™¨
    # ç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨gunicorn + eventletï¼ˆè§Dockerfileå’Œgunicorn_config.pyï¼‰
    # é€šè¿‡ç¯å¢ƒå˜é‡USE_GUNICORN=trueæ¥ä½¿ç”¨gunicornå¯åŠ¨
    if os.getenv('USE_GUNICORN') == 'true':
        logger.info("Production mode: Use 'gunicorn --config gunicorn_config.py app:app' to start")
        # ç”Ÿäº§ç¯å¢ƒåº”è¯¥ä½¿ç”¨gunicornå¯åŠ¨ï¼Œè¿™é‡Œåªæ˜¯æç¤º
        socketio.run(
            app, 
            debug=False, 
            host='0.0.0.0', 
            port=5002, 
            use_reloader=False,
            allow_unsafe_werkzeug=True
        )
    else:
        # å¼€å‘ç¯å¢ƒ
        socketio.run(
            app, 
            debug=False, 
            host='0.0.0.0', 
            port=5002, 
            use_reloader=False,
            allow_unsafe_werkzeug=True
        )
