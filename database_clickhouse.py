"""ClickHouse database utilities for market data storage."""
from __future__ import annotations

import logging
import threading
import time
from datetime import datetime, timezone, timedelta
from queue import Queue, Empty
from typing import Any, Dict, Iterable, List, Optional, Callable, Tuple

import clickhouse_connect
import config as app_config

MARKET_TICKER_TABLE = "24_market_tickers"
LEADERBOARD_TABLE = "futures_leaderboard"
MARKET_KLINES_TABLE = "market_klines"

logger = logging.getLogger(__name__)


class ClickHouseConnectionPool:
    """ClickHouse connection pool to manage client instances.
    
    This class manages a pool of ClickHouse client instances to avoid creating
    too many connections to the ClickHouse server. It provides methods to acquire
    and release connections, and supports dynamic expansion up to a maximum
    number of connections.
    """
    
    def __init__(
        self,
        host: str,
        port: int,
        username: str,
        password: str,
        database: str,
        secure: bool,
        min_connections: int = 5,
        max_connections: int = 20,
        connection_timeout: int = 30
    ):
        """Initialize the connection pool.
        
        Args:
            host: ClickHouse host
            port: ClickHouse port
            username: ClickHouse username
            password: ClickHouse password
            database: ClickHouse database
            secure: Whether to use secure connection
            min_connections: Minimum number of connections to keep in the pool
            max_connections: Maximum number of connections allowed in the pool
            connection_timeout: Timeout for acquiring a connection (seconds)
        """
        self._host = host
        self._port = port
        self._username = username
        self._password = password
        self._database = database
        self._secure = secure
        self._min_connections = min_connections
        self._max_connections = max_connections
        self._connection_timeout = connection_timeout
        
        # Create a queue to hold the connections
        self._pool = Queue(maxsize=max_connections)
        
        # Create a lock to protect the connection count
        self._lock = threading.Lock()
        
        # Current number of connections in the pool
        self._current_connections = 0
        
        # Initialize the pool with min_connections
        self._initialize_pool()
    
    def _initialize_pool(self):
        """åˆå§‹åŒ–è¿æ¥æ± ï¼Œåˆ›å»ºæœ€å°æ•°é‡çš„è¿æ¥ã€‚
        
        è¯¥æ–¹æ³•åœ¨è¿æ¥æ± åˆå§‹åŒ–æ—¶è°ƒç”¨ï¼Œæ ¹æ®é…ç½®çš„æœ€å°è¿æ¥æ•°åˆ›å»ºå¹¶æ·»åŠ è¿æ¥åˆ°æ± ä¸­ã€‚
        """
        for _ in range(self._min_connections):
            self._create_connection()
    
    def _create_connection(self) -> Optional[Any]:
        """åˆ›å»ºä¸€ä¸ªæ–°çš„ClickHouseå®¢æˆ·ç«¯è¿æ¥ã€‚
        
        Returns:
            æˆåŠŸæ—¶è¿”å›ClickHouseå®¢æˆ·ç«¯å®ä¾‹ï¼Œå¤±è´¥æˆ–è¾¾åˆ°æœ€å¤§è¿æ¥æ•°æ—¶è¿”å›None
        """
        with self._lock:
            if self._current_connections >= self._max_connections:
                return None
            
            try:
                client = clickhouse_connect.get_client(
                    host=self._host,
                    port=self._port,
                    username=self._username,
                    password=self._password,
                    database=self._database,
                    secure=self._secure
                )
                self._pool.put(client)
                self._current_connections += 1
                logger.debug(f"[ClickHouse] Created new connection. Current connections: {self._current_connections}")
                return client
            except Exception as e:
                logger.error(f"[ClickHouse] Failed to create connection: {e}")
                return None
    
    def acquire(self, timeout: Optional[int] = None) -> Optional[Any]:
        """Acquire a connection from the pool.
        
        Args:
            timeout: Timeout for acquiring a connection (seconds). If None, use the default timeout.
            
        Returns:
            A ClickHouse client instance, or None if no connection is available within the timeout.
        """
        timeout = timeout or self._connection_timeout
        
        try:
            # Try to get a connection from the pool
            client = self._pool.get(timeout=timeout)
            logger.debug(f"[ClickHouse] Acquired connection from pool")
            return client
        except Empty:
            # If the pool is empty, try to create a new connection
            logger.debug(f"[ClickHouse] Pool is empty, creating new connection")
            client = self._create_connection()
            if client:
                return client
            
            # If we can't create a new connection, try again to get from the pool
            try:
                client = self._pool.get(timeout=timeout)
                logger.debug(f"[ClickHouse] Acquired connection from pool after waiting")
                return client
            except Empty:
                logger.error(f"[ClickHouse] Failed to acquire connection within timeout {timeout} seconds")
                return None
    
    def release(self, client: Any) -> None:
        """Release a connection back to the pool.
        
        Args:
            client: The ClickHouse client instance to release
        """
        try:
            # Check if the client is still valid by sending a simple query
            client.query("SELECT 1")
            self._pool.put(client)
            logger.debug(f"[ClickHouse] Released connection back to pool")
        except Exception as e:
            # If the client is invalid, close it and don't return it to the pool
            logger.error(f"[ClickHouse] Connection is invalid, closing it: {e}")
            with self._lock:
                self._current_connections -= 1
    
    def close_all(self) -> None:
        """Close all connections in the pool."""
        with self._lock:
            while not self._pool.empty():
                try:
                    client = self._pool.get_nowait()
                    client.close()
                    self._current_connections -= 1
                except Exception as e:
                    logger.error(f"[ClickHouse] Failed to close connection: {e}")
            
            logger.info(f"[ClickHouse] Closed all connections. Current connections: {self._current_connections}")


class ClickHouseDatabase:
    """Encapsulates ClickHouse connectivity and CRUD helpers."""
    
    # ç±»çº§åˆ«çš„é”ï¼Œç”¨äºé˜²æ­¢å¹¶å‘æ‰§è¡Œ sync_leaderboard
    _sync_leaderboard_lock = threading.Lock()

    # ==================================================================
    # åˆå§‹åŒ–å’Œè¿æ¥ç®¡ç†
    # ==================================================================
    
    def __init__(self, *, auto_init_tables: bool = True) -> None:
        # Create a connection pool instead of individual client instances
        self._pool = ClickHouseConnectionPool(
            host=app_config.CLICKHOUSE_HOST,
            port=app_config.CLICKHOUSE_PORT,
            username=app_config.CLICKHOUSE_USER,
            password=app_config.CLICKHOUSE_PASSWORD,
            database=app_config.CLICKHOUSE_DATABASE,
            secure=app_config.CLICKHOUSE_SECURE,
            min_connections=5,
            max_connections=50,
            connection_timeout=30
        )
        
        self.market_ticker_table = MARKET_TICKER_TABLE
        self.leaderboard_table = getattr(app_config, 'CLICKHOUSE_LEADERBOARD_TABLE', LEADERBOARD_TABLE)

        # Kçº¿è¡¨å‰ç¼€ï¼ˆé»˜è®¤ market_klinesï¼‰ï¼ŒæŒ‰ä¸åŒ interval æ‹†åˆ†ä¸ºå¤šå¼ è¡¨ï¼š
        # market_klines_1w, market_klines_1d, market_klines_4h, market_klines_1h,
        # market_klines_15m, market_klines_5m, market_klines_1m
        self.market_klines_table: str = getattr(
            app_config,
            "CLICKHOUSE_MARKET_KLINES_TABLE",
            MARKET_KLINES_TABLE,
        )
        prefix = self.market_klines_table
        self.market_klines_tables: Dict[str, str] = {
            "1w": f"{prefix}_1w",
            "1d": f"{prefix}_1d",
            "4h": f"{prefix}_4h",
            "1h": f"{prefix}_1h",
            "15m": f"{prefix}_15m",
            "5m": f"{prefix}_5m",
            "1m": f"{prefix}_1m",
        }
        
        if auto_init_tables:
            # Initialize tables using the connection pool
            self.ensure_market_ticker_table()
            self.ensure_leaderboard_table()
            self.ensure_market_klines_table()
    
    # ==================================================================
    # è¿æ¥ç®¡ç†æ–¹æ³•
    # ==================================================================
    
    def _with_connection(self, func: Callable, *args, **kwargs) -> Any:
        """Execute a function with a ClickHouse connection from the pool.
        
        This method acquires a connection from the pool, executes the given function
        with the connection as the first argument, and then releases the connection back to the pool.
        
        Args:
            func: The function to execute
            *args: Positional arguments to pass to the function
            **kwargs: Keyword arguments to pass to the function
            
        Returns:
            The result of the function call
        """
        client = self._pool.acquire()
        if not client:
            raise Exception("Failed to acquire ClickHouse connection")
        
        try:
            return func(client, *args, **kwargs)
        finally:
            self._pool.release(client)

    # ==================================================================
    # é€šç”¨æ•°æ®åº“æ“ä½œæ–¹æ³•
    # ==================================================================
    
    def command(self, sql: str) -> None:
        """æ‰§è¡ŒåŸå§‹SQLå‘½ä»¤ã€‚
        
        Args:
            sql: è¦æ‰§è¡Œçš„SQLå‘½ä»¤å­—ç¬¦ä¸²
        """
        def _execute_command(client):
            client.command(sql)
        
        self._with_connection(_execute_command)
    
    def _check_table_exists(self, table_name: str) -> bool:
        """Check if a table exists in ClickHouse.
        
        Args:
            table_name: The name of the table to check
            
        Returns:
            True if the table exists, False otherwise
        """
        def _execute_check(client):
            try:
                # æŸ¥è¯¢ç³»ç»Ÿè¡¨æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                check_sql = f"""
                SELECT count() 
                FROM system.tables 
                WHERE database = currentDatabase() 
                AND name = '{table_name}'
                """
                result = client.query(check_sql)
                return result.result_rows[0][0] > 0 if result.result_rows else False
            except Exception as e:
                logger.warning(f"[ClickHouse] æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {e}")
                return False
        
        return self._with_connection(_execute_check)

    def insert_rows(
        self,
        table: str,
        rows: Iterable[Iterable[Any]],
        column_names: List[str],
    ) -> None:
        """å‘æŒ‡å®šè¡¨ä¸­æ’å…¥å¤šè¡Œæ•°æ®ã€‚
        
        Args:
            table: ç›®æ ‡è¡¨å
            rows: è¦æ’å…¥çš„æ•°æ®è¡Œé›†åˆï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªå€¼çš„é›†åˆ
            column_names: åˆ—ååˆ—è¡¨ï¼Œä¸æ•°æ®è¡Œä¸­çš„å€¼ä¸€ä¸€å¯¹åº”
        """
        payload = list(rows)
        if not payload:
            return
        
        def _execute_insert(client):
            client.insert(table, payload, column_names=column_names)
            logger.debug("[ClickHouse] Inserted %s rows into %s", len(payload), table)
        
        self._with_connection(_execute_insert)

    # ==================================================================
    # Market Ticker æ¨¡å—ï¼šè¡¨ç®¡ç†
    # ==================================================================
    
    def ensure_market_ticker_table(self) -> None:
        """Create the 24h market ticker table if it does not exist."""
        ddl = f"""
        CREATE TABLE IF NOT EXISTS {self.market_ticker_table} (
            event_time DateTime,
            symbol String,
            price_change Float64,
            price_change_percent Float64,
            side String,
            change_percent_text String,
            average_price Float64,
            last_price Float64,
            last_trade_volume Float64,
            open_price Float64,
            high_price Float64,
            low_price Float64,
            base_volume Float64,
            quote_volume Float64,
            stats_open_time DateTime,
            stats_close_time DateTime,
            first_trade_id UInt64,
            last_trade_id UInt64,
            trade_count UInt64,
            ingestion_time DateTime DEFAULT now(),
            update_price_date Nullable(DateTime)
        )
        ENGINE = MergeTree
        ORDER BY (symbol, stats_close_time, event_time)
        """
        self.command(ddl)
        logger.info("[ClickHouse] Ensured table %s exists", self.market_ticker_table)

    # ==================================================================
    # Market Ticker æ¨¡å—ï¼šæ•°æ®æŸ¥è¯¢
    # ==================================================================
    
    def get_existing_symbol_data(self, symbols: List[str]) -> Dict[str, Dict[str, Any]]:
        """è·å–æ•°æ®åº“ä¸­å·²å­˜åœ¨äº¤æ˜“å¯¹çš„æœ€æ–°æ•°æ®ï¼Œä¸»è¦ç”¨äºupsertæ“ä½œæ—¶è·å–å‚è€ƒä»·æ ¼ä¿¡æ¯ã€‚
        
        åŠŸèƒ½è¯´æ˜ï¼š
        1. æ‰¹é‡æŸ¥è¯¢æŒ‡å®šäº¤æ˜“å¯¹åˆ—è¡¨çš„æœ€æ–°è¡Œæƒ…è®°å½•
        2. ä¸ºæ¯ä¸ªäº¤æ˜“å¯¹è·å–open_priceã€last_priceå’Œupdate_price_dateå­—æ®µ
        3. æ™ºèƒ½å¤„ç†ä»·æ ¼æœªè®¾ç½®çš„ç‰¹æ®Šæƒ…å†µ
        4. è¿”å›æ ¼å¼åŒ–çš„å­—å…¸ï¼Œæ–¹ä¾¿upsert_market_tickersæ–¹æ³•å¿«é€ŸæŸ¥æ‰¾å’Œä½¿ç”¨
        
        æŸ¥è¯¢é€»è¾‘ï¼š
        - ä½¿ç”¨PARTITION BYå’ŒROW_NUMBER()çª—å£å‡½æ•°ä¸ºæ¯ä¸ªäº¤æ˜“å¯¹ç­›é€‰æœ€æ–°çš„ä¸€æ¡è®°å½•
        - æŒ‰event_timeé™åºæ’åºï¼Œç¡®ä¿è·å–æœ€æ–°æ•°æ®
        - åªæŸ¥è¯¢æŒ‡å®šçš„symbolåˆ—è¡¨ï¼Œæé«˜æŸ¥è¯¢æ•ˆç‡
        
        ç‰¹æ®Šå¤„ç†ï¼š
        - å…³é”®é€»è¾‘ï¼šå¦‚æœopen_priceä¸º0.0ä¸”update_price_dateä¸ºNoneï¼Œè§†ä¸º"æœªè®¾ç½®"çŠ¶æ€
        - è¿™ç§æƒ…å†µä¸‹è¿”å›open_price=Noneï¼Œè€Œä¸æ˜¯0.0ï¼Œä»¥ä¿æŒåŸæœ‰ä¸šåŠ¡é€»è¾‘çš„ä¸€è‡´æ€§
        
        é”™è¯¯å¤„ç†ï¼š
        - æŸ¥è¯¢å¤±è´¥æ—¶è¿”å›ç©ºå­—å…¸ï¼Œç¡®ä¿è°ƒç”¨æ–¹èƒ½å¤Ÿç»§ç»­æ‰§è¡Œ
        - è®°å½•è­¦å‘Šæ—¥å¿—ï¼Œä¾¿äºæ’æŸ¥é—®é¢˜
        
        Args:
            symbols: éœ€è¦æŸ¥è¯¢çš„äº¤æ˜“å¯¹åˆ—è¡¨ï¼Œæ ¼å¼å¦‚["BTCUSDT", "ETHUSDT"]
            
        Returns:
            åµŒå¥—å­—å…¸ï¼Œæ ¼å¼ä¸º{symbol: {open_price: float, last_price: float, update_price_date: datetime}}
            ç¤ºä¾‹: {"BTCUSDT": {"open_price": 35000.0, "last_price": 36000.0, "update_price_date": datetime(...)}}
            å½“äº¤æ˜“å¯¹ä¸å­˜åœ¨æˆ–open_priceæœªè®¾ç½®æ—¶ï¼Œå¯¹åº”å€¼å¯èƒ½ä¸ºNone
        """
        if not symbols:
            return {}
        
        try:
            symbols_str = "', '".join(symbols)
            query = f"""
            SELECT 
                symbol,
                open_price,
                last_price,
                update_price_date
            FROM (
                SELECT 
                    *,
                    ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY event_time DESC) as rn
                FROM {self.market_ticker_table}
                WHERE symbol IN ('{symbols_str}')
            ) AS ranked
            WHERE rn = 1
            """
            
            def _execute_query(client):
                return client.query(query)
            
            result = self._with_connection(_execute_query)
            
            symbol_data = {}
            for row in result.result_rows:
                symbol = row[0]
                open_price_raw = row[1]  # å¯èƒ½æ˜¯0.0æˆ–å®é™…ä»·æ ¼
                last_price = row[2] if row[2] is not None else None
                update_price_date = row[3] if len(row) > 3 else None
                
                # å…³é”®é€»è¾‘ï¼šå¦‚æœopen_priceä¸º0.0ä¸”update_price_dateä¸ºNoneï¼Œè§†ä¸º"æœªè®¾ç½®"
                # è¿”å›Noneä»¥ä¿æŒåŸæœ‰åˆ¤æ–­é€»è¾‘çš„æ­£ç¡®æ€§
                if open_price_raw == 0.0 and update_price_date is None:
                    open_price = None  # è§†ä¸ºæœªè®¾ç½®
                else:
                    open_price = open_price_raw if open_price_raw is not None else None
                
                symbol_data[symbol] = {
                    "open_price": open_price,
                    "last_price": last_price,
                    "update_price_date": update_price_date
                }
            
            return symbol_data
        except Exception as e:
            logger.warning("[ClickHouse] Failed to get existing symbol data: %s", e)
            return {}

    # ==================================================================
    # Market Ticker æ¨¡å—ï¼šæ•°æ®æ’å…¥å’Œæ›´æ–°
    # ==================================================================
    
    def insert_market_tickers(self, rows: Iterable[Dict[str, Any]]) -> None:
        """æ’å…¥å¸‚åœºè¡Œæƒ…æ•°æ®åˆ°market_tickerè¡¨ã€‚
        
        æ­¤æ–¹æ³•ä¼šå¯¹è¾“å…¥æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼ŒåŒ…æ‹¬ï¼š
        1. ç§»é™¤æ¥å£æ•°æ®ä¸­çš„open_priceå’Œupdate_price_dateå­—æ®µï¼ˆè¿™ä¸¤ä¸ªå­—æ®µåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°ï¼‰
        2. ç¡®ä¿æ—¥æœŸæ—¶é—´å­—æ®µæ ¼å¼æ­£ç¡®
        3. ä¸ºFloat64å­—æ®µè®¾ç½®é»˜è®¤å€¼0.0
        4. ä¸ºUInt64å­—æ®µè®¾ç½®é»˜è®¤å€¼0
        5. ä¸ºStringå­—æ®µè®¾ç½®é»˜è®¤å€¼ç©ºå­—ç¬¦ä¸²
        
        Args:
            rows: å¸‚åœºè¡Œæƒ…æ•°æ®å­—å…¸çš„å¯è¿­ä»£å¯¹è±¡
        """
        column_names = [
            "event_time",
            "symbol",
            "price_change",
            "price_change_percent",
            "side",
            "change_percent_text",
            "average_price",
            "last_price",
            "last_trade_volume",
            "open_price",
            "high_price",
            "low_price",
            "base_volume",
            "quote_volume",
            "stats_open_time",
            "stats_close_time",
            "first_trade_id",
            "last_trade_id",
            "trade_count",
            "update_price_date",
        ]

        prepared_rows: List[List[Any]] = []
        for row in rows:
            normalized = dict(row)
            
            # é‡è¦ï¼šç§»é™¤æ¥å£æ•°æ®ä¸­çš„ open_price å’Œ update_price_date å­—æ®µ
            # è¿™ä¸¤ä¸ªå­—æ®µåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°ï¼Œæ¥å£æ•°æ®ä¸èƒ½è¦†ç›–å®ƒä»¬
            if "open_price" in normalized:
                del normalized["open_price"]
                logger.debug("[ClickHouse] ç§»é™¤æ¥å£æ•°æ®ä¸­çš„ open_price å­—æ®µï¼ˆåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°ï¼‰")
            if "update_price_date" in normalized:
                del normalized["update_price_date"]
                logger.debug("[ClickHouse] ç§»é™¤æ¥å£æ•°æ®ä¸­çš„ update_price_date å­—æ®µï¼ˆåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°ï¼‰")
            
            normalized["event_time"] = _to_datetime(normalized.get("event_time"))
            normalized["stats_open_time"] = _to_datetime(normalized.get("stats_open_time"))
            normalized["stats_close_time"] = _to_datetime(normalized.get("stats_close_time"))
            
            # ç¡®ä¿æ‰€æœ‰Float64å­—æ®µä¸ä¸ºNoneï¼Œä½¿ç”¨0.0ä½œä¸ºé»˜è®¤å€¼
            # Float64å­—æ®µåˆ—è¡¨ï¼šprice_change, price_change_percent, average_price, last_price,
            # last_trade_volume, open_price, high_price, low_price, base_volume, quote_volume
            float64_fields = [
                "price_change", "price_change_percent", "average_price", "last_price",
                "last_trade_volume", "open_price", "high_price", "low_price",
                "base_volume", "quote_volume"
            ]
            for field in float64_fields:
                if normalized.get(field) is None:
                    normalized[field] = 0.0
            
            # ç¡®ä¿UInt64å­—æ®µä¸ä¸ºNone
            uint64_fields = ["first_trade_id", "last_trade_id", "trade_count"]
            for field in uint64_fields:
                if normalized.get(field) is None:
                    normalized[field] = 0
            
            # ç¡®ä¿Stringå­—æ®µä¸ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä½œä¸ºé»˜è®¤å€¼
            # Stringå­—æ®µï¼šside, change_percent_textï¼ˆè¡¨ç»“æ„ä¸­è¿™äº›å­—æ®µä¸æ˜¯Nullableï¼‰
            string_fields = ["side", "change_percent_text"]
            for field in string_fields:
                if normalized.get(field) is None:
                    normalized[field] = ""
            
            # DateTimeå­—æ®µå¤„ç†ï¼šç¡®ä¿æ‰€æœ‰éNullableçš„DateTimeå­—æ®µéƒ½ä¸ä¸ºNone
            # event_time, stats_open_time, stats_close_time å·²ç»åœ¨å‰é¢é€šè¿‡_to_datetimeå¤„ç†è¿‡äº†
            # ingestion_time å¦‚æœæ²¡æœ‰å€¼ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
            # update_price_date å¯ä»¥ä¸ºNoneï¼ˆå› ä¸ºè¡¨ç»“æ„ä¸­æ˜¯Nullable(DateTime)ï¼‰
            datetime_fields = ["event_time", "stats_open_time", "stats_close_time"]
            for field in datetime_fields:
                if normalized.get(field) is None:
                    normalized[field] = datetime.now(timezone.utc)
            
            # ingestion_time å­—æ®µå¤„ç†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if "ingestion_time" in normalized and normalized.get("ingestion_time") is None:
                normalized["ingestion_time"] = datetime.now(timezone.utc)
            
            # update_price_date å¯ä»¥ä¸ºNoneï¼ˆNullableå­—æ®µï¼‰
            normalized.setdefault("update_price_date", None)
            
            # å‡†å¤‡è¡Œæ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰å€¼
            row_data = []
            for name in column_names:
                value = normalized.get(name)
                # å¯¹äºéNullableçš„DateTimeå­—æ®µï¼Œç¡®ä¿ä¸ä¸ºNone
                if name in ["event_time", "stats_open_time", "stats_close_time", "ingestion_time"]:
                    if value is None:
                        value = datetime.now(timezone.utc)
                row_data.append(value)
            
            prepared_rows.append(row_data)

        self.insert_rows(self.market_ticker_table, prepared_rows, column_names)
        
    def upsert_market_tickers(self, rows: Iterable[Dict[str, Any]]) -> None:
        """æ›´æ–°æˆ–æ’å…¥å¸‚åœºè¡Œæƒ…æ•°æ®ï¼ˆupsertæ“ä½œï¼‰ã€‚
        
        åŠŸèƒ½è¯´æ˜ï¼š
        1. ç­›é€‰å‡ºä»¥USDTç»“å°¾çš„äº¤æ˜“å¯¹
        2. å¯¹äºåŒä¸€æ‰¹æ•°æ®ä¸­çš„é‡å¤symbolï¼Œåªä¿ç•™æœ€æ–°çš„ä¸€æ¡ï¼ˆåŸºäºstats_close_timeï¼‰
        3. æŸ¥è¯¢æ•°æ®åº“ä¸­å·²æœ‰çš„symbolæ•°æ®ï¼Œè·å–open_priceä¿¡æ¯
        4. æ ¹æ®å·²æœ‰open_priceè®¡ç®—æ¶¨è·Œå¹…ç›¸å…³å­—æ®µ
        5. æ‰§è¡Œæ‰¹é‡æ’å…¥æ“ä½œ
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        - é¦–æ¬¡æ’å…¥æ—¶ï¼Œprice_change, price_change_percent, side, change_percent_text, open_price éƒ½ä¸ºç©º
        - æ›´æ–°æ—¶ï¼Œå¦‚æœæ•°æ®åº“ä¸­open_priceæœ‰å€¼ï¼Œåˆ™é€šè¿‡last_priceå’Œopen_priceè®¡ç®—æ¶¨è·Œå¹…æŒ‡æ ‡
        - ä¿ç•™åŸæœ‰open_priceå’Œupdate_price_dateï¼Œè¿™ä¸¤ä¸ªå­—æ®µåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°
        
        æ•°æ®å¤„ç†ï¼š
        1. è§„èŒƒåŒ–æ—¶é—´å­—æ®µä¸ºdatetimeå¯¹è±¡
        2. ç§»é™¤æ¥å£æ•°æ®ä¸­çš„open_priceå’Œupdate_price_dateå­—æ®µï¼ˆä¿æŠ¤æœºåˆ¶ï¼‰
        3. æ™ºèƒ½å¤„ç†é‡å¤æ•°æ®ï¼Œç¡®ä¿æ¯ä¸ªsymbolåªä¿ç•™æœ€æ–°è®°å½•
        
        Args:
            rows: å¸‚åœºè¡Œæƒ…æ•°æ®çš„è¿­ä»£å™¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯åŒ…å«è¡Œæƒ…ä¿¡æ¯çš„å­—å…¸
        
        Returns:
            None
        """
        if not rows:
            return
            
        # Filter rows to only include symbols ending with "USDT"
        usdt_rows = [row for row in rows if row.get("symbol", "").endswith("USDT")]
        if not usdt_rows:
            logger.debug("[ClickHouse] No USDT symbols to upsert")
            return
            
        column_names = [
            "event_time",
            "symbol",
            "price_change",
            "price_change_percent",
            "side",
            "change_percent_text",
            "average_price",
            "last_price",
            "last_trade_volume",
            "open_price",
            "high_price",
            "low_price",
            "base_volume",
            "quote_volume",
            "stats_open_time",
            "stats_close_time",
            "first_trade_id",
            "last_trade_id",
            "trade_count",
            "update_price_date",
        ]

        # å¤„ç†æ•°æ®ï¼šå¯¹äºåŒä¸€æ‰¹æ•°æ®ä¸­çš„é‡å¤symbolï¼Œåªä¿ç•™æœ€æ–°çš„ä¸€æ¡ï¼ˆåŸºäºstats_close_timeï¼‰
        symbol_data_map: Dict[str, Dict[str, Any]] = {}
        
        for row in usdt_rows:
            normalized = dict(row)
            
            # é‡è¦ï¼šç§»é™¤æ¥å£æ•°æ®ä¸­çš„ open_price å’Œ update_price_date å­—æ®µ
            # è¿™ä¸¤ä¸ªå­—æ®µåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°ï¼Œæ¥å£æ•°æ®ä¸èƒ½è¦†ç›–å®ƒä»¬
            if "open_price" in normalized:
                del normalized["open_price"]
                logger.debug("[ClickHouse] ç§»é™¤æ¥å£æ•°æ®ä¸­çš„ open_price å­—æ®µï¼ˆåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°ï¼‰")
            if "update_price_date" in normalized:
                del normalized["update_price_date"]
                logger.debug("[ClickHouse] ç§»é™¤æ¥å£æ•°æ®ä¸­çš„ update_price_date å­—æ®µï¼ˆåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°ï¼‰")
            
            normalized["event_time"] = _to_datetime(normalized.get("event_time"))
            normalized["stats_open_time"] = _to_datetime(normalized.get("stats_open_time"))
            normalized["stats_close_time"] = _to_datetime(normalized.get("stats_close_time"))
            
            symbol = normalized.get("symbol")
            if not symbol:
                continue
            
            stats_close_time = normalized.get("stats_close_time")
            
            # å¦‚æœè¯¥symbolå·²å­˜åœ¨ï¼Œæ¯”è¾ƒstats_close_timeï¼Œåªä¿ç•™æœ€æ–°çš„
            if symbol in symbol_data_map:
                existing_stats_close_time = symbol_data_map[symbol].get("stats_close_time")
                if stats_close_time and existing_stats_close_time:
                    if stats_close_time > existing_stats_close_time:
                        symbol_data_map[symbol] = normalized
                elif stats_close_time:
                    # å½“å‰æ•°æ®æœ‰stats_close_timeï¼Œä¿ç•™å½“å‰æ•°æ®
                    symbol_data_map[symbol] = normalized
            else:
                symbol_data_map[symbol] = normalized
        
        if not symbol_data_map:
            return
        
        # æŸ¥è¯¢ç°æœ‰symbolçš„æ•°æ®ï¼Œè·å–open_price
        symbols_to_upsert = list(symbol_data_map.keys())
        existing_data = self.get_existing_symbol_data(symbols_to_upsert)
        
        # å‡†å¤‡æ’å…¥æ•°æ®ï¼ˆæ¯ä¸ªsymbolåªæœ‰ä¸€æ¡æœ€æ–°æ•°æ®ï¼‰
        prepared_rows: List[List[Any]] = []
        
        for symbol, normalized in symbol_data_map.items():
            # è·å–å½“å‰æŠ¥æ–‡çš„last_price
            try:
                current_last_price = float(normalized.get("last_price", 0))
            except (TypeError, ValueError):
                current_last_price = 0.0
            
            # åˆ¤æ–­æ˜¯æ’å…¥è¿˜æ˜¯æ›´æ–°
            existing_symbol_data = existing_data.get(symbol)
            existing_open_price = existing_symbol_data.get("open_price") if existing_symbol_data else None
            existing_update_price_date = existing_symbol_data.get("update_price_date") if existing_symbol_data else None
            
            # å…³é”®é€»è¾‘ï¼šåˆ¤æ–­open_priceæ˜¯å¦å·²è®¾ç½®
            # existing_open_priceä¸ºNoneè¡¨ç¤ºæœªè®¾ç½®ï¼ˆå³ä½¿æ•°æ®åº“ä¸­å­˜å‚¨çš„æ˜¯0.0ï¼Œå¦‚æœupdate_price_dateä¸ºNoneä¹Ÿè§†ä¸ºæœªè®¾ç½®ï¼‰
            # existing_open_priceä¸ä¸ºNoneä¸”ä¸ä¸º0è¡¨ç¤ºå·²è®¾ç½®ä¸”æœ‰æœ‰æ•ˆå€¼
            # å¦‚æœæ˜¯æ›´æ–°ä¸”open_priceæœ‰å€¼ï¼ˆä¸ä¸ºNoneä¸”ä¸ä¸º0ï¼‰ï¼Œåˆ™è®¡ç®—æ¶¨è·Œå¹…ç›¸å…³å­—æ®µ
            if existing_open_price is not None and existing_open_price != 0 and current_last_price != 0:
                try:
                    existing_open_price_float = float(existing_open_price)
                    current_last_price_float = float(current_last_price)
                    
                    # è®¡ç®— price_change = last_price - open_price
                    price_change = current_last_price_float - existing_open_price_float
                    
                    # è®¡ç®— price_change_percent = (last_price - open_price) / open_price * 100
                    price_change_percent = (price_change / existing_open_price_float) * 100
                    
                    # æ ¹æ®æ­£è´Ÿè®¾ç½® sideï¼ˆ0ä¸ºæ­£ï¼Œå³gainerï¼‰
                    side = "gainer" if price_change_percent >= 0 else "loser"
                    
                    # è®¾ç½® change_percent_text = price_change_percent + "%"
                    change_percent_text = f"{price_change_percent:.2f}%"
                    
                    # é‡è¦ï¼šä¿æŒåŸæœ‰çš„open_priceå’Œupdate_price_dateï¼Œä¸æ›´æ–°
                    # è¿™ä¸¤ä¸ªå­—æ®µåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°ï¼Œæ¥å£æ•°æ®ä¸èƒ½è¦†ç›–å®ƒä»¬
                    normalized["price_change"] = price_change
                    normalized["price_change_percent"] = price_change_percent
                    normalized["side"] = side
                    normalized["change_percent_text"] = change_percent_text
                    normalized["open_price"] = existing_open_price_float  # ä¿ç•™æ•°æ®åº“ä¸­çš„å€¼
                    normalized["update_price_date"] = existing_update_price_date  # ä¿ç•™æ•°æ®åº“ä¸­çš„å€¼ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
                except (TypeError, ValueError) as e:
                    logger.warning("[ClickHouse] Failed to calculate price change for symbol %s: %s", symbol, e)
                    # è®¡ç®—å¤±è´¥æ—¶ï¼Œè®¾ç½®ä¸º0.0ï¼ˆFloat64å­—æ®µä¸èƒ½ä¸ºNoneï¼‰
                    normalized["price_change"] = 0.0
                    normalized["price_change_percent"] = 0.0
                    normalized["side"] = ""  # Stringå­—æ®µä¸èƒ½ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                    normalized["change_percent_text"] = ""  # Stringå­—æ®µä¸èƒ½ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                    # é‡è¦ï¼šä¿ç•™æ•°æ®åº“ä¸­çš„open_priceå’Œupdate_price_date
                    normalized["open_price"] = existing_open_price_float if existing_open_price_float else 0.0
                    normalized["update_price_date"] = existing_update_price_date  # ä¿ç•™æ•°æ®åº“ä¸­çš„å€¼ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
            else:
                # ç¬¬ä¸€æ¬¡æ’å…¥æˆ–open_priceæœªè®¾ç½®çš„æƒ…å†µ
                # Float64å­—æ®µè®¾ç½®ä¸º0.0è€Œä¸æ˜¯Noneï¼ˆå› ä¸ºClickHouse Float64ä¸æ¥å—Noneï¼‰
                # ä½†é€»è¾‘ä¸Šï¼Œopen_price=0.0ä¸”update_price_date=Noneè¡¨ç¤º"æœªè®¾ç½®"
                # è¿™æ ·ä¸‹æ¬¡æŸ¥è¯¢æ—¶ï¼Œget_existing_symbol_dataä¼šè¿”å›open_price=Noneï¼Œä¿æŒåŸæœ‰åˆ¤æ–­é€»è¾‘æ­£ç¡®
                normalized["price_change"] = 0.0
                normalized["price_change_percent"] = 0.0
                normalized["side"] = ""  # Stringå­—æ®µä¸èƒ½ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                normalized["change_percent_text"] = ""  # Stringå­—æ®µä¸èƒ½ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                # é‡è¦ï¼šå¦‚æœæ˜¯æ›´æ–°æ“ä½œï¼Œä¿ç•™æ•°æ®åº“ä¸­çš„update_price_dateï¼›å¦‚æœæ˜¯æ’å…¥æ“ä½œï¼Œè®¾ç½®ä¸ºNone
                normalized["open_price"] = 0.0  # å­˜å‚¨ä¸º0.0ï¼Œä½†é€»è¾‘ä¸Šè§†ä¸º"æœªè®¾ç½®"ï¼ˆå› ä¸ºupdate_price_date=Noneï¼‰
                normalized["update_price_date"] = existing_update_price_date if existing_symbol_data else None
            
            # ç¡®ä¿æ‰€æœ‰Float64å­—æ®µä¸ä¸ºNoneï¼Œä½¿ç”¨0.0ä½œä¸ºé»˜è®¤å€¼
            # Float64å­—æ®µåˆ—è¡¨ï¼šprice_change, price_change_percent, average_price, last_price,
            # last_trade_volume, open_price, high_price, low_price, base_volume, quote_volume
            float64_fields = [
                "price_change", "price_change_percent", "average_price", "last_price",
                "last_trade_volume", "open_price", "high_price", "low_price",
                "base_volume", "quote_volume"
            ]
            for field in float64_fields:
                if normalized.get(field) is None:
                    normalized[field] = 0.0
            
            # ç¡®ä¿UInt64å­—æ®µä¸ä¸ºNone
            uint64_fields = ["first_trade_id", "last_trade_id", "trade_count"]
            for field in uint64_fields:
                if normalized.get(field) is None:
                    normalized[field] = 0
            
            # ç¡®ä¿Stringå­—æ®µä¸ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä½œä¸ºé»˜è®¤å€¼
            # Stringå­—æ®µï¼šside, change_percent_textï¼ˆè¡¨ç»“æ„ä¸­è¿™äº›å­—æ®µä¸æ˜¯Nullableï¼‰
            string_fields = ["side", "change_percent_text"]
            for field in string_fields:
                if normalized.get(field) is None:
                    normalized[field] = ""
            
            # DateTimeå­—æ®µå¤„ç†ï¼šç¡®ä¿æ‰€æœ‰éNullableçš„DateTimeå­—æ®µéƒ½ä¸ä¸ºNone
            # event_time, stats_open_time, stats_close_time å·²ç»åœ¨å‰é¢é€šè¿‡_to_datetimeå¤„ç†è¿‡äº†
            # ingestion_time å¦‚æœæ²¡æœ‰å€¼ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
            # update_price_date å¯ä»¥ä¸ºNoneï¼ˆå› ä¸ºè¡¨ç»“æ„ä¸­æ˜¯Nullable(DateTime)ï¼‰
            datetime_fields = ["event_time", "stats_open_time", "stats_close_time"]
            for field in datetime_fields:
                if normalized.get(field) is None:
                    normalized[field] = datetime.now(timezone.utc)
            
            # ingestion_time å­—æ®µå¤„ç†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if "ingestion_time" in normalized and normalized.get("ingestion_time") is None:
                normalized["ingestion_time"] = datetime.now(timezone.utc)
            
            # å‡†å¤‡è¡Œæ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰å€¼
            row_data = []
            for name in column_names:
                value = normalized.get(name)
                # å¯¹äºéNullableçš„DateTimeå­—æ®µï¼Œç¡®ä¿ä¸ä¸ºNone
                if name in ["event_time", "stats_open_time", "stats_close_time", "ingestion_time"]:
                    if value is None:
                        value = datetime.now(timezone.utc)
                row_data.append(value)
            
            prepared_rows.append(row_data)
        
        # For ClickHouse, the most efficient way to upsert is to delete existing rows first, then insert new ones
        # This is more efficient than UPDATE for MergeTree tables
        if symbols_to_upsert:
            # Delete existing rows with the same symbols
            # ä½¿ç”¨å»é‡åçš„symbolåˆ—è¡¨ï¼Œé¿å…é‡å¤åˆ é™¤
            unique_symbols = list(set(symbols_to_upsert))
            symbols_str = "', '".join(unique_symbols)
            delete_query = f"ALTER TABLE {self.market_ticker_table} DELETE WHERE symbol IN ('{symbols_str}')"
            try:
                self.command(delete_query)
                logger.debug("[ClickHouse] Deleted existing rows for symbols: %s", unique_symbols)
            except Exception as e:
                logger.warning("[ClickHouse] Failed to delete existing rows: %s", e)
        
        # Insert new rows (æ¯ä¸ªsymbolåªæœ‰ä¸€æ¡æ•°æ®)
        self.insert_rows(self.market_ticker_table, prepared_rows, column_names)
        logger.debug(
            "[ClickHouse] Upserted %s rows into %s (ensured no duplicate symbols)",
            len(prepared_rows), self.market_ticker_table
        )
    
    # ==================================================================
    # Market Ticker æ¨¡å—ï¼šä»·æ ¼ç®¡ç†
    # ==================================================================
    
    def get_symbols_needing_price_refresh(self) -> List[str]:
        """è·å–éœ€è¦åˆ·æ–°ä»·æ ¼çš„symbolåˆ—è¡¨ã€‚
        
        æŸ¥è¯¢æ¡ä»¶ï¼š
        - update_price_date ä¸ºç©º
        - æˆ–è€… update_price_date ä¸ä¸ºå½“å¤©
        
        Returns:
            éœ€è¦åˆ·æ–°ä»·æ ¼çš„symbolåˆ—è¡¨ï¼ˆå·²å»é‡å¹¶æŒ‰å­—æ¯é¡ºåºæ’åºï¼‰
        """
        try:
            query = f"""
            SELECT DISTINCT symbol
            FROM {self.market_ticker_table}
            WHERE symbol != ''
            AND (
                update_price_date IS NULL
                OR toDate(update_price_date) != today()
            )
            ORDER BY symbol
            """
            
            def _execute_query(client):
                return client.query(query)
            
            result = self._with_connection(_execute_query)
            symbols = [row[0] for row in result.result_rows if row[0]]
            logger.debug("[ClickHouse] Found %s symbols needing price refresh", len(symbols))
            return symbols
        except Exception as e:
            logger.error("[ClickHouse] Failed to get symbols needing price refresh: %s", e, exc_info=True)
            return []
    
    def update_open_price(self, symbol: str, open_price: float, update_date: datetime) -> bool:
        """æ›´æ–°æŒ‡å®šsymbolçš„open_priceå’Œupdate_price_date
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            open_price: å¼€ç›˜ä»·ï¼ˆæ˜¨å¤©çš„æ—¥Kçº¿æ”¶ç›˜ä»·ï¼‰
            update_date: æ›´æ–°æ—¥æœŸæ—¶é—´ï¼ˆå½“å‰åˆ·æ–°æ—¶é—´ï¼Œç”¨äºè®°å½•åˆ·æ–°æ—¶é—´æˆ³ï¼‰
            
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            # ä½¿ç”¨ALTER TABLE UPDATEæ¥æ›´æ–°æ•°æ®
            # ç”±äºClickHouseçš„UPDATEæ“ä½œæ˜¯å¼‚æ­¥çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨DELETE + INSERTçš„æ–¹å¼æ›´å¯é 
            # ä½†ä¸ºäº†æ€§èƒ½ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ALTER TABLE UPDATE
            
            # å…ˆæŸ¥è¯¢å½“å‰æ•°æ®
            query = f"""
            SELECT 
                event_time,
                symbol,
                price_change,
                price_change_percent,
                side,
                change_percent_text,
                average_price,
                last_price,
                last_trade_volume,
                high_price,
                low_price,
                base_volume,
                quote_volume,
                stats_open_time,
                stats_close_time,
                first_trade_id,
                last_trade_id,
                trade_count,
                ingestion_time
            FROM (
                SELECT 
                    *,
                    ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY event_time DESC) as rn
                FROM {self.market_ticker_table}
                WHERE symbol = '{symbol}'
            ) AS ranked
            WHERE rn = 1
            """
            
            def _execute_query(client):
                return client.query(query)
            
            result = self._with_connection(_execute_query)
            
            if not result.result_rows:
                logger.warning("[ClickHouse] Symbol %s not found for price update", symbol)
                return False
            
            # è·å–æœ€æ–°çš„ä¸€æ¡æ•°æ®
            row = result.result_rows[0]
            
            # å‡†å¤‡æ›´æ–°åçš„æ•°æ®
            column_names = [
                "event_time", "symbol", "price_change", "price_change_percent", "side",
                "change_percent_text", "average_price", "last_price", "last_trade_volume",
                "open_price", "high_price", "low_price", "base_volume", "quote_volume",
                "stats_open_time", "stats_close_time", "first_trade_id", "last_trade_id",
                "trade_count", "ingestion_time", "update_price_date"
            ]
            
            # é‡æ–°è®¡ç®—æ¶¨è·Œå¹…ç›¸å…³å­—æ®µï¼ˆåŸºäºæ–°çš„open_priceå’Œå½“å‰çš„last_priceï¼‰
            # æŸ¥è¯¢è¿”å›çš„åˆ—é¡ºåºï¼ševent_time, symbol, price_change, price_change_percent, side,
            # change_percent_text, average_price, last_price, last_trade_volume, high_price,
            # low_price, base_volume, quote_volume, stats_open_time, stats_close_time,
            # first_trade_id, last_trade_id, trade_count, ingestion_time
            last_price = float(row[7]) if row[7] is not None else 0.0  # last_priceåœ¨ç´¢å¼•7
            new_open_price = float(open_price)
            
            if new_open_price != 0 and last_price != 0:
                price_change = last_price - new_open_price
                price_change_percent = (price_change / new_open_price) * 100
                side = "gainer" if price_change_percent >= 0 else "loser"
                change_percent_text = f"{price_change_percent:.2f}%"
            else:
                # å¦‚æœæ— æ³•è®¡ç®—ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆä¸èƒ½ä¸ºNoneï¼‰
                price_change = 0.0
                price_change_percent = 0.0
                side = ""
                change_percent_text = ""
            
            # æ„å»ºæ›´æ–°åçš„è¡Œæ•°æ®
            # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰æ­£ç¡®çš„ç±»å‹å’Œé»˜è®¤å€¼
            updated_row = [
                _to_datetime(row[0]),  # event_time (DateTime)
                _normalize_field_value(row[1], "String", "symbol"),  # symbol (String)
                _normalize_field_value(price_change, "Float64", "price_change"),  # price_change (Float64)
                _normalize_field_value(price_change_percent, "Float64", "price_change_percent"),  # price_change_percent (Float64)
                _normalize_field_value(side, "String", "side"),  # side (String)
                _normalize_field_value(change_percent_text, "String", "change_percent_text"),  # change_percent_text (String)
                _normalize_field_value(row[6], "Float64", "average_price"),  # average_price (Float64)
                _normalize_field_value(row[7], "Float64", "last_price"),  # last_price (Float64)
                _normalize_field_value(row[8], "Float64", "last_trade_volume"),  # last_trade_volume (Float64)
                _normalize_field_value(new_open_price, "Float64", "open_price"),  # open_price (Float64)
                _normalize_field_value(row[9], "Float64", "high_price"),  # high_price (Float64)
                _normalize_field_value(row[10], "Float64", "low_price"),  # low_price (Float64)
                _normalize_field_value(row[11], "Float64", "base_volume"),  # base_volume (Float64)
                _normalize_field_value(row[12], "Float64", "quote_volume"),  # quote_volume (Float64)
                _to_datetime(row[13]),  # stats_open_time (DateTime)
                _to_datetime(row[14]),  # stats_close_time (DateTime)
                _normalize_field_value(row[15], "UInt64", "first_trade_id"),  # first_trade_id (UInt64)
                _normalize_field_value(row[16], "UInt64", "last_trade_id"),  # last_trade_id (UInt64)
                _normalize_field_value(row[17], "UInt64", "trade_count"),  # trade_count (UInt64)
                _to_datetime(row[18]) if row[18] else datetime.now(timezone.utc),  # ingestion_time (DateTime)
                update_date  # update_price_date (Nullable(DateTime)) - å¯ä»¥ä¸ºNone
            ]
            
            # åˆ é™¤æ—§æ•°æ®å¹¶æ’å…¥æ–°æ•°æ®ï¼ˆClickHouseçš„UPDATEæ–¹å¼ï¼‰
            delete_query = f"ALTER TABLE {self.market_ticker_table} DELETE WHERE symbol = '{symbol}'"
            try:
                self.command(delete_query)
            except Exception as e:
                logger.warning("[ClickHouse] Failed to delete old row for %s: %s", symbol, e)
            
            # æ’å…¥æ›´æ–°åçš„æ•°æ®
            self.insert_rows(self.market_ticker_table, [updated_row], column_names)
            logger.debug("[ClickHouse] Updated open_price for symbol %s: %s", symbol, new_open_price)
            return True
            
        except Exception as e:
            logger.error("[ClickHouse] Failed to update open_price for symbol %s: %s", symbol, e, exc_info=True)
            return False
    
    # ==================================================================
    # Leaderboard æ¨¡å—ï¼šè¡¨ç®¡ç†
    # ==================================================================
    
    def ensure_leaderboard_table(self) -> None:
        """åˆ›å»ºæœŸè´§æ’è¡Œæ¦œè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ã€‚
        
        è¡¨ç»“æ„è¯´æ˜ï¼š
        - symbol: äº¤æ˜“å¯¹ç¬¦å·
        - price_change: ä»·æ ¼å˜åŒ–é‡
        - price_change_percent: ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”
        - side: æ¶¨è·Œæ–¹å‘ï¼ˆgainer/loserï¼‰
        - change_percent_text: æ ¼å¼åŒ–çš„æ¶¨è·Œå¹…æ–‡æœ¬
        - last_price: æœ€æ–°ä»·æ ¼
        - rank: æ’å(1-10ç­‰)
        - create_datetime: åˆ›å»ºæ—¶é—´
        - create_datetime_long: æ¯«ç§’çº§æ—¶é—´æˆ³(ç”¨äºæ‰¹æ¬¡æ ‡è¯†)
        
        è¡¨ä½¿ç”¨MergeTreeå¼•æ“ï¼ŒæŒ‰(side, rank, symbol, create_datetime_long)æ’åºï¼Œ
        ç¡®ä¿æŸ¥è¯¢æ€§èƒ½å’Œæ•°æ®ç»„ç»‡çš„åˆç†æ€§ã€‚"""
        ddl = f"""
        CREATE TABLE IF NOT EXISTS {self.leaderboard_table} (
            event_time DateTime,
            symbol String,
            price_change Float64,
            price_change_percent Float64,
            side String,
            change_percent_text String,
            average_price Float64,
            last_price Float64,
            last_trade_volume Float64,
            open_price Float64,
            high_price Float64,
            low_price Float64,
            base_volume Float64,
            quote_volume Float64,
            stats_open_time DateTime,
            stats_close_time DateTime,
            first_trade_id UInt64,
            last_trade_id UInt64,
            trade_count UInt64,
            ingestion_time DateTime DEFAULT now(),
            rank UInt8,
            create_datetime DateTime DEFAULT now(),
            create_datetime_long UInt64 DEFAULT 0
        )
        ENGINE = MergeTree
        ORDER BY (side, rank, symbol, create_datetime_long)
        """
        self.command(ddl)
        logger.info("[ClickHouse] Ensured table %s exists", self.leaderboard_table)

    # ==================================================================
    # Leaderboard æ¨¡å—ï¼šæ•°æ®æŸ¥è¯¢
    # ==================================================================
    
    def query_recent_tickers(
        self, 
        time_window_seconds: int = 5,
        side: Optional[str] = None,
        top_n: int = 10
    ) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢æ‰€æœ‰å¸‚åœºè¡Œæƒ…æ•°æ®ï¼Œç”¨äºç”Ÿæˆæ¶¨è·Œå¹…æ¦œ
        
        æ ¸å¿ƒåŠŸèƒ½ï¼š
        - ä»24_market_tickersè¡¨æŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ˆä¸é™åˆ¶æ—¶é—´çª—å£ï¼‰
        - å¯¹æ¯ä¸ªäº¤æ˜“å¯¹å–æœ€æ–°çš„è¡Œæƒ…æ•°æ®ï¼ˆå»é‡ï¼‰
        - æ ¹æ®price_change_percentå­—æ®µåˆ¤æ–­æ¶¨è·Œï¼šæ­£ä¸ºæ¶¨ï¼Œè´Ÿä¸ºè·Œ
        - è¿”å›æ¶¨å¹…æˆ–è·Œå¹…å‰åçš„æ•°æ®
        
        æ‰§è¡Œæµç¨‹ï¼š
        1. æ ¹æ®sideå‚æ•°ç¡®å®šæŸ¥è¯¢é€»è¾‘ï¼š
           - gainerï¼šæŸ¥è¯¢price_change_percent>0çš„åˆçº¦ï¼ŒæŒ‰é™åºæ’åº
           - loserï¼šæŸ¥è¯¢price_change_percent<0çš„åˆçº¦ï¼ŒæŒ‰å‡åºæ’åº
        2. æ„å»ºæŸ¥è¯¢SQLï¼ŒåŒ…å«å»é‡é€»è¾‘ï¼ˆæŒ‰symbolåˆ†ç»„ï¼Œå–æœ€æ–°event_timeï¼‰
        3. æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›ç»“æœ
        
        Args:
            time_window_seconds: å·²åºŸå¼ƒï¼Œä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰è°ƒç”¨ï¼Œä¸å†ä½¿ç”¨
            side: ç­›é€‰æ–¹å‘ï¼Œå¯é€‰å€¼ï¼š'gainer'ï¼ˆæ¶¨å¹…æ¦œï¼‰ã€'loser'ï¼ˆè·Œå¹…æ¦œï¼‰ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
            top_n: è¿”å›å‰Nåæ•°æ®
            
        Returns:
            List[Dict[str, Any]]: è¡Œæƒ…æ•°æ®å­—å…¸åˆ—è¡¨
        """
        logger.info(f"[ClickHouse] ğŸ“Š å¼€å§‹æŸ¥è¯¢æ‰€æœ‰è¡Œæƒ…æ•°æ®...")
        logger.info(f"[ClickHouse] ğŸ“‹ æŸ¥è¯¢å‚æ•°: side={side}, top_n={top_n} (å·²ç§»é™¤æ—¶é—´çª—å£é™åˆ¶)")
        
        # æ„å»ºæŸ¥è¯¢SQLï¼šå»é‡ï¼Œå–æ¯ä¸ªsymbolæœ€æ–°çš„event_time
        # é‡è¦ï¼šåªæŸ¥è¯¢sideå­—æ®µä¸ä¸ºç©ºå­—ç¬¦ä¸²çš„æ•°æ®ï¼ˆside=''è¡¨ç¤ºä»·æ ¼å¼‚æ­¥åˆ·æ–°æœåŠ¡è¿˜æ²¡åˆ·æ–°ï¼Œæ²¡æœ‰æ¶¨è·Œæ•°æ®ï¼‰
        if side:
            if side == 'gainer':
                # æ¶¨å¹…æ¦œï¼šæŸ¥è¯¢price_change_percent>0ä¸”sideä¸ä¸ºç©ºçš„åˆçº¦ï¼ŒæŒ‰price_change_percenté™åºæ’åº
                where_clause = "price_change_percent > 0 AND side != '' AND side IS NOT NULL"
                order_by = "price_change_percent DESC"
                logger.info(f"[ClickHouse] ğŸ“ˆ æ¶¨å¹…æ¦œæŸ¥è¯¢: {where_clause}, æ’åº: {order_by}")
            else:  # loser
                # è·Œå¹…æ¦œï¼šæŸ¥è¯¢price_change_percent<0ä¸”sideä¸ä¸ºç©ºçš„åˆçº¦ï¼ŒæŒ‰price_change_percentå‡åºæ’åºï¼ˆè·Œå¹…æœ€å¤§çš„æ’åœ¨å‰é¢ï¼‰
                where_clause = "price_change_percent < 0 AND side != '' AND side IS NOT NULL"
                order_by = "price_change_percent ASC"
                logger.info(f"[ClickHouse] ğŸ“‰ è·Œå¹…æ¦œæŸ¥è¯¢: {where_clause}, æ’åº: {order_by}")
            
            query = f"""
            SELECT 
                event_time,
                symbol,
                price_change,
                price_change_percent,
                side,
                change_percent_text,
                average_price,
                last_price,
                last_trade_volume,
                open_price,
                high_price,
                low_price,
                base_volume,
                quote_volume,
                stats_open_time,
                stats_close_time,
                first_trade_id,
                last_trade_id,
                trade_count,
                ingestion_time
            FROM (
                SELECT 
                    *,
                    ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY event_time DESC) as rn
                FROM {self.market_ticker_table}
                WHERE {where_clause}
            ) AS ranked
            WHERE rn = 1
            ORDER BY {order_by}
            LIMIT {top_n}
            """
        else:
            # æŸ¥è¯¢æ‰€æœ‰ï¼Œä¸åŒºåˆ†æ¶¨è·Œï¼Œä½†åªæŸ¥è¯¢sideå­—æ®µä¸ä¸ºç©ºçš„æ•°æ®
            query = f"""
            SELECT 
                event_time,
                symbol,
                price_change,
                price_change_percent,
                CASE 
                    WHEN price_change_percent > 0 THEN 'gainer' 
                    WHEN price_change_percent < 0 THEN 'loser' 
                    ELSE 'neutral' 
                END as side,
                change_percent_text,
                average_price,
                last_price,
                last_trade_volume,
                open_price,
                high_price,
                low_price,
                base_volume,
                quote_volume,
                stats_open_time,
                stats_close_time,
                first_trade_id,
                last_trade_id,
                trade_count,
                ingestion_time
            FROM (
                SELECT 
                    *,
                    ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY event_time DESC) as rn
                FROM {self.market_ticker_table}
                WHERE side != '' AND side IS NOT NULL
            ) AS ranked
            WHERE rn = 1
            LIMIT {top_n * 2}
            """
        
        def _execute_query(client):
            return client.query(query)
        
        logger.info(f"[ClickHouse] ğŸ“ æ‰§è¡ŒæŸ¥è¯¢: {query[:100]}...")
        result = self._with_connection(_execute_query)
        logger.info(f"[ClickHouse] âœ… æŸ¥è¯¢æ‰§è¡Œå®Œæˆ")
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        columns = [
            "event_time", "symbol", "price_change", "price_change_percent", "side",
            "change_percent_text", "average_price", "last_price", "last_trade_volume",
            "open_price", "high_price", "low_price", "base_volume", "quote_volume",
            "stats_open_time", "stats_close_time", "first_trade_id", "last_trade_id",
            "trade_count", "ingestion_time"
        ]
        
        rows = []
        for row in result.result_rows:
            row_dict = dict(zip(columns, row))
            rows.append(row_dict)
        
        # è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥å‰3æ¡æ•°æ®çš„volumeå­—æ®µ
        if rows and len(rows) > 0:
            for i, row_dict in enumerate(rows[:3]):
                logger.debug(
                    f"[ClickHouse] [æŸ¥è¯¢ç»“æœ #{i+1}] Symbol: {row_dict.get('symbol')}, "
                    f"base_volume: {row_dict.get('base_volume')}, quote_volume: {row_dict.get('quote_volume')}"
                )
        
        logger.info(f"[ClickHouse] ğŸ“Š æŸ¥è¯¢ç»“æœ: å…± {len(rows)} æ¡æ•°æ®")
        return rows

    # ==================================================================
    # Leaderboard æ¨¡å—ï¼šæ•°æ®åŒæ­¥
    # ==================================================================
    
    def sync_leaderboard(
        self,
        time_window_seconds: int = 5,
        top_n: int = 10
    ) -> None:
        """Sync leaderboard data from market_ticker_table to leaderboard_table.
        
        æ ¸å¿ƒåŠŸèƒ½ï¼š
        - ä»24_market_tickersè¡¨æŸ¥è¯¢æ‰€æœ‰å¸‚åœºæ•°æ®(ä¸é™åˆ¶æ—¶é—´çª—å£)
        - å¯¹æ¯ä¸ªäº¤æ˜“å¯¹å–æœ€æ–°çš„è¡Œæƒ…æ•°æ®(å»é‡)
        - è®¡ç®—æ¯ä¸ªåˆçº¦çš„æ¶¨è·Œå¹…
        - ç­›é€‰å‡ºæ¶¨å¹…å‰Nåå’Œè·Œå¹…å‰Nå
        - ä½¿ç”¨å…¨é‡æ›´æ–°æ–¹å¼æ›´æ–°futures_leaderboardè¡¨(ä¸´æ—¶è¡¨ + REPLACE TABLE)
        
        æ‰§è¡Œæµç¨‹ï¼š
        1. æ£€æŸ¥leaderboardè¡¨æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
        2. æŸ¥è¯¢æ¶¨å¹…æ¦œå‰Nå(æŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ¶¨è·Œå¹…æ’åº)
        3. æŸ¥è¯¢è·Œå¹…æ¦œå‰Nå(æŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ¶¨è·Œå¹…æ’åº)
        4. å‡†å¤‡æ’å…¥æ•°æ®
        5. åˆ›å»ºä¸´æ—¶è¡¨å¹¶æ’å…¥æ•°æ®
        6. ä½¿ç”¨REPLACE TABLEåŸå­æ›¿æ¢åŸè¡¨(å…¨é‡æ›´æ–°)
        7. æ¸…ç†ä¸´æ—¶è¡¨
        
        Args:
            time_window_seconds: å·²åºŸå¼ƒï¼Œä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰è°ƒç”¨ï¼Œä¸å†ä½¿ç”¨æ—¶é—´çª—å£é™åˆ¶
            top_n: æ¶¨è·Œå¹…å‰Nåæ•°é‡
        """
        try:
            logger.info("[ClickHouse] ğŸš€ å¼€å§‹æ¶¨è·Œå¹…æ¦œåŒæ­¥...")
            logger.info("[ClickHouse] ğŸ“‹ åŒæ­¥å‚æ•°: top_n=%s (æŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼Œä¸é™åˆ¶æ—¶é—´çª—å£)", top_n)
            
            # é‡è¦ï¼šæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
            logger.info("[ClickHouse] ğŸ” æ£€æŸ¥leaderboardè¡¨æ˜¯å¦å­˜åœ¨...")
            table_exists = self._check_table_exists(self.leaderboard_table)
            if not table_exists:
                logger.info("[ClickHouse] ğŸ“‹ leaderboardè¡¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºè¡¨...")
                self.ensure_leaderboard_table()
                logger.info("[ClickHouse] âœ… leaderboardè¡¨åˆ›å»ºå®Œæˆ")
            else:
                logger.info("[ClickHouse] âœ… leaderboardè¡¨å·²å­˜åœ¨")
            
            # æŸ¥è¯¢æ¶¨å¹…æ¦œå‰Nåï¼ˆæŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ¶¨è·Œå¹…æ’åºï¼‰
            logger.info("[ClickHouse] ğŸ” æŸ¥è¯¢æ¶¨å¹…æ¦œå‰%såï¼ˆä»æ‰€æœ‰æ•°æ®ä¸­æ’åºï¼‰...", top_n)
            gainers = self.query_recent_tickers(
                time_window_seconds=time_window_seconds,
                side='gainer',
                top_n=top_n
            )
            logger.info("[ClickHouse] âœ… æ¶¨å¹…æ¦œæŸ¥è¯¢å®Œæˆï¼Œå…± %s æ¡æ•°æ®", len(gainers))
            
            # æŸ¥è¯¢è·Œå¹…æ¦œå‰Nåï¼ˆæŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ¶¨è·Œå¹…æ’åºï¼‰
            logger.info("[ClickHouse] ğŸ” æŸ¥è¯¢è·Œå¹…æ¦œå‰%såï¼ˆä»æ‰€æœ‰æ•°æ®ä¸­æ’åºï¼‰...", top_n)
            losers = self.query_recent_tickers(
                time_window_seconds=time_window_seconds,
                side='loser',
                top_n=top_n
            )
            logger.info("[ClickHouse] âœ… è·Œå¹…æ¦œæŸ¥è¯¢å®Œæˆï¼Œå…± %s æ¡æ•°æ®", len(losers))
            
            # é‡è¦ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®ï¼ˆsideå­—æ®µä¸ä¸ºç©ºï¼‰
            # å¦‚æœæ¶¨å¹…æ¦œå’Œè·Œå¹…æ¦œéƒ½æ²¡æœ‰æ•°æ®ï¼Œè¯´æ˜ä»·æ ¼å¼‚æ­¥åˆ·æ–°æœåŠ¡è¿˜æ²¡åˆ·æ–°ï¼Œæ­¤æ—¶ä¸åº”è¯¥æ‰§è¡ŒåŒæ­¥
            if not gainers and not losers:
                logger.warning("[ClickHouse] âš ï¸ æ¶¨å¹…æ¦œå’Œè·Œå¹…æ¦œéƒ½æ²¡æœ‰æ•°æ®ï¼ˆsideå­—æ®µä¸ºç©ºï¼‰ï¼Œè·³è¿‡åŒæ­¥æ“ä½œ")
                logger.warning("[ClickHouse] âš ï¸ è¿™å¯èƒ½æ˜¯å› ä¸ºä»·æ ¼å¼‚æ­¥åˆ·æ–°æœåŠ¡è¿˜æ²¡æœ‰åˆ·æ–°open_priceï¼Œå¯¼è‡´sideå­—æ®µä¸ºç©ºå­—ç¬¦ä¸²")
                return
            
            # å‡†å¤‡æ’å…¥æ•°æ®
            logger.info("[ClickHouse] ğŸ“ å‡†å¤‡æ’å…¥æ•°æ®...")
            all_rows = []
            column_names = [
                "event_time", "symbol", "price_change", "price_change_percent", "side",
                "change_percent_text", "average_price", "last_price", "last_trade_volume",
                "open_price", "high_price", "low_price", "base_volume", "quote_volume",
                "stats_open_time", "stats_close_time", "first_trade_id", "last_trade_id",
                "trade_count", "ingestion_time", "rank", "create_datetime", "create_datetime_long"
            ]

            # æœ¬æ¬¡åŒæ­¥æ‰¹æ¬¡çš„å”¯ä¸€æ—¶é—´æˆ³ï¼ˆæ•´æ‰¹æ’å…¥ä½¿ç”¨ç›¸åŒçš„ create_datetime å’Œ create_datetime_longï¼‰
            batch_time = datetime.now(timezone.utc)
            # ç”Ÿæˆæ¯«ç§’çº§æ—¶é—´æˆ³ï¼ˆUInt64ï¼‰ï¼Œç”¨äºç²¾ç¡®æ’åºå’ŒæŸ¥è¯¢æœ€æ–°æ‰¹æ¬¡
            batch_time_long = int(batch_time.timestamp() * 1000)
            
            # æ·»åŠ æ¶¨å¹…æ¦œæ•°æ®ï¼ˆå¸¦æ’åï¼‰
            logger.info("[ClickHouse] ğŸ“Š å¤„ç†æ¶¨å¹…æ¦œæ•°æ®...")
            for idx, row in enumerate(gainers, 1):
                symbol = row.get("symbol", "")
                base_volume_raw = row.get("base_volume")
                quote_volume_raw = row.get("quote_volume")
                
                # è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥åŸå§‹æ•°æ®
                if idx <= 3:  # åªè®°å½•å‰3æ¡ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                    logger.debug(
                        f"[ClickHouse] [æ¶¨å¹…æ¦œ #{idx}] Symbol: {symbol}, "
                        f"base_volume_raw: {base_volume_raw}, quote_volume_raw: {quote_volume_raw}"
                    )
                
                row_data = [
                    _to_datetime(row.get("event_time")),  # DateTime
                    _normalize_field_value(row.get("symbol"), "String", "symbol"),  # String
                    _normalize_field_value(row.get("price_change"), "Float64", "price_change"),  # Float64
                    _normalize_field_value(row.get("price_change_percent"), "Float64", "price_change_percent"),  # Float64
                    _normalize_field_value(row.get("side", "gainer"), "String", "side"),  # String
                    _normalize_field_value(row.get("change_percent_text"), "String", "change_percent_text"),  # String
                    _normalize_field_value(row.get("average_price"), "Float64", "average_price"),  # Float64
                    _normalize_field_value(row.get("last_price"), "Float64", "last_price"),  # Float64
                    _normalize_field_value(row.get("last_trade_volume"), "Float64", "last_trade_volume"),  # Float64
                    _normalize_field_value(row.get("open_price"), "Float64", "open_price"),  # Float64
                    _normalize_field_value(row.get("high_price"), "Float64", "high_price"),  # Float64
                    _normalize_field_value(row.get("low_price"), "Float64", "low_price"),  # Float64
                    _normalize_field_value(base_volume_raw, "Float64", "base_volume"),  # Float64
                    _normalize_field_value(quote_volume_raw, "Float64", "quote_volume"),  # Float64
                    _to_datetime(row.get("stats_open_time")),  # DateTime
                    _to_datetime(row.get("stats_close_time")),  # DateTime
                    _normalize_field_value(row.get("first_trade_id"), "UInt64", "first_trade_id"),  # UInt64
                    _normalize_field_value(row.get("last_trade_id"), "UInt64", "last_trade_id"),  # UInt64
                    _normalize_field_value(row.get("trade_count"), "UInt64", "trade_count"),  # UInt64
                    _to_datetime(row.get("ingestion_time")),  # DateTime
                    _normalize_field_value(idx, "UInt8", "rank"),  # UInt8 (rank)
                    batch_time,  # create_datetimeï¼ŒåŒä¸€æ‰¹æ¬¡ä½¿ç”¨ç›¸åŒæ—¶é—´
                    batch_time_long,  # create_datetime_longï¼ŒåŒä¸€æ‰¹æ¬¡ä½¿ç”¨ç›¸åŒçš„æ¯«ç§’çº§æ—¶é—´æˆ³
                ]
                all_rows.append(row_data)
            logger.info("[ClickHouse] âœ… æ¶¨å¹…æ¦œæ•°æ®å¤„ç†å®Œæˆï¼Œå…± %s æ¡", len(gainers))
            
            # æ·»åŠ è·Œå¹…æ¦œæ•°æ®ï¼ˆå¸¦æ’åï¼‰
            logger.info("[ClickHouse] ğŸ“Š å¤„ç†è·Œå¹…æ¦œæ•°æ®...")
            for idx, row in enumerate(losers, 1):
                symbol = row.get("symbol", "")
                base_volume_raw = row.get("base_volume")
                quote_volume_raw = row.get("quote_volume")
                
                # è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥åŸå§‹æ•°æ®
                if idx <= 3:  # åªè®°å½•å‰3æ¡ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                    logger.debug(
                        f"[ClickHouse] [è·Œå¹…æ¦œ #{idx}] Symbol: {symbol}, "
                        f"base_volume_raw: {base_volume_raw}, quote_volume_raw: {quote_volume_raw}"
                    )
                
                row_data = [
                    _to_datetime(row.get("event_time")),  # DateTime
                    _normalize_field_value(row.get("symbol"), "String", "symbol"),  # String
                    _normalize_field_value(row.get("price_change"), "Float64", "price_change"),  # Float64
                    _normalize_field_value(row.get("price_change_percent"), "Float64", "price_change_percent"),  # Float64
                    _normalize_field_value(row.get("side", "loser"), "String", "side"),  # String
                    _normalize_field_value(row.get("change_percent_text"), "String", "change_percent_text"),  # String
                    _normalize_field_value(row.get("average_price"), "Float64", "average_price"),  # Float64
                    _normalize_field_value(row.get("last_price"), "Float64", "last_price"),  # Float64
                    _normalize_field_value(row.get("last_trade_volume"), "Float64", "last_trade_volume"),  # Float64
                    _normalize_field_value(row.get("open_price"), "Float64", "open_price"),  # Float64
                    _normalize_field_value(row.get("high_price"), "Float64", "high_price"),  # Float64
                    _normalize_field_value(row.get("low_price"), "Float64", "low_price"),  # Float64
                    _normalize_field_value(base_volume_raw, "Float64", "base_volume"),  # Float64
                    _normalize_field_value(quote_volume_raw, "Float64", "quote_volume"),  # Float64
                    _to_datetime(row.get("stats_open_time")),  # DateTime
                    _to_datetime(row.get("stats_close_time")),  # DateTime
                    _normalize_field_value(row.get("first_trade_id"), "UInt64", "first_trade_id"),  # UInt64
                    _normalize_field_value(row.get("last_trade_id"), "UInt64", "last_trade_id"),  # UInt64
                    _normalize_field_value(row.get("trade_count"), "UInt64", "trade_count"),  # UInt64
                    _to_datetime(row.get("ingestion_time")),  # DateTime
                    _normalize_field_value(idx, "UInt8", "rank"),  # UInt8 (rank)
                    batch_time,  # create_datetimeï¼ŒåŒä¸€æ‰¹æ¬¡ä½¿ç”¨ç›¸åŒæ—¶é—´
                    batch_time_long,  # create_datetime_longï¼ŒåŒä¸€æ‰¹æ¬¡ä½¿ç”¨ç›¸åŒçš„æ¯«ç§’çº§æ—¶é—´æˆ³
                ]
                all_rows.append(row_data)
            logger.info("[ClickHouse] âœ… è·Œå¹…æ¦œæ•°æ®å¤„ç†å®Œæˆï¼Œå…± %s æ¡", len(losers))
            
            if all_rows:
                logger.info("[ClickHouse] ğŸ’¾ å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®åˆ°ClickHouseï¼Œå…± %s æ¡...", len(all_rows))

                # ä½¿ç”¨é”é˜²æ­¢å¹¶å‘æ‰§è¡Œæ’å…¥ï¼Œé¿å…å¹¶å‘æ‰¹æ¬¡äº¤ç»‡
                with ClickHouseDatabase._sync_leaderboard_lock:
                    # ç›´æ¥ä½¿ç”¨ ClickHouse æ‰¹é‡æ’å…¥ï¼Œä¸å†ä½¿ç”¨ä¸´æ—¶è¡¨/å…¨é‡æ›¿æ¢æ–¹æ¡ˆ
                    self.insert_rows(self.leaderboard_table, all_rows, column_names)
                    logger.info(
                        "[ClickHouse] âœ… æ‰¹é‡æ’å…¥å®Œæˆï¼Œæœ¬æ¬¡æ‰¹æ¬¡æ—¶é—´æˆ³: %s (create_datetime_long=%s), æ¶¨å¹…: %d æ¡, è·Œå¹…: %d æ¡",
                        batch_time.isoformat(),
                        batch_time_long,
                        len(gainers),
                        len(losers),
                    )
            else:
                logger.warning("[ClickHouse] âš ï¸  æ²¡æœ‰æ¶¨è·Œå¹…æ¦œæ•°æ®å¯åŒæ­¥")
                
        except Exception as exc:
            logger.error("[ClickHouse] âŒ æ¶¨è·Œå¹…æ¦œåŒæ­¥å¤±è´¥: %s", exc, exc_info=True)

    def get_leaderboard(self, limit: int = 10) -> Dict[str, List[Dict]]:
        """è·å–æœ€æ–°æ‰¹æ¬¡çš„æœŸè´§æ¶¨è·Œå¹…æ¦œæ•°æ®ã€‚
        
        ä½¿ç”¨create_datetime_longå­—æ®µ(æ•°å€¼å‹æ¯«ç§’çº§æ—¶é—´æˆ³)æŸ¥è¯¢æœ€æ–°æ‰¹æ¬¡ï¼Œ
        é¿å…create_datetime(ç§’çº§ç²¾åº¦)å¯¼è‡´åŒä¸€ç§’å¤šæ¡æ•°æ®æ— æ³•åŒºåˆ†çš„é—®é¢˜ã€‚
        
        å®ç°åŸç†ï¼š
        1. é€šè¿‡å­æŸ¥è¯¢è·å–æœ€å¤§çš„create_datetime_longå€¼(æœ€æ–°æ‰¹æ¬¡æ ‡è¯†)
        2. æŸ¥è¯¢è¯¥æ‰¹æ¬¡çš„æ‰€æœ‰æ¶¨å¹…æ¦œå’Œè·Œå¹…æ¦œæ•°æ®
        3. åœ¨å†…å­˜ä¸­æŒ‰æ¶¨è·Œå¹…æ’åºå¹¶æˆªå–å‰Nå
        4. è¿”å›åŒ…å«gainerså’Œlosersä¸¤ä¸ªåˆ—è¡¨çš„å­—å…¸
        
        Args:
            limit: æ¯ä¸ªæ–¹å‘(æ¶¨å¹…/è·Œå¹…)è¿”å›çš„æœ€å¤§è®°å½•æ•°
            
        Returns:
            åŒ…å«'gainers'å’Œ'losers'ä¸¤ä¸ªåˆ—è¡¨çš„å­—å…¸ï¼Œæ¯ä¸ªåˆ—è¡¨åŒ…å«æ ¼å¼åŒ–åçš„æ’è¡Œæ¦œæ•°æ®é¡¹
        """
        try:
            # ä¸€æ¡ SQLï¼šå…ˆé”å®šæœ€æ–°æ‰¹æ¬¡çš„ create_datetime_longï¼ˆæ•°å€¼å‹ï¼Œæ¯«ç§’çº§ç²¾åº¦ï¼‰ï¼Œå†å–è¯¥æ‰¹æ¬¡æ‰€æœ‰æ¶¨è·Œæ•°æ®
            query = f"""
            SELECT
                symbol,
                last_price,
                price_change_percent,
                side,
                change_percent_text,
                quote_volume,
                rank,
                create_datetime_long
            FROM {self.leaderboard_table}
            WHERE create_datetime_long = (
                SELECT max(create_datetime_long) FROM {self.leaderboard_table}
            )
              AND side IN ('gainer', 'loser')
            """

            def _execute_query(client):
                return client.query(query)

            result = self._with_connection(_execute_query)
            rows = result.result_rows

            if not rows:
                logger.warning("[ClickHouse] get_leaderboard: futures_leaderboard ä¸­æ²¡æœ‰æ•°æ®")
                return {'gainers': [], 'losers': []}

            gainers: List[Dict] = []
            losers: List[Dict] = []

            # å…ˆæŒ‰ side åˆ†ç±»ï¼Œå†åœ¨å†…å­˜ä¸­æ’åº + æˆªæ–­å‰ N
            for row in rows:
                try:
                    side = str(row[3]) if row[3] else ''
                    item = {
                        'symbol': str(row[0]) if row[0] else '',
                        'price': float(row[1]) if row[1] is not None else 0.0,
                        'change_percent': float(row[2]) if row[2] is not None else 0.0,
                        'side': side or ('gainer' if (row[2] or 0) >= 0 else 'loser'),
                        'change_percent_text': str(row[4]) if row[4] else '',
                        'quote_volume': float(row[5]) if len(row) > 5 and row[5] is not None else 0.0,
                        'rank': int(row[6]) if len(row) > 6 and row[6] is not None else 0,
                    }

                    if item['side'] == 'gainer':
                        gainers.append(item)
                    elif item['side'] == 'loser':
                        losers.append(item)
                except (TypeError, ValueError, IndexError) as e:
                    logger.warning("[ClickHouse] Failed to parse leaderboard row: %s, error: %s", row, e)
                    continue

            # åœ¨å†…å­˜ä¸­æŒ‰æ¶¨è·Œå¹…æ’åºå¹¶æˆªå–å‰ N å
            gainers.sort(key=lambda x: x.get('change_percent', 0.0), reverse=True)
            losers.sort(key=lambda x: x.get('change_percent', 0.0))  # è·Œå¹…è¶Šå°ï¼ˆæ›´è´Ÿï¼‰æ’è¶Šå‰

            gainers = gainers[: max(0, int(limit or 0))] if limit else gainers
            losers = losers[: max(0, int(limit or 0))] if limit else losers

            return {
                'gainers': gainers,
                'losers': losers,
            }
        except Exception as exc:
            logger.error("[ClickHouse] Failed to get leaderboard: %s", exc, exc_info=True)
            return {'gainers': [], 'losers': []}

    def get_leaderboard_symbols(self) -> List[str]:
        """è·å–æ’è¡Œæ¦œä¸­æ‰€æœ‰ä¸åŒçš„äº¤æ˜“å¯¹ç¬¦å·ã€‚
        
        ä»leaderboardè¡¨ä¸­æŸ¥è¯¢æ‰€æœ‰éç©ºçš„symbolå­—æ®µï¼Œå¹¶è¿›è¡Œå»é‡å¤„ç†ã€‚
        
        Returns:
            å»é‡åçš„äº¤æ˜“å¯¹ç¬¦å·åˆ—è¡¨
        """
        try:
            query = f"""
            SELECT symbol
            FROM {self.leaderboard_table}
            WHERE symbol != ''
            GROUP BY symbol
            """
            
            def _execute_query(client):
                return client.query(query)
            
            result = self._with_connection(_execute_query)
            symbols = [row[0] for row in result.result_rows if row[0]]
            return symbols
        except Exception as exc:
            logger.error("[ClickHouse] Failed to get leaderboard symbols: %s", exc, exc_info=True)
            return []

    # ==================================================================
    # Leaderboard æ¨¡å—ï¼šæ•°æ®æ¸…ç†
    # ==================================================================
    
    def cleanup_old_leaderboard(self, minutes: int = 10) -> dict:
        """æ¸…ç†æŒ‡å®šæ—¶é—´ä¹‹å‰çš„æ—§æ’è¡Œæ¦œæ•°æ®ã€‚
        
        ä½¿ç”¨ create_datetime_longï¼ˆæ•°å€¼å‹æ¯«ç§’çº§æ—¶é—´æˆ³ï¼‰è¿›è¡Œæ¸…ç†ï¼Œé¿å… create_datetimeï¼ˆç§’çº§ç²¾åº¦ï¼‰
        å¯¼è‡´åŒä¸€ç§’å¤šæ¡æ•°æ®æ— æ³•å‡†ç¡®åŒºåˆ†çš„é—®é¢˜ã€‚
        
        å®ç°åŸç†ï¼š
        1. è®¡ç®—å½“å‰æ—¶é—´å‡å»æŒ‡å®šåˆ†é’Ÿæ•°åçš„æ¯«ç§’çº§æ—¶é—´æˆ³ä½œä¸ºæˆªæ­¢æ—¶é—´
        2. æŸ¥è¯¢æ¸…ç†å‰çš„æ•°æ®é‡ç»Ÿè®¡
        3. ä½¿ç”¨ALTER TABLE DELETEè¯­å¥åˆ é™¤æ‰€æœ‰æ—©äºæˆªæ­¢æ—¶é—´çš„è®°å½•
        4. æŸ¥è¯¢æ¸…ç†åçš„æ•°æ®é‡ç»Ÿè®¡
        5. è®°å½•è¯¦ç»†çš„æ¸…ç†æ—¥å¿—å¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            minutes: ä¿ç•™æ—¶é—´çª—å£ï¼ˆåˆ†é’Ÿï¼‰ï¼Œåˆ é™¤create_datetime_longæ—©äºå½“å‰æ—¶é—´è¯¥åˆ†é’Ÿæ•°ä¹‹å‰çš„æ•°æ®
        
        Returns:
            åŒ…å«æ¸…ç†ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ï¼š
            - total_before: æ¸…ç†å‰çš„æ€»æ•°æ®é‡
            - total_after: æ¸…ç†åçš„æ€»æ•°æ®é‡ï¼ˆä¼°ç®—ï¼Œå› ä¸ºDELETEæ˜¯å¼‚æ­¥çš„ï¼‰
            - to_delete_count: å¾…åˆ é™¤çš„æ•°æ®é‡ï¼ˆä¼°ç®—ï¼‰
            - cutoff_timestamp_ms: æˆªæ­¢æ—¶é—´æˆ³
            - cutoff_time: æˆªæ­¢æ—¶é—´ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰
        """
        cleanup_start_time = time.time()
        stats = {
            'total_before': 0,
            'total_after': 0,
            'to_delete_count': 0,
            'cutoff_timestamp_ms': 0,
            'cutoff_time': '',
            'execution_time': 0.0
        }
        
        try:
            from datetime import datetime, timezone
            
            # è®¡ç®—å½“å‰æ—¶é—´å‡å»æŒ‡å®šåˆ†é’Ÿæ•°åçš„æ¯«ç§’çº§æ—¶é—´æˆ³
            current_time = datetime.now(timezone.utc)
            cutoff_time = current_time
            cutoff_timestamp_ms = int((cutoff_time.timestamp() - minutes * 60) * 1000)
            cutoff_time_str = datetime.fromtimestamp(cutoff_timestamp_ms / 1000, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
            
            stats['cutoff_timestamp_ms'] = cutoff_timestamp_ms
            stats['cutoff_time'] = cutoff_time_str
            
            logger.info(
                "[ClickHouse] ğŸ§¹ å¼€å§‹æ¸…ç†æ¶¨è·Œæ¦œå†å²æ•°æ® | ä¿ç•™æ—¶é—´: %s åˆ†é’Ÿ | æˆªæ­¢æ—¶é—´: %s (timestamp_ms=%s)",
                minutes,
                cutoff_time_str,
                cutoff_timestamp_ms,
            )
            
            # æŸ¥è¯¢æ¸…ç†å‰çš„æ•°æ®é‡ç»Ÿè®¡
            try:
                count_before_sql = f"SELECT count() FROM {self.leaderboard_table}"
                count_to_delete_sql = f"SELECT count() FROM {self.leaderboard_table} WHERE create_datetime_long < {cutoff_timestamp_ms}"
                
                def _execute_count_before(client):
                    result = client.query(count_before_sql)
                    return result.result_rows[0][0] if result.result_rows else 0
                
                def _execute_count_to_delete(client):
                    result = client.query(count_to_delete_sql)
                    return result.result_rows[0][0] if result.result_rows else 0
                
                stats['total_before'] = self._with_connection(_execute_count_before)
                stats['to_delete_count'] = self._with_connection(_execute_count_to_delete)
                
                logger.info(
                    "[ClickHouse] ğŸ“Š æ¸…ç†å‰æ•°æ®ç»Ÿè®¡ | æ€»æ•°æ®é‡: %s æ¡ | å¾…åˆ é™¤æ•°æ®é‡: %s æ¡ | ä¿ç•™æ•°æ®é‡: %s æ¡",
                    stats['total_before'],
                    stats['to_delete_count'],
                    stats['total_before'] - stats['to_delete_count'],
                )
            except Exception as count_exc:
                logger.warning(
                    "[ClickHouse] âš ï¸ æŸ¥è¯¢æ¸…ç†å‰æ•°æ®é‡æ—¶å‡ºé”™: %s (ç»§ç»­æ‰§è¡Œæ¸…ç†)",
                    count_exc,
                )
            
            # æ‰§è¡Œåˆ é™¤æ“ä½œ
            delete_sql = f"""
            ALTER TABLE {self.leaderboard_table}
            DELETE WHERE create_datetime_long < {cutoff_timestamp_ms}
            """
            
            logger.info("[ClickHouse] ğŸ”¨ æ‰§è¡Œåˆ é™¤æ“ä½œ...")
            self.command(delete_sql)
            
            # æŸ¥è¯¢æ¸…ç†åçš„æ•°æ®é‡ï¼ˆç”±äºDELETEæ˜¯å¼‚æ­¥çš„ï¼Œè¿™é‡Œåªæ˜¯ä¼°ç®—ï¼‰
            try:
                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©DELETEæ“ä½œå¼€å§‹æ‰§è¡Œ
                time.sleep(0.5)  # ç­‰å¾…500ms
                
                def _execute_count_after(client):
                    result = client.query(count_before_sql)
                    return result.result_rows[0][0] if result.result_rows else 0
                
                stats['total_after'] = self._with_connection(_execute_count_after)
            except Exception as count_after_exc:
                logger.warning(
                    "[ClickHouse] âš ï¸ æŸ¥è¯¢æ¸…ç†åæ•°æ®é‡æ—¶å‡ºé”™: %s",
                    count_after_exc,
                )
                # ä¼°ç®—æ¸…ç†åçš„æ•°æ®é‡
                stats['total_after'] = stats['total_before'] - stats['to_delete_count']
            
            cleanup_end_time = time.time()
            stats['execution_time'] = cleanup_end_time - cleanup_start_time
            
            # è®°å½•è¯¦ç»†çš„æ¸…ç†ç»“æœæ—¥å¿—
            logger.info(
                "[ClickHouse] âœ… æ¸…ç†æ“ä½œå·²å®Œæˆ | æ‰§è¡Œæ—¶é—´: %.3f ç§’ | æ¸…ç†å‰: %s æ¡ | å¾…åˆ é™¤: %s æ¡ | æ¸…ç†å(ä¼°ç®—): %s æ¡",
                stats['execution_time'],
                stats['total_before'],
                stats['to_delete_count'],
                stats['total_after'],
            )
            
            # å¦‚æœå¾…åˆ é™¤çš„æ•°æ®é‡å¾ˆå¤§ï¼Œè®°å½•è­¦å‘Š
            if stats['to_delete_count'] > 100000:
                logger.warning(
                    "[ClickHouse] âš ï¸ å¾…åˆ é™¤æ•°æ®é‡è¾ƒå¤§: %s æ¡ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´å®Œæˆåˆ é™¤æ“ä½œ",
                    stats['to_delete_count'],
                )
            
            # å¦‚æœæ¸…ç†åæ•°æ®é‡ä»ç„¶å¾ˆå¤§ï¼Œè®°å½•è­¦å‘Š
            if stats['total_after'] > 50000:
                logger.warning(
                    "[ClickHouse] âš ï¸ æ¸…ç†åæ•°æ®é‡ä»ç„¶è¾ƒå¤§: %s æ¡ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æ’å…¥é¢‘ç‡æˆ–è°ƒæ•´ä¿ç•™æ—¶é—´",
                    stats['total_after'],
                )
            
            return stats
            
        except Exception as exc:
            cleanup_end_time = time.time()
            stats['execution_time'] = cleanup_end_time - cleanup_start_time
            logger.error(
                "[ClickHouse] âŒ æ¸…ç†æ¶¨è·Œæ¦œå†å²æ•°æ®å¤±è´¥ | æ‰§è¡Œæ—¶é—´: %.3f ç§’ | é”™è¯¯: %s",
                stats['execution_time'],
                exc,
                exc_info=True,
            )
            return stats

    # ==================================================================
    # Market Klines æ¨¡å—ï¼šè¡¨ç®¡ç†
    # ==================================================================
    
    def ensure_market_klines_table(self) -> None:
        """Create per-interval market_klines tables if they do not exist.

        æ‹†åˆ†åŸæ¥çš„å•è¡¨ market_klines ä¸º 7 å¼ æŒ‰ interval åˆ’åˆ†çš„è¡¨ï¼š
        - market_klines_1w, market_klines_1d, market_klines_4h, market_klines_1h,
          market_klines_15m, market_klines_5m, market_klines_1m
        """
        for interval, table_name in self.market_klines_tables.items():
            ddl = f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                event_time DateTime,
                symbol String,
                contract_type String,
                kline_start_time DateTime,
                kline_end_time DateTime,
                interval String,
                first_trade_id UInt64,
                last_trade_id UInt64,
                open_price Float64,
                close_price Float64,
                high_price Float64,
                low_price Float64,
                base_volume Float64,
                trade_count UInt64,
                is_closed UInt8,
                quote_volume Float64,
                taker_buy_base_volume Float64,
                taker_buy_quote_volume Float64,
                create_time DateTime DEFAULT now()
            )
            ENGINE = MergeTree
            ORDER BY (symbol, interval, kline_end_time, event_time)
            TTL kline_end_time + INTERVAL 2 DAY
            """
            self.command(ddl)
            logger.info("[ClickHouse] Ensured kline table %s (interval=%s) exists", table_name, interval)

    # ==================================================================
    # Market Klines æ¨¡å—ï¼šæ•°æ®æ’å…¥
    # ==================================================================
    
    def insert_market_klines(self, rows: Iterable[Dict[str, Any]]) -> None:
        """å°†Kçº¿æ•°æ®æ’å…¥åˆ°æŒ‰æ—¶é—´é—´éš”åˆ’åˆ†çš„market_klinesè¡¨ä¸­ã€‚
        
        åŠŸèƒ½è¯´æ˜ï¼š
        1. å°†Kçº¿æ•°æ®æŒ‰æ—¶é—´é—´éš”ï¼ˆintervalï¼‰å½’ç±»åˆ°å¯¹åº”çš„è¡¨ä¸­
        2. æ”¯æŒ7ç§æ—¶é—´é—´éš”ï¼š1w, 1d, 4h, 1h, 15m, 5m, 1m
        3. å¯¹æ¯æ¡æ•°æ®è¿›è¡Œå­—æ®µæ ‡å‡†åŒ–å¤„ç†
        4. æŒ‰ä¸åŒæ—¶é—´é—´éš”è¡¨è¿›è¡Œæ‰¹é‡æ’å…¥
        
        Args:
            rows: Kçº¿æ•°æ®å­—å…¸çš„å¯è¿­ä»£å¯¹è±¡ï¼Œæ¯æ¡æ•°æ®åŒ…å«event_timeã€symbolã€intervalç­‰å­—æ®µ
        """
        rows = list(rows)
        if not rows:
            return

        column_names = [
            "event_time",
            "symbol",
            "contract_type",
            "kline_start_time",
            "kline_end_time",
            "interval",
            "first_trade_id",
            "last_trade_id",
            "open_price",
            "close_price",
            "high_price",
            "low_price",
            "base_volume",
            "trade_count",
            "is_closed",
            "quote_volume",
            "taker_buy_base_volume",
            "taker_buy_quote_volume",
        ]

        # æŒ‰ interval å½’ç±»åˆ°å¯¹åº”çš„è¡¨
        bucketed: Dict[str, List[Tuple[Any, ...]]] = {}
        for row in rows:
            interval = (row.get("interval") or "").strip()
            table_name = self.market_klines_tables.get(interval)
            if not table_name:
                logger.debug(
                    "[ClickHouse] Skip kline row with unsupported interval: %s (row=%s)",
                    interval,
                    row,
                )
                continue

            values = (
                row.get("event_time"),
                row.get("symbol", ""),
                row.get("contract_type", ""),
                row.get("kline_start_time"),
                row.get("kline_end_time"),
                interval,
                row.get("first_trade_id", 0),
                row.get("last_trade_id", 0),
                row.get("open_price", 0.0),
                row.get("close_price", 0.0),
                row.get("high_price", 0.0),
                row.get("low_price", 0.0),
                row.get("base_volume", 0.0),
                row.get("trade_count", 0),
                row.get("is_closed", 0),
                row.get("quote_volume", 0.0),
                row.get("taker_buy_base_volume", 0.0),
                row.get("taker_buy_quote_volume", 0.0),
            )
            bucketed.setdefault(table_name, []).append(values)

        # æ‰¹é‡å†™å…¥å„ interval å¯¹åº”çš„è¡¨
        for table_name, payload in bucketed.items():
            if not payload:
                continue
            self.insert_rows(table_name, payload, column_names)
            logger.debug(
                "[ClickHouse] Inserted %s kline rows into %s",
                len(payload),
                table_name,
            )

    # ==================================================================
    # Market Klines æ¨¡å—ï¼šæ•°æ®æ¸…ç†
    # ==================================================================
    
    def cleanup_old_klines(self, days: int = 2) -> int:
        """æ¸…ç†æŒ‡å®šå¤©æ•°ä¹‹å‰çš„æ—§Kçº¿æ•°æ®ã€‚
        
        åŠŸèƒ½è¯´æ˜ï¼š
        1. éå†æ‰€æœ‰æ—¶é—´é—´éš”çš„Kçº¿è¡¨ï¼ˆ1w, 1d, 4h, 1h, 15m, 5m, 1mï¼‰
        2. å¯¹æ¯ä¸ªè¡¨æ‰§è¡ŒALTER TABLE DELETEæ“ä½œï¼Œåˆ é™¤è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ•°æ®
        3. è®°å½•æ¸…ç†æ“ä½œæ—¥å¿—
        
        æ³¨æ„ï¼šClickHouseçš„DELETEæ“ä½œæ˜¯å¼‚æ­¥æ‰§è¡Œçš„ï¼Œæ‰€ä»¥æ— æ³•ç«‹å³è·å–åˆ é™¤çš„å…·ä½“è¡Œæ•°
        
        Args:
            days: ä¿ç•™å¤©æ•°ï¼Œåˆ é™¤kline_end_timeæ—©äºå½“å‰æ—¶é—´å‡å»è¯¥å¤©æ•°çš„æ•°æ®
            
        Returns:
            ç”±äºæ˜¯å¼‚æ­¥æ“ä½œï¼Œè¿”å›0è¡¨ç¤ºåˆ é™¤ä»»åŠ¡å·²æäº¤
        """
        try:
            # ClickHouse DELETE requires mutations, use ALTER TABLE DELETE
            for interval, table_name in self.market_klines_tables.items():
                delete_sql = f"""
                ALTER TABLE {table_name}
                DELETE WHERE kline_end_time < now() - INTERVAL {days} DAY
                """
                self.command(delete_sql)
                logger.info(
                    "[ClickHouse] Initiated cleanup of klines older than %s days for %s (interval=%s)",
                    days,
                    table_name,
                    interval,
                )
            # Note: ClickHouse DELETE is asynchronous, we can't get exact count immediately
            return 0  # ClickHouse doesn't return count for async DELETE
        except Exception as exc:
            logger.error("[ClickHouse] Failed to cleanup old klines: %s", exc, exc_info=True)
            return 0


# ==================================================================
# è¾…åŠ©å‡½æ•°
# ==================================================================

def _to_datetime(value: Any) -> datetime:
    """å°†å€¼è½¬æ¢ä¸ºdatetimeå¯¹è±¡ï¼Œå¦‚æœæ— æ³•è½¬æ¢åˆ™è¿”å›å½“å‰UTCæ—¶é—´ã€‚
    
    æ”¯æŒçš„è¾“å…¥ç±»å‹ï¼š
    - datetimeå¯¹è±¡ï¼šç›´æ¥è¿”å›
    - Noneï¼šè¿”å›å½“å‰UTCæ—¶é—´
    - æ—¶é—´æˆ³ï¼ˆæ•´æ•°æˆ–æµ®ç‚¹æ•°ï¼‰ï¼šè‡ªåŠ¨è½¬æ¢ä¸ºdatetimeå¯¹è±¡
    - å…¶ä»–ç±»å‹ï¼šå°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°ä½œä¸ºæ—¶é—´æˆ³ï¼Œå¤±è´¥åˆ™è¿”å›å½“å‰UTCæ—¶é—´
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°æ°¸è¿œä¸ä¼šè¿”å›Noneï¼Œç¡®ä¿DateTimeå­—æ®µå§‹ç»ˆæœ‰å€¼
    
    Args:
        value: éœ€è¦è½¬æ¢çš„å€¼
        
    Returns:
        datetimeå¯¹è±¡ï¼Œç¡®ä¿æœ‰å€¼ä¸”å¸¦æ—¶åŒºä¿¡æ¯(UTC)
    """
    if isinstance(value, datetime):
        return value
    if value is None:
        return datetime.now(timezone.utc)
    try:
        numeric = float(value)
    except (TypeError, ValueError):
        return datetime.now(timezone.utc)

    seconds = numeric / 1000.0
    return datetime.fromtimestamp(seconds, tz=timezone.utc)


def _normalize_field_value(value: Any, field_type: str, field_name: str = "") -> Any:
    """è§„èŒƒåŒ–å­—æ®µå€¼ï¼Œç¡®ä¿ç¬¦åˆClickHouseå­—æ®µç±»å‹è¦æ±‚
    
    Args:
        value: åŸå§‹å€¼
        field_type: å­—æ®µç±»å‹ ('Float64', 'UInt64', 'UInt8', 'String', 'DateTime')
        field_name: å­—æ®µåç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
    Returns:
        è§„èŒƒåŒ–åçš„å€¼ï¼Œç¡®ä¿ä¸ä¸ºNoneï¼ˆé™¤éå­—æ®µç±»å‹å…è®¸ï¼‰
    """
    if value is None:
        if field_type == 'Float64':
            return 0.0
        elif field_type == 'UInt64':
            return 0
        elif field_type == 'UInt8':
            return 0
        elif field_type == 'String':
            return ""
        elif field_type == 'DateTime':
            return datetime.now(timezone.utc)
        else:
            logger.warning(f"[ClickHouse] Unknown field type {field_type} for field {field_name}, using None")
            return None
    
    # ç±»å‹è½¬æ¢
    try:
        if field_type == 'Float64':
            return float(value)
        elif field_type == 'UInt64':
            return int(value)
        elif field_type == 'UInt8':
            return int(value)
        elif field_type == 'String':
            return str(value) if value else ""
        elif field_type == 'DateTime':
            return _to_datetime(value)
    except (TypeError, ValueError) as e:
        logger.warning(f"[ClickHouse] Failed to convert field {field_name} ({field_type}): {e}, using default")
        # è¿”å›é»˜è®¤å€¼
        if field_type == 'Float64':
            return 0.0
        elif field_type == 'UInt64':
            return 0
        elif field_type == 'UInt8':
            return 0
        elif field_type == 'String':
            return ""
        elif field_type == 'DateTime':
            return datetime.now(timezone.utc)
    
    return value


def _derive_side(percent: Any) -> str:
    """æ ¹æ®æ¶¨è·Œå¹…ç™¾åˆ†æ¯”ç¡®å®šæ¶¨è·Œæ–¹å‘ã€‚
    
    Args:
        percent: æ¶¨è·Œå¹…ç™¾åˆ†æ¯”å€¼
        
    Returns:
        "gainer"è¡¨ç¤ºä¸Šæ¶¨ï¼Œ"loser"è¡¨ç¤ºä¸‹è·Œ
    """
    try:
        value = float(percent)
    except (TypeError, ValueError):
        value = 0.0
    return "loser" if value < 0 else "gainer"


def _format_percent_text(percent: Any) -> str:
    """å°†æ¶¨è·Œå¹…ç™¾åˆ†æ¯”æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œä¿ç•™ä¸¤ä½å°æ•°ã€‚
    
    Args:
        percent: æ¶¨è·Œå¹…ç™¾åˆ†æ¯”å€¼
        
    Returns:
        æ ¼å¼åŒ–åçš„ç™¾åˆ†æ¯”å­—ç¬¦ä¸²ï¼Œå¦‚"5.25%"
    """
    try:
        value = float(percent)
    except (TypeError, ValueError):
        value = 0.0
    return f"{value:.2f}%"
