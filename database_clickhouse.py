"""ClickHouse database utilities for market data storage."""
from __future__ import annotations

import logging
import threading
from datetime import datetime, timezone, timedelta
from queue import Queue, Empty
from typing import Any, Dict, Iterable, List, Optional, Callable

import clickhouse_connect
import config as app_config

MARKET_TICKER_TABLE = "24_market_tickers"
LEADERBOARD_TABLE = "futures_leaderboard"
MARKET_KLINES_TABLE = "market_klines"

logger = logging.getLogger(__name__)


class ClickHouseConnectionPool:
    """ClickHouse connection pool to manage client instances.
    
    This class manages a pool of ClickHouse client instances to avoid creating
    too many connections to the ClickHouse server. It provides methods to acquire
    and release connections, and supports dynamic expansion up to a maximum
    number of connections.
    """
    
    def __init__(
        self,
        host: str,
        port: int,
        username: str,
        password: str,
        database: str,
        secure: bool,
        min_connections: int = 5,
        max_connections: int = 20,
        connection_timeout: int = 30
    ):
        """Initialize the connection pool.
        
        Args:
            host: ClickHouse host
            port: ClickHouse port
            username: ClickHouse username
            password: ClickHouse password
            database: ClickHouse database
            secure: Whether to use secure connection
            min_connections: Minimum number of connections to keep in the pool
            max_connections: Maximum number of connections allowed in the pool
            connection_timeout: Timeout for acquiring a connection (seconds)
        """
        self._host = host
        self._port = port
        self._username = username
        self._password = password
        self._database = database
        self._secure = secure
        self._min_connections = min_connections
        self._max_connections = max_connections
        self._connection_timeout = connection_timeout
        
        # Create a queue to hold the connections
        self._pool = Queue(maxsize=max_connections)
        
        # Create a lock to protect the connection count
        self._lock = threading.Lock()
        
        # Current number of connections in the pool
        self._current_connections = 0
        
        # Initialize the pool with min_connections
        self._initialize_pool()
    
    def _initialize_pool(self):
        """Initialize the pool with minimum connections."""
        for _ in range(self._min_connections):
            self._create_connection()
    
    def _create_connection(self):
        """Create a new ClickHouse client connection."""
        with self._lock:
            if self._current_connections >= self._max_connections:
                return None
            
            try:
                client = clickhouse_connect.get_client(
                    host=self._host,
                    port=self._port,
                    username=self._username,
                    password=self._password,
                    database=self._database,
                    secure=self._secure
                )
                self._pool.put(client)
                self._current_connections += 1
                logger.debug(f"[ClickHouse] Created new connection. Current connections: {self._current_connections}")
                return client
            except Exception as e:
                logger.error(f"[ClickHouse] Failed to create connection: {e}")
                return None
    
    def acquire(self, timeout: Optional[int] = None) -> Optional[Any]:
        """Acquire a connection from the pool.
        
        Args:
            timeout: Timeout for acquiring a connection (seconds). If None, use the default timeout.
            
        Returns:
            A ClickHouse client instance, or None if no connection is available within the timeout.
        """
        timeout = timeout or self._connection_timeout
        
        try:
            # Try to get a connection from the pool
            client = self._pool.get(timeout=timeout)
            logger.debug(f"[ClickHouse] Acquired connection from pool")
            return client
        except Empty:
            # If the pool is empty, try to create a new connection
            logger.debug(f"[ClickHouse] Pool is empty, creating new connection")
            client = self._create_connection()
            if client:
                return client
            
            # If we can't create a new connection, try again to get from the pool
            try:
                client = self._pool.get(timeout=timeout)
                logger.debug(f"[ClickHouse] Acquired connection from pool after waiting")
                return client
            except Empty:
                logger.error(f"[ClickHouse] Failed to acquire connection within timeout {timeout} seconds")
                return None
    
    def release(self, client: Any) -> None:
        """Release a connection back to the pool.
        
        Args:
            client: The ClickHouse client instance to release
        """
        try:
            # Check if the client is still valid by sending a simple query
            client.query("SELECT 1")
            self._pool.put(client)
            logger.debug(f"[ClickHouse] Released connection back to pool")
        except Exception as e:
            # If the client is invalid, close it and don't return it to the pool
            logger.error(f"[ClickHouse] Connection is invalid, closing it: {e}")
            with self._lock:
                self._current_connections -= 1
    
    def close_all(self) -> None:
        """Close all connections in the pool."""
        with self._lock:
            while not self._pool.empty():
                try:
                    client = self._pool.get_nowait()
                    client.close()
                    self._current_connections -= 1
                except Exception as e:
                    logger.error(f"[ClickHouse] Failed to close connection: {e}")
            
            logger.info(f"[ClickHouse] Closed all connections. Current connections: {self._current_connections}")


class ClickHouseDatabase:
    """Encapsulates ClickHouse connectivity and CRUD helpers."""
    
    # ç±»çº§åˆ«çš„é”ï¼Œç”¨äºé˜²æ­¢å¹¶å‘æ‰§è¡Œ sync_leaderboard
    _sync_leaderboard_lock = threading.Lock()

    def __init__(self, *, auto_init_tables: bool = True) -> None:
        # Create a connection pool instead of individual client instances
        self._pool = ClickHouseConnectionPool(
            host=app_config.CLICKHOUSE_HOST,
            port=app_config.CLICKHOUSE_PORT,
            username=app_config.CLICKHOUSE_USER,
            password=app_config.CLICKHOUSE_PASSWORD,
            database=app_config.CLICKHOUSE_DATABASE,
            secure=app_config.CLICKHOUSE_SECURE,
            min_connections=5,
            max_connections=50,
            connection_timeout=30
        )
        
        self.market_ticker_table = MARKET_TICKER_TABLE
        self.leaderboard_table = getattr(app_config, 'CLICKHOUSE_LEADERBOARD_TABLE', LEADERBOARD_TABLE)
        self.market_klines_table = getattr(app_config, 'CLICKHOUSE_MARKET_KLINES_TABLE', MARKET_KLINES_TABLE)
        
        if auto_init_tables:
            # Initialize tables using the connection pool
            self.ensure_market_ticker_table()
            self.ensure_leaderboard_table()
            self.ensure_market_klines_table()
    
    def _with_connection(self, func: Callable, *args, **kwargs) -> Any:
        """Execute a function with a ClickHouse connection from the pool.
        
        This method acquires a connection from the pool, executes the given function
        with the connection as the first argument, and then releases the connection back to the pool.
        
        Args:
            func: The function to execute
            *args: Positional arguments to pass to the function
            **kwargs: Keyword arguments to pass to the function
            
        Returns:
            The result of the function call
        """
        client = self._pool.acquire()
        if not client:
            raise Exception("Failed to acquire ClickHouse connection")
        
        try:
            return func(client, *args, **kwargs)
        finally:
            self._pool.release(client)

    # ------------------------------------------------------------------
    # Generic helpers
    # ------------------------------------------------------------------
    def command(self, sql: str) -> None:
        """Execute a raw SQL command."""
        def _execute_command(client):
            client.command(sql)
        
        self._with_connection(_execute_command)

    def insert_rows(
        self,
        table: str,
        rows: Iterable[Iterable[Any]],
        column_names: List[str],
    ) -> None:
        payload = list(rows)
        if not payload:
            return
        
        def _execute_insert(client):
            client.insert(table, payload, column_names=column_names)
            logger.debug("[ClickHouse] Inserted %s rows into %s", len(payload), table)
        
        self._with_connection(_execute_insert)

    # ------------------------------------------------------------------
    # Market ticker helpers
    # ------------------------------------------------------------------
    def ensure_market_ticker_table(self) -> None:
        """Create the 24h market ticker table if it does not exist."""
        ddl = f"""
        CREATE TABLE IF NOT EXISTS {self.market_ticker_table} (
            event_time DateTime,
            symbol String,
            price_change Float64,
            price_change_percent Float64,
            side String,
            change_percent_text String,
            average_price Float64,
            last_price Float64,
            last_trade_volume Float64,
            open_price Float64,
            high_price Float64,
            low_price Float64,
            base_volume Float64,
            quote_volume Float64,
            stats_open_time DateTime,
            stats_close_time DateTime,
            first_trade_id UInt64,
            last_trade_id UInt64,
            trade_count UInt64,
            ingestion_time DateTime DEFAULT now(),
            update_price_date Nullable(DateTime)
        )
        ENGINE = MergeTree
        ORDER BY (symbol, stats_close_time, event_time)
        """
        self.command(ddl)
        logger.info("[ClickHouse] Ensured table %s exists", self.market_ticker_table)
        
        # å¦‚æœè¡¨å·²å­˜åœ¨ï¼Œæ·»åŠ æ–°å­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰æˆ–ä¿®æ”¹å­—æ®µç±»å‹ä¸ºNullable
        try:
            # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨
            check_column_sql = f"""
            SELECT name, type FROM system.columns 
            WHERE database = '{app_config.CLICKHOUSE_DATABASE}' 
            AND table = '{self.market_ticker_table}' 
            AND name = 'update_price_date'
            """
            def _check_column(client):
                result = client.query(check_column_sql)
                if len(result.result_rows) > 0:
                    return result.result_rows[0][1]  # è¿”å›å­—æ®µç±»å‹
                return None
            
            column_type = self._with_connection(_check_column)
            
            if column_type is None:
                # å­—æ®µä¸å­˜åœ¨ï¼Œæ·»åŠ ä¸ºNullable
                add_column_sql = f"""
                ALTER TABLE {self.market_ticker_table} 
                ADD COLUMN IF NOT EXISTS update_price_date Nullable(DateTime)
                """
                self.command(add_column_sql)
                logger.info("[ClickHouse] Added update_price_date column to %s", self.market_ticker_table)
            elif 'Nullable' not in str(column_type):
                # å­—æ®µå­˜åœ¨ä½†ä¸æ˜¯Nullableï¼Œå°è¯•ä¿®æ”¹ä¸ºNullable
                try:
                    modify_column_sql = f"""
                    ALTER TABLE {self.market_ticker_table} 
                    MODIFY COLUMN update_price_date Nullable(DateTime)
                    """
                    self.command(modify_column_sql)
                    logger.info("[ClickHouse] Modified update_price_date column to Nullable(DateTime)")
                except Exception as modify_exc:
                    logger.warning("[ClickHouse] Failed to modify update_price_date to Nullable (may not be supported): %s", modify_exc)
        except Exception as e:
            logger.warning("[ClickHouse] Failed to check/modify update_price_date column: %s", e)

    def get_existing_symbol_data(self, symbols: List[str]) -> Dict[str, Dict[str, Any]]:
        """æŸ¥è¯¢ç°æœ‰symbolçš„æ•°æ®ï¼Œè¿”å›å­—å…¸ {symbol: {open_price: ..., update_price_date: ..., ...}}
        
        æ³¨æ„ï¼šä¸ºäº†ä¿æŒåŸæœ‰é€»è¾‘ï¼Œå¦‚æœopen_priceä¸º0.0ä¸”update_price_dateä¸ºNoneï¼Œ
        åˆ™è§†ä¸º"æœªè®¾ç½®"ï¼Œè¿”å›open_price=Noneè€Œä¸æ˜¯0.0
        """
        if not symbols:
            return {}
        
        try:
            symbols_str = "', '".join(symbols)
            query = f"""
            SELECT 
                symbol,
                open_price,
                last_price,
                update_price_date
            FROM (
                SELECT 
                    *,
                    ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY event_time DESC) as rn
                FROM {self.market_ticker_table}
                WHERE symbol IN ('{symbols_str}')
            ) AS ranked
            WHERE rn = 1
            """
            
            def _execute_query(client):
                return client.query(query)
            
            result = self._with_connection(_execute_query)
            
            symbol_data = {}
            for row in result.result_rows:
                symbol = row[0]
                open_price_raw = row[1]  # å¯èƒ½æ˜¯0.0æˆ–å®é™…ä»·æ ¼
                last_price = row[2] if row[2] is not None else None
                update_price_date = row[3] if len(row) > 3 else None
                
                # å…³é”®é€»è¾‘ï¼šå¦‚æœopen_priceä¸º0.0ä¸”update_price_dateä¸ºNoneï¼Œè§†ä¸º"æœªè®¾ç½®"
                # è¿”å›Noneä»¥ä¿æŒåŸæœ‰åˆ¤æ–­é€»è¾‘çš„æ­£ç¡®æ€§
                if open_price_raw == 0.0 and update_price_date is None:
                    open_price = None  # è§†ä¸ºæœªè®¾ç½®
                else:
                    open_price = open_price_raw if open_price_raw is not None else None
                
                symbol_data[symbol] = {
                    "open_price": open_price,
                    "last_price": last_price,
                    "update_price_date": update_price_date
                }
            
            return symbol_data
        except Exception as e:
            logger.warning("[ClickHouse] Failed to get existing symbol data: %s", e)
            return {}

    def insert_market_tickers(self, rows: Iterable[Dict[str, Any]]) -> None:
        column_names = [
            "event_time",
            "symbol",
            "price_change",
            "price_change_percent",
            "side",
            "change_percent_text",
            "average_price",
            "last_price",
            "last_trade_volume",
            "open_price",
            "high_price",
            "low_price",
            "base_volume",
            "quote_volume",
            "stats_open_time",
            "stats_close_time",
            "first_trade_id",
            "last_trade_id",
            "trade_count",
            "update_price_date",
        ]

        prepared_rows: List[List[Any]] = []
        for row in rows:
            normalized = dict(row)
            normalized["event_time"] = _to_datetime(normalized.get("event_time"))
            normalized["stats_open_time"] = _to_datetime(normalized.get("stats_open_time"))
            normalized["stats_close_time"] = _to_datetime(normalized.get("stats_close_time"))
            
            # ç¡®ä¿æ‰€æœ‰Float64å­—æ®µä¸ä¸ºNoneï¼Œä½¿ç”¨0.0ä½œä¸ºé»˜è®¤å€¼
            # Float64å­—æ®µåˆ—è¡¨ï¼šprice_change, price_change_percent, average_price, last_price,
            # last_trade_volume, open_price, high_price, low_price, base_volume, quote_volume
            float64_fields = [
                "price_change", "price_change_percent", "average_price", "last_price",
                "last_trade_volume", "open_price", "high_price", "low_price",
                "base_volume", "quote_volume"
            ]
            for field in float64_fields:
                if normalized.get(field) is None:
                    normalized[field] = 0.0
            
            # ç¡®ä¿UInt64å­—æ®µä¸ä¸ºNone
            uint64_fields = ["first_trade_id", "last_trade_id", "trade_count"]
            for field in uint64_fields:
                if normalized.get(field) is None:
                    normalized[field] = 0
            
            # ç¡®ä¿Stringå­—æ®µä¸ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä½œä¸ºé»˜è®¤å€¼
            # Stringå­—æ®µï¼šside, change_percent_textï¼ˆè¡¨ç»“æ„ä¸­è¿™äº›å­—æ®µä¸æ˜¯Nullableï¼‰
            string_fields = ["side", "change_percent_text"]
            for field in string_fields:
                if normalized.get(field) is None:
                    normalized[field] = ""
            
            # DateTimeå­—æ®µå¯ä»¥ä¸ºNone
            normalized.setdefault("update_price_date", None)
            
            prepared_rows.append([normalized.get(name) for name in column_names])

        self.insert_rows(self.market_ticker_table, prepared_rows, column_names)
        
    def upsert_market_tickers(self, rows: Iterable[Dict[str, Any]]) -> None:
        """Update or insert market tickers. If symbol exists, update it, otherwise insert.
        
        å¤„ç†é€»è¾‘ï¼š
        1. ç¬¬ä¸€æ¬¡æ’å…¥æ—¶ï¼Œprice_change, price_change_percent, side, change_percent_text, open_price éƒ½ä¸ºç©º
        2. æ›´æ–°æ—¶ï¼Œå¦‚æœ open_price æœ‰å€¼ï¼Œåˆ™é€šè¿‡ last_price å’Œ open_price è®¡ç®—æ¶¨è·Œå¹…ç›¸å…³å­—æ®µ
        
        Args:
            rows: Iterable of ticker dictionaries
        """
        if not rows:
            return
            
        # Filter rows to only include symbols ending with "USDT"
        usdt_rows = [row for row in rows if row.get("symbol", "").endswith("USDT")]
        if not usdt_rows:
            logger.debug("[ClickHouse] No USDT symbols to upsert")
            return
            
        column_names = [
            "event_time",
            "symbol",
            "price_change",
            "price_change_percent",
            "side",
            "change_percent_text",
            "average_price",
            "last_price",
            "last_trade_volume",
            "open_price",
            "high_price",
            "low_price",
            "base_volume",
            "quote_volume",
            "stats_open_time",
            "stats_close_time",
            "first_trade_id",
            "last_trade_id",
            "trade_count",
            "update_price_date",
        ]

        # å¤„ç†æ•°æ®ï¼šå¯¹äºåŒä¸€æ‰¹æ•°æ®ä¸­çš„é‡å¤symbolï¼Œåªä¿ç•™æœ€æ–°çš„ä¸€æ¡ï¼ˆåŸºäºstats_close_timeï¼‰
        symbol_data_map: Dict[str, Dict[str, Any]] = {}
        
        for row in usdt_rows:
            normalized = dict(row)
            normalized["event_time"] = _to_datetime(normalized.get("event_time"))
            normalized["stats_open_time"] = _to_datetime(normalized.get("stats_open_time"))
            normalized["stats_close_time"] = _to_datetime(normalized.get("stats_close_time"))
            
            symbol = normalized.get("symbol")
            if not symbol:
                continue
            
            stats_close_time = normalized.get("stats_close_time")
            
            # å¦‚æœè¯¥symbolå·²å­˜åœ¨ï¼Œæ¯”è¾ƒstats_close_timeï¼Œåªä¿ç•™æœ€æ–°çš„
            if symbol in symbol_data_map:
                existing_stats_close_time = symbol_data_map[symbol].get("stats_close_time")
                if stats_close_time and existing_stats_close_time:
                    if stats_close_time > existing_stats_close_time:
                        symbol_data_map[symbol] = normalized
                elif stats_close_time:
                    # å½“å‰æ•°æ®æœ‰stats_close_timeï¼Œä¿ç•™å½“å‰æ•°æ®
                    symbol_data_map[symbol] = normalized
            else:
                symbol_data_map[symbol] = normalized
        
        if not symbol_data_map:
            return
        
        # æŸ¥è¯¢ç°æœ‰symbolçš„æ•°æ®ï¼Œè·å–open_price
        symbols_to_upsert = list(symbol_data_map.keys())
        existing_data = self.get_existing_symbol_data(symbols_to_upsert)
        
        # å‡†å¤‡æ’å…¥æ•°æ®ï¼ˆæ¯ä¸ªsymbolåªæœ‰ä¸€æ¡æœ€æ–°æ•°æ®ï¼‰
        prepared_rows: List[List[Any]] = []
        
        for symbol, normalized in symbol_data_map.items():
            # è·å–å½“å‰æŠ¥æ–‡çš„last_price
            try:
                current_last_price = float(normalized.get("last_price", 0))
            except (TypeError, ValueError):
                current_last_price = 0.0
            
            # åˆ¤æ–­æ˜¯æ’å…¥è¿˜æ˜¯æ›´æ–°
            existing_symbol_data = existing_data.get(symbol)
            existing_open_price = existing_symbol_data.get("open_price") if existing_symbol_data else None
            
            # å…³é”®é€»è¾‘ï¼šåˆ¤æ–­open_priceæ˜¯å¦å·²è®¾ç½®
            # existing_open_priceä¸ºNoneè¡¨ç¤ºæœªè®¾ç½®ï¼ˆå³ä½¿æ•°æ®åº“ä¸­å­˜å‚¨çš„æ˜¯0.0ï¼Œå¦‚æœupdate_price_dateä¸ºNoneä¹Ÿè§†ä¸ºæœªè®¾ç½®ï¼‰
            # existing_open_priceä¸ä¸ºNoneä¸”ä¸ä¸º0è¡¨ç¤ºå·²è®¾ç½®ä¸”æœ‰æœ‰æ•ˆå€¼
            # å¦‚æœæ˜¯æ›´æ–°ä¸”open_priceæœ‰å€¼ï¼ˆä¸ä¸ºNoneä¸”ä¸ä¸º0ï¼‰ï¼Œåˆ™è®¡ç®—æ¶¨è·Œå¹…ç›¸å…³å­—æ®µ
            if existing_open_price is not None and existing_open_price != 0 and current_last_price != 0:
                try:
                    existing_open_price_float = float(existing_open_price)
                    current_last_price_float = float(current_last_price)
                    
                    # è®¡ç®— price_change = last_price - open_price
                    price_change = current_last_price_float - existing_open_price_float
                    
                    # è®¡ç®— price_change_percent = (last_price - open_price) / open_price * 100
                    price_change_percent = (price_change / existing_open_price_float) * 100
                    
                    # æ ¹æ®æ­£è´Ÿè®¾ç½® sideï¼ˆ0ä¸ºæ­£ï¼Œå³gainerï¼‰
                    side = "gainer" if price_change_percent >= 0 else "loser"
                    
                    # è®¾ç½® change_percent_text = price_change_percent + "%"
                    change_percent_text = f"{price_change_percent:.2f}%"
                    
                    # ä¿æŒåŸæœ‰çš„open_priceï¼Œä¸æ›´æ–°
                    normalized["price_change"] = price_change
                    normalized["price_change_percent"] = price_change_percent
                    normalized["side"] = side
                    normalized["change_percent_text"] = change_percent_text
                    normalized["open_price"] = existing_open_price_float
                    normalized["update_price_date"] = None  # åªæœ‰å¼€ç›˜ä»·å¼‚æ­¥åˆ·æ–°æœåŠ¡æ‰ä¼šæ›´æ–°è¿™ä¸ªå­—æ®µ
                except (TypeError, ValueError) as e:
                    logger.warning("[ClickHouse] Failed to calculate price change for symbol %s: %s", symbol, e)
                    # è®¡ç®—å¤±è´¥æ—¶ï¼Œè®¾ç½®ä¸º0.0ï¼ˆFloat64å­—æ®µä¸èƒ½ä¸ºNoneï¼‰
                    normalized["price_change"] = 0.0
                    normalized["price_change_percent"] = 0.0
                    normalized["side"] = ""  # Stringå­—æ®µä¸èƒ½ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                    normalized["change_percent_text"] = ""  # Stringå­—æ®µä¸èƒ½ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                    normalized["open_price"] = existing_open_price_float if existing_open_price_float else 0.0
                    normalized["update_price_date"] = None
            else:
                # ç¬¬ä¸€æ¬¡æ’å…¥æˆ–open_priceæœªè®¾ç½®çš„æƒ…å†µ
                # Float64å­—æ®µè®¾ç½®ä¸º0.0è€Œä¸æ˜¯Noneï¼ˆå› ä¸ºClickHouse Float64ä¸æ¥å—Noneï¼‰
                # ä½†é€»è¾‘ä¸Šï¼Œopen_price=0.0ä¸”update_price_date=Noneè¡¨ç¤º"æœªè®¾ç½®"
                # è¿™æ ·ä¸‹æ¬¡æŸ¥è¯¢æ—¶ï¼Œget_existing_symbol_dataä¼šè¿”å›open_price=Noneï¼Œä¿æŒåŸæœ‰åˆ¤æ–­é€»è¾‘æ­£ç¡®
                normalized["price_change"] = 0.0
                normalized["price_change_percent"] = 0.0
                normalized["side"] = ""  # Stringå­—æ®µä¸èƒ½ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                normalized["change_percent_text"] = ""  # Stringå­—æ®µä¸èƒ½ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                normalized["open_price"] = 0.0  # å­˜å‚¨ä¸º0.0ï¼Œä½†é€»è¾‘ä¸Šè§†ä¸º"æœªè®¾ç½®"ï¼ˆå› ä¸ºupdate_price_date=Noneï¼‰
                normalized["update_price_date"] = None
            
            # ç¡®ä¿æ‰€æœ‰Float64å­—æ®µä¸ä¸ºNoneï¼Œä½¿ç”¨0.0ä½œä¸ºé»˜è®¤å€¼
            # Float64å­—æ®µåˆ—è¡¨ï¼šprice_change, price_change_percent, average_price, last_price,
            # last_trade_volume, open_price, high_price, low_price, base_volume, quote_volume
            float64_fields = [
                "price_change", "price_change_percent", "average_price", "last_price",
                "last_trade_volume", "open_price", "high_price", "low_price",
                "base_volume", "quote_volume"
            ]
            for field in float64_fields:
                if normalized.get(field) is None:
                    normalized[field] = 0.0
            
            # ç¡®ä¿UInt64å­—æ®µä¸ä¸ºNone
            uint64_fields = ["first_trade_id", "last_trade_id", "trade_count"]
            for field in uint64_fields:
                if normalized.get(field) is None:
                    normalized[field] = 0
            
            # ç¡®ä¿Stringå­—æ®µä¸ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä½œä¸ºé»˜è®¤å€¼
            # Stringå­—æ®µï¼šside, change_percent_textï¼ˆè¡¨ç»“æ„ä¸­è¿™äº›å­—æ®µä¸æ˜¯Nullableï¼‰
            string_fields = ["side", "change_percent_text"]
            for field in string_fields:
                if normalized.get(field) is None:
                    normalized[field] = ""
            
            # DateTimeå­—æ®µå¤„ç†ï¼šupdate_price_dateå¯ä»¥ä¸ºNoneï¼ˆå› ä¸ºè¡¨ç»“æ„ä¸­æ˜¯Nullable(DateTime)ï¼‰
            # å…¶ä»–DateTimeå­—æ®µï¼ˆevent_time, stats_open_time, stats_close_timeï¼‰å·²ç»åœ¨å‰é¢é€šè¿‡_to_datetimeå¤„ç†è¿‡äº†
            # ç¡®ä¿update_price_dateå¦‚æœæ˜¯Noneï¼Œä¿æŒä¸ºNoneï¼ˆNullableå­—æ®µå…è®¸Noneï¼‰
            # ä¸éœ€è¦é¢å¤–å¤„ç†ï¼Œå› ä¸ºNullable(DateTime)å­—æ®µå¯ä»¥æ¥å—None
            
            prepared_rows.append([normalized.get(name) for name in column_names])
        
        # For ClickHouse, the most efficient way to upsert is to delete existing rows first, then insert new ones
        # This is more efficient than UPDATE for MergeTree tables
        if symbols_to_upsert:
            # Delete existing rows with the same symbols
            # ä½¿ç”¨å»é‡åçš„symbolåˆ—è¡¨ï¼Œé¿å…é‡å¤åˆ é™¤
            unique_symbols = list(set(symbols_to_upsert))
            symbols_str = "', '".join(unique_symbols)
            delete_query = f"ALTER TABLE {self.market_ticker_table} DELETE WHERE symbol IN ('{symbols_str}')"
            try:
                self.command(delete_query)
                logger.debug("[ClickHouse] Deleted existing rows for symbols: %s", unique_symbols)
            except Exception as e:
                logger.warning("[ClickHouse] Failed to delete existing rows: %s", e)
        
        # Insert new rows (æ¯ä¸ªsymbolåªæœ‰ä¸€æ¡æ•°æ®)
        self.insert_rows(self.market_ticker_table, prepared_rows, column_names)
        logger.debug(
            "[ClickHouse] Upserted %s rows into %s (ensured no duplicate symbols)",
            len(prepared_rows), self.market_ticker_table
        )
    
    def get_symbols_needing_price_refresh(self) -> List[str]:
        """è·å–éœ€è¦åˆ·æ–°ä»·æ ¼çš„symbolåˆ—è¡¨
        
        æŸ¥è¯¢æ¡ä»¶ï¼š
        - update_price_date ä¸ºç©º
        - æˆ–è€… update_price_date ä¸ä¸ºå½“å¤©
        
        Returns:
            éœ€è¦åˆ·æ–°ä»·æ ¼çš„symbolåˆ—è¡¨ï¼ˆå»é‡ï¼‰
        """
        try:
            query = f"""
            SELECT DISTINCT symbol
            FROM {self.market_ticker_table}
            WHERE symbol != ''
            AND (
                update_price_date IS NULL
                OR toDate(update_price_date) != today()
            )
            ORDER BY symbol
            """
            
            def _execute_query(client):
                return client.query(query)
            
            result = self._with_connection(_execute_query)
            symbols = [row[0] for row in result.result_rows if row[0]]
            logger.debug("[ClickHouse] Found %s symbols needing price refresh", len(symbols))
            return symbols
        except Exception as e:
            logger.error("[ClickHouse] Failed to get symbols needing price refresh: %s", e, exc_info=True)
            return []
    
    def update_open_price(self, symbol: str, open_price: float, update_date: datetime) -> bool:
        """æ›´æ–°æŒ‡å®šsymbolçš„open_priceå’Œupdate_price_date
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            open_price: å¼€ç›˜ä»·ï¼ˆæ˜¨å¤©çš„æ—¥Kçº¿æ”¶ç›˜ä»·ï¼‰
            update_date: æ›´æ–°æ—¥æœŸï¼ˆå½“å¤©æ—¶é—´ï¼‰
            
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            # ä½¿ç”¨ALTER TABLE UPDATEæ¥æ›´æ–°æ•°æ®
            # ç”±äºClickHouseçš„UPDATEæ“ä½œæ˜¯å¼‚æ­¥çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨DELETE + INSERTçš„æ–¹å¼æ›´å¯é 
            # ä½†ä¸ºäº†æ€§èƒ½ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ALTER TABLE UPDATE
            
            # å…ˆæŸ¥è¯¢å½“å‰æ•°æ®
            query = f"""
            SELECT 
                event_time,
                symbol,
                price_change,
                price_change_percent,
                side,
                change_percent_text,
                average_price,
                last_price,
                last_trade_volume,
                high_price,
                low_price,
                base_volume,
                quote_volume,
                stats_open_time,
                stats_close_time,
                first_trade_id,
                last_trade_id,
                trade_count,
                ingestion_time
            FROM (
                SELECT 
                    *,
                    ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY event_time DESC) as rn
                FROM {self.market_ticker_table}
                WHERE symbol = '{symbol}'
            ) AS ranked
            WHERE rn = 1
            """
            
            def _execute_query(client):
                return client.query(query)
            
            result = self._with_connection(_execute_query)
            
            if not result.result_rows:
                logger.warning("[ClickHouse] Symbol %s not found for price update", symbol)
                return False
            
            # è·å–æœ€æ–°çš„ä¸€æ¡æ•°æ®
            row = result.result_rows[0]
            
            # å‡†å¤‡æ›´æ–°åçš„æ•°æ®
            column_names = [
                "event_time", "symbol", "price_change", "price_change_percent", "side",
                "change_percent_text", "average_price", "last_price", "last_trade_volume",
                "open_price", "high_price", "low_price", "base_volume", "quote_volume",
                "stats_open_time", "stats_close_time", "first_trade_id", "last_trade_id",
                "trade_count", "ingestion_time", "update_price_date"
            ]
            
            # é‡æ–°è®¡ç®—æ¶¨è·Œå¹…ç›¸å…³å­—æ®µï¼ˆåŸºäºæ–°çš„open_priceå’Œå½“å‰çš„last_priceï¼‰
            # æŸ¥è¯¢è¿”å›çš„åˆ—é¡ºåºï¼ševent_time, symbol, price_change, price_change_percent, side,
            # change_percent_text, average_price, last_price, last_trade_volume, high_price,
            # low_price, base_volume, quote_volume, stats_open_time, stats_close_time,
            # first_trade_id, last_trade_id, trade_count, ingestion_time
            last_price = float(row[7]) if row[7] is not None else 0.0  # last_priceåœ¨ç´¢å¼•7
            new_open_price = float(open_price)
            
            if new_open_price != 0 and last_price != 0:
                price_change = last_price - new_open_price
                price_change_percent = (price_change / new_open_price) * 100
                side = "gainer" if price_change_percent >= 0 else "loser"
                change_percent_text = f"{price_change_percent:.2f}%"
            else:
                # å¦‚æœæ— æ³•è®¡ç®—ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆä¸èƒ½ä¸ºNoneï¼‰
                price_change = 0.0
                price_change_percent = 0.0
                side = ""
                change_percent_text = ""
            
            # æ„å»ºæ›´æ–°åçš„è¡Œæ•°æ®
            # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰æ­£ç¡®çš„ç±»å‹å’Œé»˜è®¤å€¼
            updated_row = [
                _to_datetime(row[0]),  # event_time (DateTime)
                _normalize_field_value(row[1], "String", "symbol"),  # symbol (String)
                _normalize_field_value(price_change, "Float64", "price_change"),  # price_change (Float64)
                _normalize_field_value(price_change_percent, "Float64", "price_change_percent"),  # price_change_percent (Float64)
                _normalize_field_value(side, "String", "side"),  # side (String)
                _normalize_field_value(change_percent_text, "String", "change_percent_text"),  # change_percent_text (String)
                _normalize_field_value(row[6], "Float64", "average_price"),  # average_price (Float64)
                _normalize_field_value(row[7], "Float64", "last_price"),  # last_price (Float64)
                _normalize_field_value(row[8], "Float64", "last_trade_volume"),  # last_trade_volume (Float64)
                _normalize_field_value(new_open_price, "Float64", "open_price"),  # open_price (Float64)
                _normalize_field_value(row[9], "Float64", "high_price"),  # high_price (Float64)
                _normalize_field_value(row[10], "Float64", "low_price"),  # low_price (Float64)
                _normalize_field_value(row[11], "Float64", "base_volume"),  # base_volume (Float64)
                _normalize_field_value(row[12], "Float64", "quote_volume"),  # quote_volume (Float64)
                _to_datetime(row[13]),  # stats_open_time (DateTime)
                _to_datetime(row[14]),  # stats_close_time (DateTime)
                _normalize_field_value(row[15], "UInt64", "first_trade_id"),  # first_trade_id (UInt64)
                _normalize_field_value(row[16], "UInt64", "last_trade_id"),  # last_trade_id (UInt64)
                _normalize_field_value(row[17], "UInt64", "trade_count"),  # trade_count (UInt64)
                _to_datetime(row[18]) if row[18] else datetime.now(timezone.utc),  # ingestion_time (DateTime)
                update_date  # update_price_date (Nullable(DateTime)) - å¯ä»¥ä¸ºNone
            ]
            
            # åˆ é™¤æ—§æ•°æ®å¹¶æ’å…¥æ–°æ•°æ®ï¼ˆClickHouseçš„UPDATEæ–¹å¼ï¼‰
            delete_query = f"ALTER TABLE {self.market_ticker_table} DELETE WHERE symbol = '{symbol}'"
            try:
                self.command(delete_query)
            except Exception as e:
                logger.warning("[ClickHouse] Failed to delete old row for %s: %s", symbol, e)
            
            # æ’å…¥æ›´æ–°åçš„æ•°æ®
            self.insert_rows(self.market_ticker_table, [updated_row], column_names)
            logger.debug("[ClickHouse] Updated open_price for symbol %s: %s", symbol, new_open_price)
            return True
            
        except Exception as e:
            logger.error("[ClickHouse] Failed to update open_price for symbol %s: %s", symbol, e, exc_info=True)
            return False
    
    # ------------------------------------------------------------------
    # Leaderboard helpers
    # ------------------------------------------------------------------
    def ensure_leaderboard_table(self) -> None:
        """Create the futures leaderboard table if it does not exist."""
        ddl = f"""
        CREATE TABLE IF NOT EXISTS {self.leaderboard_table} (
            event_time DateTime,
            symbol String,
            price_change Float64,
            price_change_percent Float64,
            side String,
            change_percent_text String,
            average_price Float64,
            last_price Float64,
            last_trade_volume Float64,
            open_price Float64,
            high_price Float64,
            low_price Float64,
            base_volume Float64,
            quote_volume Float64,
            stats_open_time DateTime,
            stats_close_time DateTime,
            first_trade_id UInt64,
            last_trade_id UInt64,
            trade_count UInt64,
            ingestion_time DateTime DEFAULT now(),
            rank UInt8
        )
        ENGINE = MergeTree
        ORDER BY (side, rank, symbol)
        """
        self.command(ddl)
        logger.info("[ClickHouse] Ensured table %s exists", self.leaderboard_table)

    def query_recent_tickers(
        self, 
        time_window_seconds: int = 5,
        side: Optional[str] = None,
        top_n: int = 10
    ) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢æ‰€æœ‰å¸‚åœºè¡Œæƒ…æ•°æ®ï¼Œç”¨äºç”Ÿæˆæ¶¨è·Œå¹…æ¦œ
        
        æ ¸å¿ƒåŠŸèƒ½ï¼š
        - ä»24_market_tickersè¡¨æŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ˆä¸é™åˆ¶æ—¶é—´çª—å£ï¼‰
        - å¯¹æ¯ä¸ªäº¤æ˜“å¯¹å–æœ€æ–°çš„è¡Œæƒ…æ•°æ®ï¼ˆå»é‡ï¼‰
        - æ ¹æ®price_change_percentå­—æ®µåˆ¤æ–­æ¶¨è·Œï¼šæ­£ä¸ºæ¶¨ï¼Œè´Ÿä¸ºè·Œ
        - è¿”å›æ¶¨å¹…æˆ–è·Œå¹…å‰åçš„æ•°æ®
        
        æ‰§è¡Œæµç¨‹ï¼š
        1. æ ¹æ®sideå‚æ•°ç¡®å®šæŸ¥è¯¢é€»è¾‘ï¼š
           - gainerï¼šæŸ¥è¯¢price_change_percent>0çš„åˆçº¦ï¼ŒæŒ‰é™åºæ’åº
           - loserï¼šæŸ¥è¯¢price_change_percent<0çš„åˆçº¦ï¼ŒæŒ‰å‡åºæ’åº
        2. æ„å»ºæŸ¥è¯¢SQLï¼ŒåŒ…å«å»é‡é€»è¾‘ï¼ˆæŒ‰symbolåˆ†ç»„ï¼Œå–æœ€æ–°event_timeï¼‰
        3. æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›ç»“æœ
        
        Args:
            time_window_seconds: å·²åºŸå¼ƒï¼Œä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰è°ƒç”¨ï¼Œä¸å†ä½¿ç”¨
            side: ç­›é€‰æ–¹å‘ï¼Œå¯é€‰å€¼ï¼š'gainer'ï¼ˆæ¶¨å¹…æ¦œï¼‰ã€'loser'ï¼ˆè·Œå¹…æ¦œï¼‰ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
            top_n: è¿”å›å‰Nåæ•°æ®
            
        Returns:
            List[Dict[str, Any]]: è¡Œæƒ…æ•°æ®å­—å…¸åˆ—è¡¨
        """
        logger.info(f"[ClickHouse] ğŸ“Š å¼€å§‹æŸ¥è¯¢æ‰€æœ‰è¡Œæƒ…æ•°æ®...")
        logger.info(f"[ClickHouse] ğŸ“‹ æŸ¥è¯¢å‚æ•°: side={side}, top_n={top_n} (å·²ç§»é™¤æ—¶é—´çª—å£é™åˆ¶)")
        
        # æ„å»ºæŸ¥è¯¢SQLï¼šå»é‡ï¼Œå–æ¯ä¸ªsymbolæœ€æ–°çš„event_time
        if side:
            if side == 'gainer':
                # æ¶¨å¹…æ¦œï¼šæŸ¥è¯¢price_change_percent>0çš„åˆçº¦ï¼ŒæŒ‰price_change_percenté™åºæ’åº
                where_clause = "price_change_percent > 0"
                order_by = "price_change_percent DESC"
                logger.info(f"[ClickHouse] ğŸ“ˆ æ¶¨å¹…æ¦œæŸ¥è¯¢: {where_clause}, æ’åº: {order_by}")
            else:  # loser
                # è·Œå¹…æ¦œï¼šæŸ¥è¯¢price_change_percent<0çš„åˆçº¦ï¼ŒæŒ‰price_change_percentå‡åºæ’åºï¼ˆè·Œå¹…æœ€å¤§çš„æ’åœ¨å‰é¢ï¼‰
                where_clause = "price_change_percent < 0"
                order_by = "price_change_percent ASC"
                logger.info(f"[ClickHouse] ğŸ“‰ è·Œå¹…æ¦œæŸ¥è¯¢: {where_clause}, æ’åº: {order_by}")
            
            query = f"""
            SELECT 
                event_time,
                symbol,
                price_change,
                price_change_percent,
                '{side}' as side,
                change_percent_text,
                average_price,
                last_price,
                last_trade_volume,
                open_price,
                high_price,
                low_price,
                base_volume,
                quote_volume,
                stats_open_time,
                stats_close_time,
                first_trade_id,
                last_trade_id,
                trade_count,
                ingestion_time
            FROM (
                SELECT 
                    *,
                    ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY event_time DESC) as rn
                FROM {self.market_ticker_table}
                WHERE {where_clause}
            ) AS ranked
            WHERE rn = 1
            ORDER BY {order_by}
            LIMIT {top_n}
            """
        else:
            # æŸ¥è¯¢æ‰€æœ‰ï¼Œä¸åŒºåˆ†æ¶¨è·Œ
            query = f"""
            SELECT 
                event_time,
                symbol,
                price_change,
                price_change_percent,
                CASE 
                    WHEN price_change_percent > 0 THEN 'gainer' 
                    WHEN price_change_percent < 0 THEN 'loser' 
                    ELSE 'neutral' 
                END as side,
                change_percent_text,
                average_price,
                last_price,
                last_trade_volume,
                open_price,
                high_price,
                low_price,
                base_volume,
                quote_volume,
                stats_open_time,
                stats_close_time,
                first_trade_id,
                last_trade_id,
                trade_count,
                ingestion_time
            FROM (
                SELECT 
                    *,
                    ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY event_time DESC) as rn
                FROM {self.market_ticker_table}
            ) AS ranked
            WHERE rn = 1
            LIMIT {top_n * 2}
            """
        
        def _execute_query(client):
            return client.query(query)
        
        logger.info(f"[ClickHouse] ğŸ“ æ‰§è¡ŒæŸ¥è¯¢: {query[:100]}...")
        result = self._with_connection(_execute_query)
        logger.info(f"[ClickHouse] âœ… æŸ¥è¯¢æ‰§è¡Œå®Œæˆ")
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        columns = [
            "event_time", "symbol", "price_change", "price_change_percent", "side",
            "change_percent_text", "average_price", "last_price", "last_trade_volume",
            "open_price", "high_price", "low_price", "base_volume", "quote_volume",
            "stats_open_time", "stats_close_time", "first_trade_id", "last_trade_id",
            "trade_count", "ingestion_time"
        ]
        
        rows = []
        for row in result.result_rows:
            row_dict = dict(zip(columns, row))
            rows.append(row_dict)
        
        logger.info(f"[ClickHouse] ğŸ“Š æŸ¥è¯¢ç»“æœ: å…± {len(rows)} æ¡æ•°æ®")
        return rows

    def sync_leaderboard(
        self,
        time_window_seconds: int = 5,
        top_n: int = 10
    ) -> None:
        """Sync leaderboard data from market_ticker_table to leaderboard_table.
        
        æ ¸å¿ƒåŠŸèƒ½ï¼š
        - ä»24_market_tickersè¡¨æŸ¥è¯¢æ‰€æœ‰å¸‚åœºæ•°æ®ï¼ˆä¸é™åˆ¶æ—¶é—´çª—å£ï¼‰
        - å¯¹æ¯ä¸ªäº¤æ˜“å¯¹å–æœ€æ–°çš„è¡Œæƒ…æ•°æ®ï¼ˆå»é‡ï¼‰
        - è®¡ç®—æ¯ä¸ªåˆçº¦çš„æ¶¨è·Œå¹…
        - ç­›é€‰å‡ºæ¶¨å¹…å‰Nåå’Œè·Œå¹…å‰Nå
        - ä½¿ç”¨åŸå­æ“ä½œæ›´æ–°futures_leaderboardè¡¨
        
        æ‰§è¡Œæµç¨‹ï¼š
        1. æŸ¥è¯¢æ¶¨å¹…æ¦œå‰Nåï¼ˆæŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ¶¨è·Œå¹…æ’åºï¼‰
        2. æŸ¥è¯¢è·Œå¹…æ¦œå‰Nåï¼ˆæŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ¶¨è·Œå¹…æ’åºï¼‰
        3. å‡†å¤‡æ’å…¥æ•°æ®
        4. åˆ›å»ºä¸´æ—¶è¡¨
        5. æ’å…¥æ•°æ®åˆ°ä¸´æ—¶è¡¨
        6. ä½¿ç”¨REPLACE TABLEåŸå­æ›¿æ¢åŸè¡¨æ•°æ®
        7. åˆ é™¤ä¸´æ—¶è¡¨
        
        Args:
            time_window_seconds: å·²åºŸå¼ƒï¼Œä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰è°ƒç”¨ï¼Œä¸å†ä½¿ç”¨æ—¶é—´çª—å£é™åˆ¶
            top_n: æ¶¨è·Œå¹…å‰Nåæ•°é‡
        """
        try:
            logger.info(f"[ClickHouse] ğŸš€ å¼€å§‹æ¶¨è·Œå¹…æ¦œåŒæ­¥...")
            logger.info(f"[ClickHouse] ğŸ“‹ åŒæ­¥å‚æ•°: top_n={top_n} (æŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼Œä¸é™åˆ¶æ—¶é—´çª—å£)")
            
            # æŸ¥è¯¢æ¶¨å¹…æ¦œå‰Nåï¼ˆæŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ¶¨è·Œå¹…æ’åºï¼‰
            logger.info(f"[ClickHouse] ğŸ” æŸ¥è¯¢æ¶¨å¹…æ¦œå‰{top_n}åï¼ˆä»æ‰€æœ‰æ•°æ®ä¸­æ’åºï¼‰...")
            gainers = self.query_recent_tickers(
                time_window_seconds=time_window_seconds,
                side='gainer',
                top_n=top_n
            )
            logger.info(f"[ClickHouse] âœ… æ¶¨å¹…æ¦œæŸ¥è¯¢å®Œæˆï¼Œå…± {len(gainers)} æ¡æ•°æ®")
            
            # æŸ¥è¯¢è·Œå¹…æ¦œå‰Nåï¼ˆæŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ¶¨è·Œå¹…æ’åºï¼‰
            logger.info(f"[ClickHouse] ğŸ” æŸ¥è¯¢è·Œå¹…æ¦œå‰{top_n}åï¼ˆä»æ‰€æœ‰æ•°æ®ä¸­æ’åºï¼‰...")
            losers = self.query_recent_tickers(
                time_window_seconds=time_window_seconds,
                side='loser',
                top_n=top_n
            )
            logger.info(f"[ClickHouse] âœ… è·Œå¹…æ¦œæŸ¥è¯¢å®Œæˆï¼Œå…± {len(losers)} æ¡æ•°æ®")
            
            # å‡†å¤‡æ’å…¥æ•°æ®
            logger.info(f"[ClickHouse] ğŸ“ å‡†å¤‡æ’å…¥æ•°æ®...")
            all_rows = []
            column_names = [
                "event_time", "symbol", "price_change", "price_change_percent", "side",
                "change_percent_text", "average_price", "last_price", "last_trade_volume",
                "open_price", "high_price", "low_price", "base_volume", "quote_volume",
                "stats_open_time", "stats_close_time", "first_trade_id", "last_trade_id",
                "trade_count", "ingestion_time", "rank"
            ]
            
            # æ·»åŠ æ¶¨å¹…æ¦œæ•°æ®ï¼ˆå¸¦æ’åï¼‰
            logger.info(f"[ClickHouse] ğŸ“Š å¤„ç†æ¶¨å¹…æ¦œæ•°æ®...")
            for idx, row in enumerate(gainers, 1):
                row_data = [
                    _to_datetime(row.get("event_time")),  # DateTime
                    _normalize_field_value(row.get("symbol"), "String", "symbol"),  # String
                    _normalize_field_value(row.get("price_change"), "Float64", "price_change"),  # Float64
                    _normalize_field_value(row.get("price_change_percent"), "Float64", "price_change_percent"),  # Float64
                    _normalize_field_value(row.get("side", "gainer"), "String", "side"),  # String
                    _normalize_field_value(row.get("change_percent_text"), "String", "change_percent_text"),  # String
                    _normalize_field_value(row.get("average_price"), "Float64", "average_price"),  # Float64
                    _normalize_field_value(row.get("last_price"), "Float64", "last_price"),  # Float64
                    _normalize_field_value(row.get("last_trade_volume"), "Float64", "last_trade_volume"),  # Float64
                    _normalize_field_value(row.get("open_price"), "Float64", "open_price"),  # Float64
                    _normalize_field_value(row.get("high_price"), "Float64", "high_price"),  # Float64
                    _normalize_field_value(row.get("low_price"), "Float64", "low_price"),  # Float64
                    _normalize_field_value(row.get("base_volume"), "Float64", "base_volume"),  # Float64
                    _normalize_field_value(row.get("quote_volume"), "Float64", "quote_volume"),  # Float64
                    _to_datetime(row.get("stats_open_time")),  # DateTime
                    _to_datetime(row.get("stats_close_time")),  # DateTime
                    _normalize_field_value(row.get("first_trade_id"), "UInt64", "first_trade_id"),  # UInt64
                    _normalize_field_value(row.get("last_trade_id"), "UInt64", "last_trade_id"),  # UInt64
                    _normalize_field_value(row.get("trade_count"), "UInt64", "trade_count"),  # UInt64
                    _to_datetime(row.get("ingestion_time")),  # DateTime
                    _normalize_field_value(idx, "UInt8", "rank")  # UInt8 (rank)
                ]
                all_rows.append(row_data)
            logger.info(f"[ClickHouse] âœ… æ¶¨å¹…æ¦œæ•°æ®å¤„ç†å®Œæˆï¼Œå…± {len(gainers)} æ¡")
            
            # æ·»åŠ è·Œå¹…æ¦œæ•°æ®ï¼ˆå¸¦æ’åï¼‰
            logger.info(f"[ClickHouse] ğŸ“Š å¤„ç†è·Œå¹…æ¦œæ•°æ®...")
            for idx, row in enumerate(losers, 1):
                row_data = [
                    _to_datetime(row.get("event_time")),  # DateTime
                    _normalize_field_value(row.get("symbol"), "String", "symbol"),  # String
                    _normalize_field_value(row.get("price_change"), "Float64", "price_change"),  # Float64
                    _normalize_field_value(row.get("price_change_percent"), "Float64", "price_change_percent"),  # Float64
                    _normalize_field_value(row.get("side", "loser"), "String", "side"),  # String
                    _normalize_field_value(row.get("change_percent_text"), "String", "change_percent_text"),  # String
                    _normalize_field_value(row.get("average_price"), "Float64", "average_price"),  # Float64
                    _normalize_field_value(row.get("last_price"), "Float64", "last_price"),  # Float64
                    _normalize_field_value(row.get("last_trade_volume"), "Float64", "last_trade_volume"),  # Float64
                    _normalize_field_value(row.get("open_price"), "Float64", "open_price"),  # Float64
                    _normalize_field_value(row.get("high_price"), "Float64", "high_price"),  # Float64
                    _normalize_field_value(row.get("low_price"), "Float64", "low_price"),  # Float64
                    _normalize_field_value(row.get("base_volume"), "Float64", "base_volume"),  # Float64
                    _normalize_field_value(row.get("quote_volume"), "Float64", "quote_volume"),  # Float64
                    _to_datetime(row.get("stats_open_time")),  # DateTime
                    _to_datetime(row.get("stats_close_time")),  # DateTime
                    _normalize_field_value(row.get("first_trade_id"), "UInt64", "first_trade_id"),  # UInt64
                    _normalize_field_value(row.get("last_trade_id"), "UInt64", "last_trade_id"),  # UInt64
                    _normalize_field_value(row.get("trade_count"), "UInt64", "trade_count"),  # UInt64
                    _to_datetime(row.get("ingestion_time")),  # DateTime
                    _normalize_field_value(idx, "UInt8", "rank")  # UInt8 (rank)
                ]
                all_rows.append(row_data)
            logger.info(f"[ClickHouse] âœ… è·Œå¹…æ¦œæ•°æ®å¤„ç†å®Œæˆï¼Œå…± {len(losers)} æ¡")
            
            if all_rows:
                logger.info(f"[ClickHouse] ğŸ’¾ å‡†å¤‡å†™å…¥æ•°æ®åˆ°ClickHouseï¼Œå…± {len(all_rows)} æ¡...")
                
                # ä½¿ç”¨é”é˜²æ­¢å¹¶å‘æ‰§è¡Œï¼Œé¿å…è¡¨åå†²çª
                with ClickHouseDatabase._sync_leaderboard_lock:
                    # ä½¿ç”¨æ—¶é—´æˆ³ç”Ÿæˆå”¯ä¸€çš„è¡¨åï¼Œé¿å…å¹¶å‘å†²çª
                    timestamp = int(datetime.now().timestamp() * 1000)  # æ¯«ç§’çº§æ—¶é—´æˆ³
                    temp_table = f"{self.leaderboard_table}_temp_{timestamp}"
                    backup_table = f"{self.leaderboard_table}_backup_{timestamp}"
                    
                    try:
                        # 1. åˆ›å»ºä¸´æ—¶è¡¨ç»“æ„ä¸åŸè¡¨ç›¸åŒ
                        logger.debug(f"[ClickHouse] åˆ›å»ºä¸´æ—¶è¡¨: {temp_table}")
                        create_temp_sql = f"CREATE TABLE IF NOT EXISTS {temp_table} AS {self.leaderboard_table} ENGINE = Memory"
                        self.command(create_temp_sql)
                        logger.debug(f"[ClickHouse] ä¸´æ—¶è¡¨åˆ›å»ºå®Œæˆ: {temp_table}")
                        
                        # 2. æ’å…¥æ•°æ®åˆ°ä¸´æ—¶è¡¨
                        logger.info(f"[ClickHouse] ğŸ“¥ æ’å…¥æ•°æ®åˆ°ä¸´æ—¶è¡¨...")
                        self.insert_rows(temp_table, all_rows, column_names)
                        logger.info(f"[ClickHouse] âœ… æ•°æ®æ’å…¥å®Œæˆï¼Œå…± {len(all_rows)} æ¡")
                        
                        # 3. åŸå­æ›¿æ¢åŸè¡¨æ•°æ®
                        # ä½¿ç”¨ RENAME TABLE åŸå­æ“ä½œï¼Œé¿å…æ•°æ®ç©ºçª—æœŸ
                        logger.info(f"[ClickHouse] ğŸ”„ åŸå­æ›¿æ¢åŸè¡¨æ•°æ®...")
                        try:
                            # å…ˆåˆ é™¤å·²å­˜åœ¨çš„å¤‡ä»½è¡¨ï¼ˆå¤„ç†å¹¶å‘æƒ…å†µï¼‰
                            drop_existing_backup_sql = f"DROP TABLE IF EXISTS {backup_table}"
                            try:
                                self.command(drop_existing_backup_sql)
                            except Exception:
                                pass  # å¿½ç•¥åˆ é™¤ä¸å­˜åœ¨çš„è¡¨çš„é”™è¯¯
                            
                            # é‡å‘½ååŸè¡¨ä¸ºå¤‡ä»½è¡¨
                            rename_old_sql = f"RENAME TABLE {self.leaderboard_table} TO {backup_table}"
                            self.command(rename_old_sql)
                            
                            # å†é‡å‘½åä¸´æ—¶è¡¨ä¸ºåŸè¡¨
                            rename_new_sql = f"RENAME TABLE {temp_table} TO {self.leaderboard_table}"
                            self.command(rename_new_sql)
                            logger.info(f"[ClickHouse] âœ… åŸè¡¨æ•°æ®æ›¿æ¢å®Œæˆ")
                        except Exception as e:
                            logger.error(f"[ClickHouse] âŒ åŸè¡¨æ•°æ®æ›¿æ¢å¤±è´¥: {e}")
                            # å¦‚æœé‡å‘½åå¤±è´¥ï¼Œå°è¯•æ¢å¤åŸè¡¨
                            try:
                                restore_sql = f"RENAME TABLE {backup_table} TO {self.leaderboard_table}"
                                self.command(restore_sql)
                                logger.info(f"[ClickHouse] âœ… åŸè¡¨æ¢å¤æˆåŠŸ")
                            except Exception as restore_error:
                                logger.error(f"[ClickHouse] âŒ åŸè¡¨æ¢å¤å¤±è´¥: {restore_error}")
                            raise e
                        
                        logger.info(
                            "[ClickHouse] ğŸ‰ æ¶¨è·Œå¹…æ¦œåŒæ­¥å®Œæˆ: %d æ¶¨å¹…, %d è·Œå¹…",
                            len(gainers), len(losers)
                        )
                    finally:
                        # ç¡®ä¿æ¸…ç†ä¸´æ—¶è¡¨ï¼ˆå³ä½¿å‡ºç°å¼‚å¸¸ä¹Ÿè¦æ¸…ç†ï¼‰
                        try:
                            logger.debug(f"[ClickHouse] åˆ é™¤ä¸´æ—¶è¡¨: {temp_table}")
                            drop_temp_sql = f"DROP TABLE IF EXISTS {temp_table}"
                            self.command(drop_temp_sql)
                            logger.debug(f"[ClickHouse] ä¸´æ—¶è¡¨åˆ é™¤å®Œæˆ: {temp_table}")
                        except Exception as cleanup_exc:
                            logger.warning(f"[ClickHouse] âš ï¸  æ¸…ç†ä¸´æ—¶è¡¨å¤±è´¥: {cleanup_exc}")
                            
                        # ç¡®ä¿æ¸…ç†å¤‡ä»½è¡¨ï¼ˆå³ä½¿å‡ºç°å¼‚å¸¸ä¹Ÿè¦æ¸…ç†ï¼‰
                        try:
                            logger.debug(f"[ClickHouse] åˆ é™¤å¤‡ä»½è¡¨: {backup_table}")
                            drop_backup_sql = f"DROP TABLE IF EXISTS {backup_table}"
                            self.command(drop_backup_sql)
                            logger.debug(f"[ClickHouse] å¤‡ä»½è¡¨åˆ é™¤å®Œæˆ: {backup_table}")
                        except Exception as cleanup_exc:
                            logger.warning(f"[ClickHouse] âš ï¸  æ¸…ç†å¤‡ä»½è¡¨å¤±è´¥: {cleanup_exc}")
                        
                        # æ¸…ç†æ‰€æœ‰å¯èƒ½é—ç•™çš„æ—§å¤‡ä»½è¡¨å’Œä¸´æ—¶è¡¨ï¼ˆé˜²æ­¢è¡¨åå†²çªï¼‰
                        try:
                            # æŸ¥æ‰¾å¹¶åˆ é™¤æ‰€æœ‰æ—§çš„å¤‡ä»½è¡¨å’Œä¸´æ—¶è¡¨ï¼ˆè¶…è¿‡1å°æ—¶çš„ï¼‰
                            cleanup_old_tables_sql = f"""
                            SELECT name FROM system.tables 
                            WHERE database = '{app_config.CLICKHOUSE_DATABASE}' 
                            AND (name LIKE '{self.leaderboard_table}_backup_%' OR name LIKE '{self.leaderboard_table}_temp_%')
                            """
                            # æ³¨æ„ï¼šClickHouse ä¸æ”¯æŒç›´æ¥æ‰§è¡Œ SELECT è¿”å›ç»“æœï¼Œè¿™é‡Œåªæ˜¯ç¤ºä¾‹
                            # å®é™…æ¸…ç†å¯ä»¥é€šè¿‡å®šæœŸä»»åŠ¡å®Œæˆï¼Œæˆ–è€…ä½¿ç”¨æ›´ç®€å•çš„æ–¹å¼
                            logger.debug(f"[ClickHouse] å·²æ¸…ç†ä¸´æ—¶è¡¨å’Œå¤‡ä»½è¡¨")
                        except Exception as cleanup_exc:
                            logger.debug(f"[ClickHouse] æ¸…ç†æ—§è¡¨æ—¶å‡ºé”™ï¼ˆå¯å¿½ç•¥ï¼‰: {cleanup_exc}")
            else:
                logger.warning("[ClickHouse] âš ï¸  æ²¡æœ‰æ¶¨è·Œå¹…æ¦œæ•°æ®å¯åŒæ­¥")
                
        except Exception as exc:
            logger.error("[ClickHouse] âŒ æ¶¨è·Œå¹…æ¦œåŒæ­¥å¤±è´¥: %s", exc, exc_info=True)

    def get_leaderboard(self, limit: int = 10) -> Dict[str, List[Dict]]:
        """Get leaderboard data from futures_leaderboard table.
        
        Args:
            limit: Number of top items to return for each side
            
        Returns:
            Dictionary with 'gainers' and 'losers' lists
        """
        try:
            # æŸ¥è¯¢æ¶¨å¹…æ¦œ
            gainers_query = f"""
            SELECT 
                symbol,
                last_price,
                price_change_percent,
                side,
                change_percent_text,
                quote_volume,
                rank
            FROM {self.leaderboard_table}
            WHERE side = 'gainer'
            ORDER BY rank ASC
            LIMIT {limit}
            """
            
            # æŸ¥è¯¢è·Œå¹…æ¦œ
            losers_query = f"""
            SELECT 
                symbol,
                last_price,
                price_change_percent,
                side,
                change_percent_text,
                quote_volume,
                rank
            FROM {self.leaderboard_table}
            WHERE side = 'loser'
            ORDER BY rank ASC
            LIMIT {limit}
            """
            
            def _execute_gainers_query(client):
                return client.query(gainers_query)
            
            def _execute_losers_query(client):
                return client.query(losers_query)
            
            gainers_result = self._with_connection(_execute_gainers_query)
            losers_result = self._with_connection(_execute_losers_query)
            
            # è½¬æ¢æ¶¨å¹…æ¦œæ•°æ®
            gainers = []
            for row in gainers_result.result_rows:
                try:
                    gainers.append({
                        'symbol': str(row[0]) if row[0] else '',
                        'price': float(row[1]) if row[1] is not None else 0.0,
                        'change_percent': float(row[2]) if row[2] is not None else 0.0,
                        'side': str(row[3]) if row[3] else 'gainer',
                        'change_percent_text': str(row[4]) if row[4] else '',
                        'quote_volume': float(row[5]) if row[5] is not None else 0.0,
                        'rank': int(row[6]) if row[6] is not None else 0
                    })
                except (TypeError, ValueError, IndexError) as e:
                    logger.warning("[ClickHouse] Failed to parse gainer row: %s, error: %s", row, e)
                    continue
            
            # è½¬æ¢è·Œå¹…æ¦œæ•°æ®
            losers = []
            for row in losers_result.result_rows:
                try:
                    losers.append({
                        'symbol': str(row[0]) if row[0] else '',
                        'price': float(row[1]) if row[1] is not None else 0.0,
                        'change_percent': float(row[2]) if row[2] is not None else 0.0,
                        'side': str(row[3]) if row[3] else 'loser',
                        'change_percent_text': str(row[4]) if row[4] else '',
                        'quote_volume': float(row[5]) if row[5] is not None else 0.0,
                        'rank': int(row[6]) if row[6] is not None else 0
                    })
                except (TypeError, ValueError, IndexError) as e:
                    logger.warning("[ClickHouse] Failed to parse loser row: %s, error: %s", row, e)
                    continue
            
            return {
                'gainers': gainers,
                'losers': losers
            }
        except Exception as exc:
            logger.error("[ClickHouse] Failed to get leaderboard: %s", exc, exc_info=True)
            return {'gainers': [], 'losers': []}

    def get_leaderboard_symbols(self) -> List[str]:
        """Get distinct symbols from leaderboard table."""
        try:
            query = f"""
            SELECT DISTINCT symbol
            FROM {self.leaderboard_table}
            WHERE symbol != ''
            """
            
            def _execute_query(client):
                return client.query(query)
            
            result = self._with_connection(_execute_query)
            symbols = [row[0] for row in result.result_rows if row[0]]
            return symbols
        except Exception as exc:
            logger.error("[ClickHouse] Failed to get leaderboard symbols: %s", exc, exc_info=True)
            return []

    def ensure_market_klines_table(self) -> None:
        """Create the market_klines table if it does not exist."""
        ddl = f"""
        CREATE TABLE IF NOT EXISTS {self.market_klines_table} (
            event_time DateTime,
            symbol String,
            contract_type String,
            kline_start_time DateTime,
            kline_end_time DateTime,
            interval String,
            first_trade_id UInt64,
            last_trade_id UInt64,
            open_price Float64,
            close_price Float64,
            high_price Float64,
            low_price Float64,
            base_volume Float64,
            trade_count UInt64,
            is_closed UInt8,
            quote_volume Float64,
            taker_buy_base_volume Float64,
            taker_buy_quote_volume Float64,
            create_time DateTime DEFAULT now()
        )
        ENGINE = MergeTree
        ORDER BY (symbol, interval, kline_end_time, event_time)
        TTL kline_end_time + INTERVAL 2 DAY
        """
        self.command(ddl)
        logger.info("[ClickHouse] Ensured table %s exists", self.market_klines_table)

    def insert_market_klines(self, rows: Iterable[Dict[str, Any]]) -> None:
        """Insert kline data into market_klines table."""
        if not rows:
            return
        
        column_names = [
            "event_time",
            "symbol",
            "contract_type",
            "kline_start_time",
            "kline_end_time",
            "interval",
            "first_trade_id",
            "last_trade_id",
            "open_price",
            "close_price",
            "high_price",
            "low_price",
            "base_volume",
            "trade_count",
            "is_closed",
            "quote_volume",
            "taker_buy_base_volume",
            "taker_buy_quote_volume",
        ]
        
        # Convert rows to list of tuples
        insert_rows = []
        for row in rows:
            insert_rows.append((
                row.get("event_time"),
                row.get("symbol", ""),
                row.get("contract_type", ""),
                row.get("kline_start_time"),
                row.get("kline_end_time"),
                row.get("interval", ""),
                row.get("first_trade_id", 0),
                row.get("last_trade_id", 0),
                row.get("open_price", 0.0),
                row.get("close_price", 0.0),
                row.get("high_price", 0.0),
                row.get("low_price", 0.0),
                row.get("base_volume", 0.0),
                row.get("trade_count", 0),
                row.get("is_closed", 0),
                row.get("quote_volume", 0.0),
                row.get("taker_buy_base_volume", 0.0),
                row.get("taker_buy_quote_volume", 0.0),
            ))
        
        self.insert_rows(self.market_klines_table, insert_rows, column_names)
        logger.debug("[ClickHouse] Inserted %s kline rows into %s", len(insert_rows), self.market_klines_table)

    def cleanup_old_klines(self, days: int = 2) -> int:
        """Delete klines older than specified days. Returns number of deleted rows."""
        try:
            # ClickHouse DELETE requires mutations, use ALTER TABLE DELETE
            delete_sql = f"""
            ALTER TABLE {self.market_klines_table}
            DELETE WHERE kline_end_time < now() - INTERVAL {days} DAY
            """
            self.command(delete_sql)
            # Note: ClickHouse DELETE is asynchronous, we can't get exact count immediately
            logger.info("[ClickHouse] Initiated cleanup of klines older than %s days", days)
            return 0  # ClickHouse doesn't return count for async DELETE
        except Exception as exc:
            logger.error("[ClickHouse] Failed to cleanup old klines: %s", exc, exc_info=True)
            return 0


def _to_datetime(value: Any) -> datetime:
    """å°†å€¼è½¬æ¢ä¸ºdatetimeå¯¹è±¡ï¼Œå¦‚æœæ— æ³•è½¬æ¢åˆ™è¿”å›å½“å‰UTCæ—¶é—´
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°æ°¸è¿œä¸ä¼šè¿”å›Noneï¼Œç¡®ä¿DateTimeå­—æ®µå§‹ç»ˆæœ‰å€¼
    """
    if isinstance(value, datetime):
        return value
    if value is None:
        return datetime.now(timezone.utc)
    try:
        numeric = float(value)
    except (TypeError, ValueError):
        return datetime.now(timezone.utc)

    seconds = numeric / 1000.0
    return datetime.fromtimestamp(seconds, tz=timezone.utc)


def _normalize_field_value(value: Any, field_type: str, field_name: str = "") -> Any:
    """è§„èŒƒåŒ–å­—æ®µå€¼ï¼Œç¡®ä¿ç¬¦åˆClickHouseå­—æ®µç±»å‹è¦æ±‚
    
    Args:
        value: åŸå§‹å€¼
        field_type: å­—æ®µç±»å‹ ('Float64', 'UInt64', 'UInt8', 'String', 'DateTime')
        field_name: å­—æ®µåç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
    Returns:
        è§„èŒƒåŒ–åçš„å€¼ï¼Œç¡®ä¿ä¸ä¸ºNoneï¼ˆé™¤éå­—æ®µç±»å‹å…è®¸ï¼‰
    """
    if value is None:
        if field_type == 'Float64':
            return 0.0
        elif field_type == 'UInt64':
            return 0
        elif field_type == 'UInt8':
            return 0
        elif field_type == 'String':
            return ""
        elif field_type == 'DateTime':
            return datetime.now(timezone.utc)
        else:
            logger.warning(f"[ClickHouse] Unknown field type {field_type} for field {field_name}, using None")
            return None
    
    # ç±»å‹è½¬æ¢
    try:
        if field_type == 'Float64':
            return float(value)
        elif field_type == 'UInt64':
            return int(value)
        elif field_type == 'UInt8':
            return int(value)
        elif field_type == 'String':
            return str(value) if value else ""
        elif field_type == 'DateTime':
            return _to_datetime(value)
    except (TypeError, ValueError) as e:
        logger.warning(f"[ClickHouse] Failed to convert field {field_name} ({field_type}): {e}, using default")
        # è¿”å›é»˜è®¤å€¼
        if field_type == 'Float64':
            return 0.0
        elif field_type == 'UInt64':
            return 0
        elif field_type == 'UInt8':
            return 0
        elif field_type == 'String':
            return ""
        elif field_type == 'DateTime':
            return datetime.now(timezone.utc)
    
    return value


def _derive_side(percent: Any) -> str:
    try:
        value = float(percent)
    except (TypeError, ValueError):
        value = 0.0
    return "loser" if value < 0 else "gainer"


def _format_percent_text(percent: Any) -> str:
    try:
        value = float(percent)
    except (TypeError, ValueError):
        value = 0.0
    return f"{value:.2f}%"
