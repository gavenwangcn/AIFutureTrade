"""ClickHouse database utilities for market data storage."""
from __future__ import annotations

import logging
import threading
from datetime import datetime, timezone, timedelta
from queue import Queue, Empty
from typing import Any, Dict, Iterable, List, Optional, Callable

import clickhouse_connect
import config as app_config

MARKET_TICKER_TABLE = "24_market_tickers"
LEADERBOARD_TABLE = "futures_leaderboard"
MARKET_KLINES_TABLE = "market_klines"

logger = logging.getLogger(__name__)


class ClickHouseConnectionPool:
    """ClickHouse connection pool to manage client instances.
    
    This class manages a pool of ClickHouse client instances to avoid creating
    too many connections to the ClickHouse server. It provides methods to acquire
    and release connections, and supports dynamic expansion up to a maximum
    number of connections.
    """
    
    def __init__(
        self,
        host: str,
        port: int,
        username: str,
        password: str,
        database: str,
        secure: bool,
        min_connections: int = 5,
        max_connections: int = 20,
        connection_timeout: int = 30
    ):
        """Initialize the connection pool.
        
        Args:
            host: ClickHouse host
            port: ClickHouse port
            username: ClickHouse username
            password: ClickHouse password
            database: ClickHouse database
            secure: Whether to use secure connection
            min_connections: Minimum number of connections to keep in the pool
            max_connections: Maximum number of connections allowed in the pool
            connection_timeout: Timeout for acquiring a connection (seconds)
        """
        self._host = host
        self._port = port
        self._username = username
        self._password = password
        self._database = database
        self._secure = secure
        self._min_connections = min_connections
        self._max_connections = max_connections
        self._connection_timeout = connection_timeout
        
        # Create a queue to hold the connections
        self._pool = Queue(maxsize=max_connections)
        
        # Create a lock to protect the connection count
        self._lock = threading.Lock()
        
        # Current number of connections in the pool
        self._current_connections = 0
        
        # Initialize the pool with min_connections
        self._initialize_pool()
    
    def _initialize_pool(self):
        """Initialize the pool with minimum connections."""
        for _ in range(self._min_connections):
            self._create_connection()
    
    def _create_connection(self):
        """Create a new ClickHouse client connection."""
        with self._lock:
            if self._current_connections >= self._max_connections:
                return None
            
            try:
                client = clickhouse_connect.get_client(
                    host=self._host,
                    port=self._port,
                    username=self._username,
                    password=self._password,
                    database=self._database,
                    secure=self._secure
                )
                self._pool.put(client)
                self._current_connections += 1
                logger.debug(f"[ClickHouse] Created new connection. Current connections: {self._current_connections}")
                return client
            except Exception as e:
                logger.error(f"[ClickHouse] Failed to create connection: {e}")
                return None
    
    def acquire(self, timeout: Optional[int] = None) -> Optional[Any]:
        """Acquire a connection from the pool.
        
        Args:
            timeout: Timeout for acquiring a connection (seconds). If None, use the default timeout.
            
        Returns:
            A ClickHouse client instance, or None if no connection is available within the timeout.
        """
        timeout = timeout or self._connection_timeout
        
        try:
            # Try to get a connection from the pool
            client = self._pool.get(timeout=timeout)
            logger.debug(f"[ClickHouse] Acquired connection from pool")
            return client
        except Empty:
            # If the pool is empty, try to create a new connection
            logger.debug(f"[ClickHouse] Pool is empty, creating new connection")
            client = self._create_connection()
            if client:
                return client
            
            # If we can't create a new connection, try again to get from the pool
            try:
                client = self._pool.get(timeout=timeout)
                logger.debug(f"[ClickHouse] Acquired connection from pool after waiting")
                return client
            except Empty:
                logger.error(f"[ClickHouse] Failed to acquire connection within timeout {timeout} seconds")
                return None
    
    def release(self, client: Any) -> None:
        """Release a connection back to the pool.
        
        Args:
            client: The ClickHouse client instance to release
        """
        try:
            # Check if the client is still valid by sending a simple query
            client.query("SELECT 1")
            self._pool.put(client)
            logger.debug(f"[ClickHouse] Released connection back to pool")
        except Exception as e:
            # If the client is invalid, close it and don't return it to the pool
            logger.error(f"[ClickHouse] Connection is invalid, closing it: {e}")
            with self._lock:
                self._current_connections -= 1
    
    def close_all(self) -> None:
        """Close all connections in the pool."""
        with self._lock:
            while not self._pool.empty():
                try:
                    client = self._pool.get_nowait()
                    client.close()
                    self._current_connections -= 1
                except Exception as e:
                    logger.error(f"[ClickHouse] Failed to close connection: {e}")
            
            logger.info(f"[ClickHouse] Closed all connections. Current connections: {self._current_connections}")


class ClickHouseDatabase:
    """Encapsulates ClickHouse connectivity and CRUD helpers."""
    
    # ç±»çº§åˆ«çš„é”ï¼Œç”¨äºé˜²æ­¢å¹¶å‘æ‰§è¡Œ sync_leaderboard
    _sync_leaderboard_lock = threading.Lock()

    def __init__(self, *, auto_init_tables: bool = True) -> None:
        # Create a connection pool instead of individual client instances
        self._pool = ClickHouseConnectionPool(
            host=app_config.CLICKHOUSE_HOST,
            port=app_config.CLICKHOUSE_PORT,
            username=app_config.CLICKHOUSE_USER,
            password=app_config.CLICKHOUSE_PASSWORD,
            database=app_config.CLICKHOUSE_DATABASE,
            secure=app_config.CLICKHOUSE_SECURE,
            min_connections=5,
            max_connections=50,
            connection_timeout=30
        )
        
        self.market_ticker_table = MARKET_TICKER_TABLE
        self.leaderboard_table = getattr(app_config, 'CLICKHOUSE_LEADERBOARD_TABLE', LEADERBOARD_TABLE)

        # Kçº¿è¡¨å‰ç¼€ï¼ˆé»˜è®¤ market_klinesï¼‰ï¼ŒæŒ‰ä¸åŒ interval æ‹†åˆ†ä¸ºå¤šå¼ è¡¨ï¼š
        # market_klines_1w, market_klines_1d, market_klines_4h, market_klines_1h,
        # market_klines_15m, market_klines_5m, market_klines_1m
        self.market_klines_table: str = getattr(
            app_config,
            "CLICKHOUSE_MARKET_KLINES_TABLE",
            MARKET_KLINES_TABLE,
        )
        prefix = self.market_klines_table
        self.market_klines_tables: Dict[str, str] = {
            "1w": f"{prefix}_1w",
            "1d": f"{prefix}_1d",
            "4h": f"{prefix}_4h",
            "1h": f"{prefix}_1h",
            "15m": f"{prefix}_15m",
            "5m": f"{prefix}_5m",
            "1m": f"{prefix}_1m",
        }
        
        if auto_init_tables:
            # Initialize tables using the connection pool
            self.ensure_market_ticker_table()
            self.ensure_leaderboard_table()
            self.ensure_market_klines_table()
    
    def _with_connection(self, func: Callable, *args, **kwargs) -> Any:
        """Execute a function with a ClickHouse connection from the pool.
        
        This method acquires a connection from the pool, executes the given function
        with the connection as the first argument, and then releases the connection back to the pool.
        
        Args:
            func: The function to execute
            *args: Positional arguments to pass to the function
            **kwargs: Keyword arguments to pass to the function
            
        Returns:
            The result of the function call
        """
        client = self._pool.acquire()
        if not client:
            raise Exception("Failed to acquire ClickHouse connection")
        
        try:
            return func(client, *args, **kwargs)
        finally:
            self._pool.release(client)

    # ------------------------------------------------------------------
    # Generic helpers
    # ------------------------------------------------------------------
    def command(self, sql: str) -> None:
        """Execute a raw SQL command."""
        def _execute_command(client):
            client.command(sql)
        
        self._with_connection(_execute_command)
    
    def _check_table_exists(self, table_name: str) -> bool:
        """Check if a table exists in ClickHouse.
        
        Args:
            table_name: The name of the table to check
            
        Returns:
            True if the table exists, False otherwise
        """
        def _execute_check(client):
            try:
                # æŸ¥è¯¢ç³»ç»Ÿè¡¨æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                check_sql = f"""
                SELECT count() 
                FROM system.tables 
                WHERE database = currentDatabase() 
                AND name = '{table_name}'
                """
                result = client.query(check_sql)
                return result.result_rows[0][0] > 0 if result.result_rows else False
            except Exception as e:
                logger.warning(f"[ClickHouse] æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {e}")
                return False
        
        return self._with_connection(_execute_check)

    def insert_rows(
        self,
        table: str,
        rows: Iterable[Iterable[Any]],
        column_names: List[str],
    ) -> None:
        payload = list(rows)
        if not payload:
            return
        
        def _execute_insert(client):
            client.insert(table, payload, column_names=column_names)
            logger.debug("[ClickHouse] Inserted %s rows into %s", len(payload), table)
        
        self._with_connection(_execute_insert)

    # ------------------------------------------------------------------
    # Market ticker helpers
    # ------------------------------------------------------------------
    def ensure_market_ticker_table(self) -> None:
        """Create the 24h market ticker table if it does not exist."""
        ddl = f"""
        CREATE TABLE IF NOT EXISTS {self.market_ticker_table} (
            event_time DateTime,
            symbol String,
            price_change Float64,
            price_change_percent Float64,
            side String,
            change_percent_text String,
            average_price Float64,
            last_price Float64,
            last_trade_volume Float64,
            open_price Float64,
            high_price Float64,
            low_price Float64,
            base_volume Float64,
            quote_volume Float64,
            stats_open_time DateTime,
            stats_close_time DateTime,
            first_trade_id UInt64,
            last_trade_id UInt64,
            trade_count UInt64,
            ingestion_time DateTime DEFAULT now(),
            update_price_date Nullable(DateTime)
        )
        ENGINE = MergeTree
        ORDER BY (symbol, stats_close_time, event_time)
        """
        self.command(ddl)
        logger.info("[ClickHouse] Ensured table %s exists", self.market_ticker_table)
        
        # å¦‚æœè¡¨å·²å­˜åœ¨ï¼Œæ·»åŠ æ–°å­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰æˆ–ä¿®æ”¹å­—æ®µç±»å‹ä¸ºNullable
        try:
            # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨
            check_column_sql = f"""
            SELECT name, type FROM system.columns 
            WHERE database = '{app_config.CLICKHOUSE_DATABASE}' 
            AND table = '{self.market_ticker_table}' 
            AND name = 'update_price_date'
            """
            def _check_column(client):
                result = client.query(check_column_sql)
                if len(result.result_rows) > 0:
                    return result.result_rows[0][1]  # è¿”å›å­—æ®µç±»å‹
                return None
            
            column_type = self._with_connection(_check_column)
            
            if column_type is None:
                # å­—æ®µä¸å­˜åœ¨ï¼Œæ·»åŠ ä¸ºNullable
                add_column_sql = f"""
                ALTER TABLE {self.market_ticker_table} 
                ADD COLUMN IF NOT EXISTS update_price_date Nullable(DateTime)
                """
                self.command(add_column_sql)
                logger.info("[ClickHouse] Added update_price_date column to %s", self.market_ticker_table)
            elif 'Nullable' not in str(column_type):
                # å­—æ®µå­˜åœ¨ä½†ä¸æ˜¯Nullableï¼Œå°è¯•ä¿®æ”¹ä¸ºNullable
                try:
                    modify_column_sql = f"""
                    ALTER TABLE {self.market_ticker_table} 
                    MODIFY COLUMN update_price_date Nullable(DateTime)
                    """
                    self.command(modify_column_sql)
                    logger.info("[ClickHouse] Modified update_price_date column to Nullable(DateTime)")
                except Exception as modify_exc:
                    logger.warning("[ClickHouse] Failed to modify update_price_date to Nullable (may not be supported): %s", modify_exc)
        except Exception as e:
            logger.warning("[ClickHouse] Failed to check/modify update_price_date column: %s", e)

    def get_existing_symbol_data(self, symbols: List[str]) -> Dict[str, Dict[str, Any]]:
        """æŸ¥è¯¢ç°æœ‰symbolçš„æ•°æ®ï¼Œè¿”å›å­—å…¸ {symbol: {open_price: ..., update_price_date: ..., ...}}
        
        æ³¨æ„ï¼šä¸ºäº†ä¿æŒåŸæœ‰é€»è¾‘ï¼Œå¦‚æœopen_priceä¸º0.0ä¸”update_price_dateä¸ºNoneï¼Œ
        åˆ™è§†ä¸º"æœªè®¾ç½®"ï¼Œè¿”å›open_price=Noneè€Œä¸æ˜¯0.0
        """
        if not symbols:
            return {}
        
        try:
            symbols_str = "', '".join(symbols)
            query = f"""
            SELECT 
                symbol,
                open_price,
                last_price,
                update_price_date
            FROM (
                SELECT 
                    *,
                    ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY event_time DESC) as rn
                FROM {self.market_ticker_table}
                WHERE symbol IN ('{symbols_str}')
            ) AS ranked
            WHERE rn = 1
            """
            
            def _execute_query(client):
                return client.query(query)
            
            result = self._with_connection(_execute_query)
            
            symbol_data = {}
            for row in result.result_rows:
                symbol = row[0]
                open_price_raw = row[1]  # å¯èƒ½æ˜¯0.0æˆ–å®é™…ä»·æ ¼
                last_price = row[2] if row[2] is not None else None
                update_price_date = row[3] if len(row) > 3 else None
                
                # å…³é”®é€»è¾‘ï¼šå¦‚æœopen_priceä¸º0.0ä¸”update_price_dateä¸ºNoneï¼Œè§†ä¸º"æœªè®¾ç½®"
                # è¿”å›Noneä»¥ä¿æŒåŸæœ‰åˆ¤æ–­é€»è¾‘çš„æ­£ç¡®æ€§
                if open_price_raw == 0.0 and update_price_date is None:
                    open_price = None  # è§†ä¸ºæœªè®¾ç½®
                else:
                    open_price = open_price_raw if open_price_raw is not None else None
                
                symbol_data[symbol] = {
                    "open_price": open_price,
                    "last_price": last_price,
                    "update_price_date": update_price_date
                }
            
            return symbol_data
        except Exception as e:
            logger.warning("[ClickHouse] Failed to get existing symbol data: %s", e)
            return {}

    def insert_market_tickers(self, rows: Iterable[Dict[str, Any]]) -> None:
        column_names = [
            "event_time",
            "symbol",
            "price_change",
            "price_change_percent",
            "side",
            "change_percent_text",
            "average_price",
            "last_price",
            "last_trade_volume",
            "open_price",
            "high_price",
            "low_price",
            "base_volume",
            "quote_volume",
            "stats_open_time",
            "stats_close_time",
            "first_trade_id",
            "last_trade_id",
            "trade_count",
            "update_price_date",
        ]

        prepared_rows: List[List[Any]] = []
        for row in rows:
            normalized = dict(row)
            
            # é‡è¦ï¼šç§»é™¤æ¥å£æ•°æ®ä¸­çš„ open_price å’Œ update_price_date å­—æ®µ
            # è¿™ä¸¤ä¸ªå­—æ®µåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°ï¼Œæ¥å£æ•°æ®ä¸èƒ½è¦†ç›–å®ƒä»¬
            if "open_price" in normalized:
                del normalized["open_price"]
                logger.debug("[ClickHouse] ç§»é™¤æ¥å£æ•°æ®ä¸­çš„ open_price å­—æ®µï¼ˆåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°ï¼‰")
            if "update_price_date" in normalized:
                del normalized["update_price_date"]
                logger.debug("[ClickHouse] ç§»é™¤æ¥å£æ•°æ®ä¸­çš„ update_price_date å­—æ®µï¼ˆåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°ï¼‰")
            
            normalized["event_time"] = _to_datetime(normalized.get("event_time"))
            normalized["stats_open_time"] = _to_datetime(normalized.get("stats_open_time"))
            normalized["stats_close_time"] = _to_datetime(normalized.get("stats_close_time"))
            
            # ç¡®ä¿æ‰€æœ‰Float64å­—æ®µä¸ä¸ºNoneï¼Œä½¿ç”¨0.0ä½œä¸ºé»˜è®¤å€¼
            # Float64å­—æ®µåˆ—è¡¨ï¼šprice_change, price_change_percent, average_price, last_price,
            # last_trade_volume, open_price, high_price, low_price, base_volume, quote_volume
            float64_fields = [
                "price_change", "price_change_percent", "average_price", "last_price",
                "last_trade_volume", "open_price", "high_price", "low_price",
                "base_volume", "quote_volume"
            ]
            for field in float64_fields:
                if normalized.get(field) is None:
                    normalized[field] = 0.0
            
            # ç¡®ä¿UInt64å­—æ®µä¸ä¸ºNone
            uint64_fields = ["first_trade_id", "last_trade_id", "trade_count"]
            for field in uint64_fields:
                if normalized.get(field) is None:
                    normalized[field] = 0
            
            # ç¡®ä¿Stringå­—æ®µä¸ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä½œä¸ºé»˜è®¤å€¼
            # Stringå­—æ®µï¼šside, change_percent_textï¼ˆè¡¨ç»“æ„ä¸­è¿™äº›å­—æ®µä¸æ˜¯Nullableï¼‰
            string_fields = ["side", "change_percent_text"]
            for field in string_fields:
                if normalized.get(field) is None:
                    normalized[field] = ""
            
            # DateTimeå­—æ®µå¤„ç†ï¼šç¡®ä¿æ‰€æœ‰éNullableçš„DateTimeå­—æ®µéƒ½ä¸ä¸ºNone
            # event_time, stats_open_time, stats_close_time å·²ç»åœ¨å‰é¢é€šè¿‡_to_datetimeå¤„ç†è¿‡äº†
            # ingestion_time å¦‚æœæ²¡æœ‰å€¼ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
            # update_price_date å¯ä»¥ä¸ºNoneï¼ˆå› ä¸ºè¡¨ç»“æ„ä¸­æ˜¯Nullable(DateTime)ï¼‰
            datetime_fields = ["event_time", "stats_open_time", "stats_close_time"]
            for field in datetime_fields:
                if normalized.get(field) is None:
                    normalized[field] = datetime.now(timezone.utc)
            
            # ingestion_time å­—æ®µå¤„ç†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if "ingestion_time" in normalized and normalized.get("ingestion_time") is None:
                normalized["ingestion_time"] = datetime.now(timezone.utc)
            
            # update_price_date å¯ä»¥ä¸ºNoneï¼ˆNullableå­—æ®µï¼‰
            normalized.setdefault("update_price_date", None)
            
            # å‡†å¤‡è¡Œæ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰å€¼
            row_data = []
            for name in column_names:
                value = normalized.get(name)
                # å¯¹äºéNullableçš„DateTimeå­—æ®µï¼Œç¡®ä¿ä¸ä¸ºNone
                if name in ["event_time", "stats_open_time", "stats_close_time", "ingestion_time"]:
                    if value is None:
                        value = datetime.now(timezone.utc)
                row_data.append(value)
            
            prepared_rows.append(row_data)

        self.insert_rows(self.market_ticker_table, prepared_rows, column_names)
        
    def upsert_market_tickers(self, rows: Iterable[Dict[str, Any]]) -> None:
        """Update or insert market tickers. If symbol exists, update it, otherwise insert.
        
        å¤„ç†é€»è¾‘ï¼š
        1. ç¬¬ä¸€æ¬¡æ’å…¥æ—¶ï¼Œprice_change, price_change_percent, side, change_percent_text, open_price éƒ½ä¸ºç©º
        2. æ›´æ–°æ—¶ï¼Œå¦‚æœ open_price æœ‰å€¼ï¼Œåˆ™é€šè¿‡ last_price å’Œ open_price è®¡ç®—æ¶¨è·Œå¹…ç›¸å…³å­—æ®µ
        
        Args:
            rows: Iterable of ticker dictionaries
        """
        if not rows:
            return
            
        # Filter rows to only include symbols ending with "USDT"
        usdt_rows = [row for row in rows if row.get("symbol", "").endswith("USDT")]
        if not usdt_rows:
            logger.debug("[ClickHouse] No USDT symbols to upsert")
            return
            
        column_names = [
            "event_time",
            "symbol",
            "price_change",
            "price_change_percent",
            "side",
            "change_percent_text",
            "average_price",
            "last_price",
            "last_trade_volume",
            "open_price",
            "high_price",
            "low_price",
            "base_volume",
            "quote_volume",
            "stats_open_time",
            "stats_close_time",
            "first_trade_id",
            "last_trade_id",
            "trade_count",
            "update_price_date",
        ]

        # å¤„ç†æ•°æ®ï¼šå¯¹äºåŒä¸€æ‰¹æ•°æ®ä¸­çš„é‡å¤symbolï¼Œåªä¿ç•™æœ€æ–°çš„ä¸€æ¡ï¼ˆåŸºäºstats_close_timeï¼‰
        symbol_data_map: Dict[str, Dict[str, Any]] = {}
        
        for row in usdt_rows:
            normalized = dict(row)
            
            # é‡è¦ï¼šç§»é™¤æ¥å£æ•°æ®ä¸­çš„ open_price å’Œ update_price_date å­—æ®µ
            # è¿™ä¸¤ä¸ªå­—æ®µåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°ï¼Œæ¥å£æ•°æ®ä¸èƒ½è¦†ç›–å®ƒä»¬
            if "open_price" in normalized:
                del normalized["open_price"]
                logger.debug("[ClickHouse] ç§»é™¤æ¥å£æ•°æ®ä¸­çš„ open_price å­—æ®µï¼ˆåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°ï¼‰")
            if "update_price_date" in normalized:
                del normalized["update_price_date"]
                logger.debug("[ClickHouse] ç§»é™¤æ¥å£æ•°æ®ä¸­çš„ update_price_date å­—æ®µï¼ˆåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°ï¼‰")
            
            normalized["event_time"] = _to_datetime(normalized.get("event_time"))
            normalized["stats_open_time"] = _to_datetime(normalized.get("stats_open_time"))
            normalized["stats_close_time"] = _to_datetime(normalized.get("stats_close_time"))
            
            symbol = normalized.get("symbol")
            if not symbol:
                continue
            
            stats_close_time = normalized.get("stats_close_time")
            
            # å¦‚æœè¯¥symbolå·²å­˜åœ¨ï¼Œæ¯”è¾ƒstats_close_timeï¼Œåªä¿ç•™æœ€æ–°çš„
            if symbol in symbol_data_map:
                existing_stats_close_time = symbol_data_map[symbol].get("stats_close_time")
                if stats_close_time and existing_stats_close_time:
                    if stats_close_time > existing_stats_close_time:
                        symbol_data_map[symbol] = normalized
                elif stats_close_time:
                    # å½“å‰æ•°æ®æœ‰stats_close_timeï¼Œä¿ç•™å½“å‰æ•°æ®
                    symbol_data_map[symbol] = normalized
            else:
                symbol_data_map[symbol] = normalized
        
        if not symbol_data_map:
            return
        
        # æŸ¥è¯¢ç°æœ‰symbolçš„æ•°æ®ï¼Œè·å–open_price
        symbols_to_upsert = list(symbol_data_map.keys())
        existing_data = self.get_existing_symbol_data(symbols_to_upsert)
        
        # å‡†å¤‡æ’å…¥æ•°æ®ï¼ˆæ¯ä¸ªsymbolåªæœ‰ä¸€æ¡æœ€æ–°æ•°æ®ï¼‰
        prepared_rows: List[List[Any]] = []
        
        for symbol, normalized in symbol_data_map.items():
            # è·å–å½“å‰æŠ¥æ–‡çš„last_price
            try:
                current_last_price = float(normalized.get("last_price", 0))
            except (TypeError, ValueError):
                current_last_price = 0.0
            
            # åˆ¤æ–­æ˜¯æ’å…¥è¿˜æ˜¯æ›´æ–°
            existing_symbol_data = existing_data.get(symbol)
            existing_open_price = existing_symbol_data.get("open_price") if existing_symbol_data else None
            existing_update_price_date = existing_symbol_data.get("update_price_date") if existing_symbol_data else None
            
            # å…³é”®é€»è¾‘ï¼šåˆ¤æ–­open_priceæ˜¯å¦å·²è®¾ç½®
            # existing_open_priceä¸ºNoneè¡¨ç¤ºæœªè®¾ç½®ï¼ˆå³ä½¿æ•°æ®åº“ä¸­å­˜å‚¨çš„æ˜¯0.0ï¼Œå¦‚æœupdate_price_dateä¸ºNoneä¹Ÿè§†ä¸ºæœªè®¾ç½®ï¼‰
            # existing_open_priceä¸ä¸ºNoneä¸”ä¸ä¸º0è¡¨ç¤ºå·²è®¾ç½®ä¸”æœ‰æœ‰æ•ˆå€¼
            # å¦‚æœæ˜¯æ›´æ–°ä¸”open_priceæœ‰å€¼ï¼ˆä¸ä¸ºNoneä¸”ä¸ä¸º0ï¼‰ï¼Œåˆ™è®¡ç®—æ¶¨è·Œå¹…ç›¸å…³å­—æ®µ
            if existing_open_price is not None and existing_open_price != 0 and current_last_price != 0:
                try:
                    existing_open_price_float = float(existing_open_price)
                    current_last_price_float = float(current_last_price)
                    
                    # è®¡ç®— price_change = last_price - open_price
                    price_change = current_last_price_float - existing_open_price_float
                    
                    # è®¡ç®— price_change_percent = (last_price - open_price) / open_price * 100
                    price_change_percent = (price_change / existing_open_price_float) * 100
                    
                    # æ ¹æ®æ­£è´Ÿè®¾ç½® sideï¼ˆ0ä¸ºæ­£ï¼Œå³gainerï¼‰
                    side = "gainer" if price_change_percent >= 0 else "loser"
                    
                    # è®¾ç½® change_percent_text = price_change_percent + "%"
                    change_percent_text = f"{price_change_percent:.2f}%"
                    
                    # é‡è¦ï¼šä¿æŒåŸæœ‰çš„open_priceå’Œupdate_price_dateï¼Œä¸æ›´æ–°
                    # è¿™ä¸¤ä¸ªå­—æ®µåªèƒ½ç”±å¼‚æ­¥ä»·æ ¼åˆ·æ–°æœåŠ¡æ›´æ–°ï¼Œæ¥å£æ•°æ®ä¸èƒ½è¦†ç›–å®ƒä»¬
                    normalized["price_change"] = price_change
                    normalized["price_change_percent"] = price_change_percent
                    normalized["side"] = side
                    normalized["change_percent_text"] = change_percent_text
                    normalized["open_price"] = existing_open_price_float  # ä¿ç•™æ•°æ®åº“ä¸­çš„å€¼
                    normalized["update_price_date"] = existing_update_price_date  # ä¿ç•™æ•°æ®åº“ä¸­çš„å€¼ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
                except (TypeError, ValueError) as e:
                    logger.warning("[ClickHouse] Failed to calculate price change for symbol %s: %s", symbol, e)
                    # è®¡ç®—å¤±è´¥æ—¶ï¼Œè®¾ç½®ä¸º0.0ï¼ˆFloat64å­—æ®µä¸èƒ½ä¸ºNoneï¼‰
                    normalized["price_change"] = 0.0
                    normalized["price_change_percent"] = 0.0
                    normalized["side"] = ""  # Stringå­—æ®µä¸èƒ½ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                    normalized["change_percent_text"] = ""  # Stringå­—æ®µä¸èƒ½ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                    # é‡è¦ï¼šä¿ç•™æ•°æ®åº“ä¸­çš„open_priceå’Œupdate_price_date
                    normalized["open_price"] = existing_open_price_float if existing_open_price_float else 0.0
                    normalized["update_price_date"] = existing_update_price_date  # ä¿ç•™æ•°æ®åº“ä¸­çš„å€¼ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
            else:
                # ç¬¬ä¸€æ¬¡æ’å…¥æˆ–open_priceæœªè®¾ç½®çš„æƒ…å†µ
                # Float64å­—æ®µè®¾ç½®ä¸º0.0è€Œä¸æ˜¯Noneï¼ˆå› ä¸ºClickHouse Float64ä¸æ¥å—Noneï¼‰
                # ä½†é€»è¾‘ä¸Šï¼Œopen_price=0.0ä¸”update_price_date=Noneè¡¨ç¤º"æœªè®¾ç½®"
                # è¿™æ ·ä¸‹æ¬¡æŸ¥è¯¢æ—¶ï¼Œget_existing_symbol_dataä¼šè¿”å›open_price=Noneï¼Œä¿æŒåŸæœ‰åˆ¤æ–­é€»è¾‘æ­£ç¡®
                normalized["price_change"] = 0.0
                normalized["price_change_percent"] = 0.0
                normalized["side"] = ""  # Stringå­—æ®µä¸èƒ½ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                normalized["change_percent_text"] = ""  # Stringå­—æ®µä¸èƒ½ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                # é‡è¦ï¼šå¦‚æœæ˜¯æ›´æ–°æ“ä½œï¼Œä¿ç•™æ•°æ®åº“ä¸­çš„update_price_dateï¼›å¦‚æœæ˜¯æ’å…¥æ“ä½œï¼Œè®¾ç½®ä¸ºNone
                normalized["open_price"] = 0.0  # å­˜å‚¨ä¸º0.0ï¼Œä½†é€»è¾‘ä¸Šè§†ä¸º"æœªè®¾ç½®"ï¼ˆå› ä¸ºupdate_price_date=Noneï¼‰
                normalized["update_price_date"] = existing_update_price_date if existing_symbol_data else None
            
            # ç¡®ä¿æ‰€æœ‰Float64å­—æ®µä¸ä¸ºNoneï¼Œä½¿ç”¨0.0ä½œä¸ºé»˜è®¤å€¼
            # Float64å­—æ®µåˆ—è¡¨ï¼šprice_change, price_change_percent, average_price, last_price,
            # last_trade_volume, open_price, high_price, low_price, base_volume, quote_volume
            float64_fields = [
                "price_change", "price_change_percent", "average_price", "last_price",
                "last_trade_volume", "open_price", "high_price", "low_price",
                "base_volume", "quote_volume"
            ]
            for field in float64_fields:
                if normalized.get(field) is None:
                    normalized[field] = 0.0
            
            # ç¡®ä¿UInt64å­—æ®µä¸ä¸ºNone
            uint64_fields = ["first_trade_id", "last_trade_id", "trade_count"]
            for field in uint64_fields:
                if normalized.get(field) is None:
                    normalized[field] = 0
            
            # ç¡®ä¿Stringå­—æ®µä¸ä¸ºNoneï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä½œä¸ºé»˜è®¤å€¼
            # Stringå­—æ®µï¼šside, change_percent_textï¼ˆè¡¨ç»“æ„ä¸­è¿™äº›å­—æ®µä¸æ˜¯Nullableï¼‰
            string_fields = ["side", "change_percent_text"]
            for field in string_fields:
                if normalized.get(field) is None:
                    normalized[field] = ""
            
            # DateTimeå­—æ®µå¤„ç†ï¼šç¡®ä¿æ‰€æœ‰éNullableçš„DateTimeå­—æ®µéƒ½ä¸ä¸ºNone
            # event_time, stats_open_time, stats_close_time å·²ç»åœ¨å‰é¢é€šè¿‡_to_datetimeå¤„ç†è¿‡äº†
            # ingestion_time å¦‚æœæ²¡æœ‰å€¼ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
            # update_price_date å¯ä»¥ä¸ºNoneï¼ˆå› ä¸ºè¡¨ç»“æ„ä¸­æ˜¯Nullable(DateTime)ï¼‰
            datetime_fields = ["event_time", "stats_open_time", "stats_close_time"]
            for field in datetime_fields:
                if normalized.get(field) is None:
                    normalized[field] = datetime.now(timezone.utc)
            
            # ingestion_time å­—æ®µå¤„ç†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if "ingestion_time" in normalized and normalized.get("ingestion_time") is None:
                normalized["ingestion_time"] = datetime.now(timezone.utc)
            
            # å‡†å¤‡è¡Œæ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰å€¼
            row_data = []
            for name in column_names:
                value = normalized.get(name)
                # å¯¹äºéNullableçš„DateTimeå­—æ®µï¼Œç¡®ä¿ä¸ä¸ºNone
                if name in ["event_time", "stats_open_time", "stats_close_time", "ingestion_time"]:
                    if value is None:
                        value = datetime.now(timezone.utc)
                row_data.append(value)
            
            prepared_rows.append(row_data)
        
        # For ClickHouse, the most efficient way to upsert is to delete existing rows first, then insert new ones
        # This is more efficient than UPDATE for MergeTree tables
        if symbols_to_upsert:
            # Delete existing rows with the same symbols
            # ä½¿ç”¨å»é‡åçš„symbolåˆ—è¡¨ï¼Œé¿å…é‡å¤åˆ é™¤
            unique_symbols = list(set(symbols_to_upsert))
            symbols_str = "', '".join(unique_symbols)
            delete_query = f"ALTER TABLE {self.market_ticker_table} DELETE WHERE symbol IN ('{symbols_str}')"
            try:
                self.command(delete_query)
                logger.debug("[ClickHouse] Deleted existing rows for symbols: %s", unique_symbols)
            except Exception as e:
                logger.warning("[ClickHouse] Failed to delete existing rows: %s", e)
        
        # Insert new rows (æ¯ä¸ªsymbolåªæœ‰ä¸€æ¡æ•°æ®)
        self.insert_rows(self.market_ticker_table, prepared_rows, column_names)
        logger.debug(
            "[ClickHouse] Upserted %s rows into %s (ensured no duplicate symbols)",
            len(prepared_rows), self.market_ticker_table
        )
    
    def get_symbols_needing_price_refresh(self) -> List[str]:
        """è·å–éœ€è¦åˆ·æ–°ä»·æ ¼çš„symbolåˆ—è¡¨
        
        æŸ¥è¯¢æ¡ä»¶ï¼š
        - update_price_date ä¸ºç©º
        - æˆ–è€… update_price_date ä¸ä¸ºå½“å¤©
        
        Returns:
            éœ€è¦åˆ·æ–°ä»·æ ¼çš„symbolåˆ—è¡¨ï¼ˆå»é‡ï¼‰
        """
        try:
            query = f"""
            SELECT DISTINCT symbol
            FROM {self.market_ticker_table}
            WHERE symbol != ''
            AND (
                update_price_date IS NULL
                OR toDate(update_price_date) != today()
            )
            ORDER BY symbol
            """
            
            def _execute_query(client):
                return client.query(query)
            
            result = self._with_connection(_execute_query)
            symbols = [row[0] for row in result.result_rows if row[0]]
            logger.debug("[ClickHouse] Found %s symbols needing price refresh", len(symbols))
            return symbols
        except Exception as e:
            logger.error("[ClickHouse] Failed to get symbols needing price refresh: %s", e, exc_info=True)
            return []
    
    def update_open_price(self, symbol: str, open_price: float, update_date: datetime) -> bool:
        """æ›´æ–°æŒ‡å®šsymbolçš„open_priceå’Œupdate_price_date
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            open_price: å¼€ç›˜ä»·ï¼ˆæ˜¨å¤©çš„æ—¥Kçº¿æ”¶ç›˜ä»·ï¼‰
            update_date: æ›´æ–°æ—¥æœŸæ—¶é—´ï¼ˆå½“å‰åˆ·æ–°æ—¶é—´ï¼Œç”¨äºè®°å½•åˆ·æ–°æ—¶é—´æˆ³ï¼‰
            
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            # ä½¿ç”¨ALTER TABLE UPDATEæ¥æ›´æ–°æ•°æ®
            # ç”±äºClickHouseçš„UPDATEæ“ä½œæ˜¯å¼‚æ­¥çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨DELETE + INSERTçš„æ–¹å¼æ›´å¯é 
            # ä½†ä¸ºäº†æ€§èƒ½ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ALTER TABLE UPDATE
            
            # å…ˆæŸ¥è¯¢å½“å‰æ•°æ®
            query = f"""
            SELECT 
                event_time,
                symbol,
                price_change,
                price_change_percent,
                side,
                change_percent_text,
                average_price,
                last_price,
                last_trade_volume,
                high_price,
                low_price,
                base_volume,
                quote_volume,
                stats_open_time,
                stats_close_time,
                first_trade_id,
                last_trade_id,
                trade_count,
                ingestion_time
            FROM (
                SELECT 
                    *,
                    ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY event_time DESC) as rn
                FROM {self.market_ticker_table}
                WHERE symbol = '{symbol}'
            ) AS ranked
            WHERE rn = 1
            """
            
            def _execute_query(client):
                return client.query(query)
            
            result = self._with_connection(_execute_query)
            
            if not result.result_rows:
                logger.warning("[ClickHouse] Symbol %s not found for price update", symbol)
                return False
            
            # è·å–æœ€æ–°çš„ä¸€æ¡æ•°æ®
            row = result.result_rows[0]
            
            # å‡†å¤‡æ›´æ–°åçš„æ•°æ®
            column_names = [
                "event_time", "symbol", "price_change", "price_change_percent", "side",
                "change_percent_text", "average_price", "last_price", "last_trade_volume",
                "open_price", "high_price", "low_price", "base_volume", "quote_volume",
                "stats_open_time", "stats_close_time", "first_trade_id", "last_trade_id",
                "trade_count", "ingestion_time", "update_price_date"
            ]
            
            # é‡æ–°è®¡ç®—æ¶¨è·Œå¹…ç›¸å…³å­—æ®µï¼ˆåŸºäºæ–°çš„open_priceå’Œå½“å‰çš„last_priceï¼‰
            # æŸ¥è¯¢è¿”å›çš„åˆ—é¡ºåºï¼ševent_time, symbol, price_change, price_change_percent, side,
            # change_percent_text, average_price, last_price, last_trade_volume, high_price,
            # low_price, base_volume, quote_volume, stats_open_time, stats_close_time,
            # first_trade_id, last_trade_id, trade_count, ingestion_time
            last_price = float(row[7]) if row[7] is not None else 0.0  # last_priceåœ¨ç´¢å¼•7
            new_open_price = float(open_price)
            
            if new_open_price != 0 and last_price != 0:
                price_change = last_price - new_open_price
                price_change_percent = (price_change / new_open_price) * 100
                side = "gainer" if price_change_percent >= 0 else "loser"
                change_percent_text = f"{price_change_percent:.2f}%"
            else:
                # å¦‚æœæ— æ³•è®¡ç®—ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆä¸èƒ½ä¸ºNoneï¼‰
                price_change = 0.0
                price_change_percent = 0.0
                side = ""
                change_percent_text = ""
            
            # æ„å»ºæ›´æ–°åçš„è¡Œæ•°æ®
            # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰æ­£ç¡®çš„ç±»å‹å’Œé»˜è®¤å€¼
            updated_row = [
                _to_datetime(row[0]),  # event_time (DateTime)
                _normalize_field_value(row[1], "String", "symbol"),  # symbol (String)
                _normalize_field_value(price_change, "Float64", "price_change"),  # price_change (Float64)
                _normalize_field_value(price_change_percent, "Float64", "price_change_percent"),  # price_change_percent (Float64)
                _normalize_field_value(side, "String", "side"),  # side (String)
                _normalize_field_value(change_percent_text, "String", "change_percent_text"),  # change_percent_text (String)
                _normalize_field_value(row[6], "Float64", "average_price"),  # average_price (Float64)
                _normalize_field_value(row[7], "Float64", "last_price"),  # last_price (Float64)
                _normalize_field_value(row[8], "Float64", "last_trade_volume"),  # last_trade_volume (Float64)
                _normalize_field_value(new_open_price, "Float64", "open_price"),  # open_price (Float64)
                _normalize_field_value(row[9], "Float64", "high_price"),  # high_price (Float64)
                _normalize_field_value(row[10], "Float64", "low_price"),  # low_price (Float64)
                _normalize_field_value(row[11], "Float64", "base_volume"),  # base_volume (Float64)
                _normalize_field_value(row[12], "Float64", "quote_volume"),  # quote_volume (Float64)
                _to_datetime(row[13]),  # stats_open_time (DateTime)
                _to_datetime(row[14]),  # stats_close_time (DateTime)
                _normalize_field_value(row[15], "UInt64", "first_trade_id"),  # first_trade_id (UInt64)
                _normalize_field_value(row[16], "UInt64", "last_trade_id"),  # last_trade_id (UInt64)
                _normalize_field_value(row[17], "UInt64", "trade_count"),  # trade_count (UInt64)
                _to_datetime(row[18]) if row[18] else datetime.now(timezone.utc),  # ingestion_time (DateTime)
                update_date  # update_price_date (Nullable(DateTime)) - å¯ä»¥ä¸ºNone
            ]
            
            # åˆ é™¤æ—§æ•°æ®å¹¶æ’å…¥æ–°æ•°æ®ï¼ˆClickHouseçš„UPDATEæ–¹å¼ï¼‰
            delete_query = f"ALTER TABLE {self.market_ticker_table} DELETE WHERE symbol = '{symbol}'"
            try:
                self.command(delete_query)
            except Exception as e:
                logger.warning("[ClickHouse] Failed to delete old row for %s: %s", symbol, e)
            
            # æ’å…¥æ›´æ–°åçš„æ•°æ®
            self.insert_rows(self.market_ticker_table, [updated_row], column_names)
            logger.debug("[ClickHouse] Updated open_price for symbol %s: %s", symbol, new_open_price)
            return True
            
        except Exception as e:
            logger.error("[ClickHouse] Failed to update open_price for symbol %s: %s", symbol, e, exc_info=True)
            return False
    
    # ------------------------------------------------------------------
    # Leaderboard helpers
    # ------------------------------------------------------------------
    def ensure_leaderboard_table(self) -> None:
        """Create the futures leaderboard table if it does not exist."""
        ddl = f"""
        CREATE TABLE IF NOT EXISTS {self.leaderboard_table} (
            event_time DateTime,
            symbol String,
            price_change Float64,
            price_change_percent Float64,
            side String,
            change_percent_text String,
            average_price Float64,
            last_price Float64,
            last_trade_volume Float64,
            open_price Float64,
            high_price Float64,
            low_price Float64,
            base_volume Float64,
            quote_volume Float64,
            stats_open_time DateTime,
            stats_close_time DateTime,
            first_trade_id UInt64,
            last_trade_id UInt64,
            trade_count UInt64,
            ingestion_time DateTime DEFAULT now(),
            rank UInt8,
            create_datetime DateTime DEFAULT now()
        )
        ENGINE = MergeTree
        ORDER BY (side, rank, symbol, create_datetime)
        """
        self.command(ddl)
        logger.info("[ClickHouse] Ensured table %s exists", self.leaderboard_table)

        # å¦‚æœè¡¨å·²å­˜åœ¨ï¼Œç¡®ä¿æ–°å¢å­—æ®µ create_datetime å­˜åœ¨
        try:
            check_column_sql = f"""
            SELECT count()
            FROM system.columns
            WHERE database = '{app_config.CLICKHOUSE_DATABASE}'
              AND table = '{self.leaderboard_table}'
              AND name = 'create_datetime'
            """

            def _check_column(client):
                result = client.query(check_column_sql)
                return result.result_rows[0][0] > 0 if result.result_rows else False

            has_column = self._with_connection(_check_column)
            if not has_column:
                add_column_sql = f"""
                ALTER TABLE {self.leaderboard_table}
                ADD COLUMN IF NOT EXISTS create_datetime DateTime DEFAULT now()
                """
                self.command(add_column_sql)
                logger.info("[ClickHouse] Added create_datetime column to %s", self.leaderboard_table)
        except Exception as e:
            logger.warning("[ClickHouse] Failed to ensure create_datetime column on %s: %s", self.leaderboard_table, e)

    def query_recent_tickers(
        self, 
        time_window_seconds: int = 5,
        side: Optional[str] = None,
        top_n: int = 10
    ) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢æ‰€æœ‰å¸‚åœºè¡Œæƒ…æ•°æ®ï¼Œç”¨äºç”Ÿæˆæ¶¨è·Œå¹…æ¦œ
        
        æ ¸å¿ƒåŠŸèƒ½ï¼š
        - ä»24_market_tickersè¡¨æŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ˆä¸é™åˆ¶æ—¶é—´çª—å£ï¼‰
        - å¯¹æ¯ä¸ªäº¤æ˜“å¯¹å–æœ€æ–°çš„è¡Œæƒ…æ•°æ®ï¼ˆå»é‡ï¼‰
        - æ ¹æ®price_change_percentå­—æ®µåˆ¤æ–­æ¶¨è·Œï¼šæ­£ä¸ºæ¶¨ï¼Œè´Ÿä¸ºè·Œ
        - è¿”å›æ¶¨å¹…æˆ–è·Œå¹…å‰åçš„æ•°æ®
        
        æ‰§è¡Œæµç¨‹ï¼š
        1. æ ¹æ®sideå‚æ•°ç¡®å®šæŸ¥è¯¢é€»è¾‘ï¼š
           - gainerï¼šæŸ¥è¯¢price_change_percent>0çš„åˆçº¦ï¼ŒæŒ‰é™åºæ’åº
           - loserï¼šæŸ¥è¯¢price_change_percent<0çš„åˆçº¦ï¼ŒæŒ‰å‡åºæ’åº
        2. æ„å»ºæŸ¥è¯¢SQLï¼ŒåŒ…å«å»é‡é€»è¾‘ï¼ˆæŒ‰symbolåˆ†ç»„ï¼Œå–æœ€æ–°event_timeï¼‰
        3. æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›ç»“æœ
        
        Args:
            time_window_seconds: å·²åºŸå¼ƒï¼Œä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰è°ƒç”¨ï¼Œä¸å†ä½¿ç”¨
            side: ç­›é€‰æ–¹å‘ï¼Œå¯é€‰å€¼ï¼š'gainer'ï¼ˆæ¶¨å¹…æ¦œï¼‰ã€'loser'ï¼ˆè·Œå¹…æ¦œï¼‰ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
            top_n: è¿”å›å‰Nåæ•°æ®
            
        Returns:
            List[Dict[str, Any]]: è¡Œæƒ…æ•°æ®å­—å…¸åˆ—è¡¨
        """
        logger.info(f"[ClickHouse] ğŸ“Š å¼€å§‹æŸ¥è¯¢æ‰€æœ‰è¡Œæƒ…æ•°æ®...")
        logger.info(f"[ClickHouse] ğŸ“‹ æŸ¥è¯¢å‚æ•°: side={side}, top_n={top_n} (å·²ç§»é™¤æ—¶é—´çª—å£é™åˆ¶)")
        
        # æ„å»ºæŸ¥è¯¢SQLï¼šå»é‡ï¼Œå–æ¯ä¸ªsymbolæœ€æ–°çš„event_time
        # é‡è¦ï¼šåªæŸ¥è¯¢sideå­—æ®µä¸ä¸ºç©ºå­—ç¬¦ä¸²çš„æ•°æ®ï¼ˆside=''è¡¨ç¤ºä»·æ ¼å¼‚æ­¥åˆ·æ–°æœåŠ¡è¿˜æ²¡åˆ·æ–°ï¼Œæ²¡æœ‰æ¶¨è·Œæ•°æ®ï¼‰
        if side:
            if side == 'gainer':
                # æ¶¨å¹…æ¦œï¼šæŸ¥è¯¢price_change_percent>0ä¸”sideä¸ä¸ºç©ºçš„åˆçº¦ï¼ŒæŒ‰price_change_percenté™åºæ’åº
                where_clause = "price_change_percent > 0 AND side != '' AND side IS NOT NULL"
                order_by = "price_change_percent DESC"
                logger.info(f"[ClickHouse] ğŸ“ˆ æ¶¨å¹…æ¦œæŸ¥è¯¢: {where_clause}, æ’åº: {order_by}")
            else:  # loser
                # è·Œå¹…æ¦œï¼šæŸ¥è¯¢price_change_percent<0ä¸”sideä¸ä¸ºç©ºçš„åˆçº¦ï¼ŒæŒ‰price_change_percentå‡åºæ’åºï¼ˆè·Œå¹…æœ€å¤§çš„æ’åœ¨å‰é¢ï¼‰
                where_clause = "price_change_percent < 0 AND side != '' AND side IS NOT NULL"
                order_by = "price_change_percent ASC"
                logger.info(f"[ClickHouse] ğŸ“‰ è·Œå¹…æ¦œæŸ¥è¯¢: {where_clause}, æ’åº: {order_by}")
            
            query = f"""
            SELECT 
                event_time,
                symbol,
                price_change,
                price_change_percent,
                side,
                change_percent_text,
                average_price,
                last_price,
                last_trade_volume,
                open_price,
                high_price,
                low_price,
                base_volume,
                quote_volume,
                stats_open_time,
                stats_close_time,
                first_trade_id,
                last_trade_id,
                trade_count,
                ingestion_time
            FROM (
                SELECT 
                    *,
                    ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY event_time DESC) as rn
                FROM {self.market_ticker_table}
                WHERE {where_clause}
            ) AS ranked
            WHERE rn = 1
            ORDER BY {order_by}
            LIMIT {top_n}
            """
        else:
            # æŸ¥è¯¢æ‰€æœ‰ï¼Œä¸åŒºåˆ†æ¶¨è·Œï¼Œä½†åªæŸ¥è¯¢sideå­—æ®µä¸ä¸ºç©ºçš„æ•°æ®
            query = f"""
            SELECT 
                event_time,
                symbol,
                price_change,
                price_change_percent,
                CASE 
                    WHEN price_change_percent > 0 THEN 'gainer' 
                    WHEN price_change_percent < 0 THEN 'loser' 
                    ELSE 'neutral' 
                END as side,
                change_percent_text,
                average_price,
                last_price,
                last_trade_volume,
                open_price,
                high_price,
                low_price,
                base_volume,
                quote_volume,
                stats_open_time,
                stats_close_time,
                first_trade_id,
                last_trade_id,
                trade_count,
                ingestion_time
            FROM (
                SELECT 
                    *,
                    ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY event_time DESC) as rn
                FROM {self.market_ticker_table}
                WHERE side != '' AND side IS NOT NULL
            ) AS ranked
            WHERE rn = 1
            LIMIT {top_n * 2}
            """
        
        def _execute_query(client):
            return client.query(query)
        
        logger.info(f"[ClickHouse] ğŸ“ æ‰§è¡ŒæŸ¥è¯¢: {query[:100]}...")
        result = self._with_connection(_execute_query)
        logger.info(f"[ClickHouse] âœ… æŸ¥è¯¢æ‰§è¡Œå®Œæˆ")
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        columns = [
            "event_time", "symbol", "price_change", "price_change_percent", "side",
            "change_percent_text", "average_price", "last_price", "last_trade_volume",
            "open_price", "high_price", "low_price", "base_volume", "quote_volume",
            "stats_open_time", "stats_close_time", "first_trade_id", "last_trade_id",
            "trade_count", "ingestion_time"
        ]
        
        rows = []
        for row in result.result_rows:
            row_dict = dict(zip(columns, row))
            rows.append(row_dict)
        
        # è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥å‰3æ¡æ•°æ®çš„volumeå­—æ®µ
        if rows and len(rows) > 0:
            for i, row_dict in enumerate(rows[:3]):
                logger.debug(
                    f"[ClickHouse] [æŸ¥è¯¢ç»“æœ #{i+1}] Symbol: {row_dict.get('symbol')}, "
                    f"base_volume: {row_dict.get('base_volume')}, quote_volume: {row_dict.get('quote_volume')}"
                )
        
        logger.info(f"[ClickHouse] ğŸ“Š æŸ¥è¯¢ç»“æœ: å…± {len(rows)} æ¡æ•°æ®")
        return rows

    def sync_leaderboard(
        self,
        time_window_seconds: int = 5,
        top_n: int = 10
    ) -> None:
        """Sync leaderboard data from market_ticker_table to leaderboard_table.
        
        æ ¸å¿ƒåŠŸèƒ½ï¼š
        - ä»24_market_tickersè¡¨æŸ¥è¯¢æ‰€æœ‰å¸‚åœºæ•°æ®ï¼ˆä¸é™åˆ¶æ—¶é—´çª—å£ï¼‰
        - å¯¹æ¯ä¸ªäº¤æ˜“å¯¹å–æœ€æ–°çš„è¡Œæƒ…æ•°æ®ï¼ˆå»é‡ï¼‰
        - è®¡ç®—æ¯ä¸ªåˆçº¦çš„æ¶¨è·Œå¹…
        - ç­›é€‰å‡ºæ¶¨å¹…å‰Nåå’Œè·Œå¹…å‰Nå
        - ä½¿ç”¨å…¨é‡æ›´æ–°æ–¹å¼æ›´æ–°futures_leaderboardè¡¨ï¼ˆä¸´æ—¶è¡¨ + REPLACE TABLEï¼‰
        
        æ‰§è¡Œæµç¨‹ï¼š
        1. æ£€æŸ¥leaderboardè¡¨æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
        2. æŸ¥è¯¢æ¶¨å¹…æ¦œå‰Nåï¼ˆæŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ¶¨è·Œå¹…æ’åºï¼‰
        3. æŸ¥è¯¢è·Œå¹…æ¦œå‰Nåï¼ˆæŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ¶¨è·Œå¹…æ’åºï¼‰
        4. å‡†å¤‡æ’å…¥æ•°æ®
        5. åˆ›å»ºä¸´æ—¶è¡¨å¹¶æ’å…¥æ•°æ®
        6. ä½¿ç”¨REPLACE TABLEåŸå­æ›¿æ¢åŸè¡¨ï¼ˆå…¨é‡æ›´æ–°ï¼‰
        7. æ¸…ç†ä¸´æ—¶è¡¨
        
        Args:
            time_window_seconds: å·²åºŸå¼ƒï¼Œä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰è°ƒç”¨ï¼Œä¸å†ä½¿ç”¨æ—¶é—´çª—å£é™åˆ¶
            top_n: æ¶¨è·Œå¹…å‰Nåæ•°é‡
        """
        try:
            logger.info("[ClickHouse] ğŸš€ å¼€å§‹æ¶¨è·Œå¹…æ¦œåŒæ­¥...")
            logger.info("[ClickHouse] ğŸ“‹ åŒæ­¥å‚æ•°: top_n=%s (æŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼Œä¸é™åˆ¶æ—¶é—´çª—å£)", top_n)
            
            # é‡è¦ï¼šæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
            logger.info("[ClickHouse] ğŸ” æ£€æŸ¥leaderboardè¡¨æ˜¯å¦å­˜åœ¨...")
            table_exists = self._check_table_exists(self.leaderboard_table)
            if not table_exists:
                logger.info("[ClickHouse] ğŸ“‹ leaderboardè¡¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºè¡¨...")
                self.ensure_leaderboard_table()
                logger.info("[ClickHouse] âœ… leaderboardè¡¨åˆ›å»ºå®Œæˆ")
            else:
                logger.info("[ClickHouse] âœ… leaderboardè¡¨å·²å­˜åœ¨")
            
            # æŸ¥è¯¢æ¶¨å¹…æ¦œå‰Nåï¼ˆæŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ¶¨è·Œå¹…æ’åºï¼‰
            logger.info("[ClickHouse] ğŸ” æŸ¥è¯¢æ¶¨å¹…æ¦œå‰%såï¼ˆä»æ‰€æœ‰æ•°æ®ä¸­æ’åºï¼‰...", top_n)
            gainers = self.query_recent_tickers(
                time_window_seconds=time_window_seconds,
                side='gainer',
                top_n=top_n
            )
            logger.info("[ClickHouse] âœ… æ¶¨å¹…æ¦œæŸ¥è¯¢å®Œæˆï¼Œå…± %s æ¡æ•°æ®", len(gainers))
            
            # æŸ¥è¯¢è·Œå¹…æ¦œå‰Nåï¼ˆæŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰æ¶¨è·Œå¹…æ’åºï¼‰
            logger.info("[ClickHouse] ğŸ” æŸ¥è¯¢è·Œå¹…æ¦œå‰%såï¼ˆä»æ‰€æœ‰æ•°æ®ä¸­æ’åºï¼‰...", top_n)
            losers = self.query_recent_tickers(
                time_window_seconds=time_window_seconds,
                side='loser',
                top_n=top_n
            )
            logger.info("[ClickHouse] âœ… è·Œå¹…æ¦œæŸ¥è¯¢å®Œæˆï¼Œå…± %s æ¡æ•°æ®", len(losers))
            
            # é‡è¦ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®ï¼ˆsideå­—æ®µä¸ä¸ºç©ºï¼‰
            # å¦‚æœæ¶¨å¹…æ¦œå’Œè·Œå¹…æ¦œéƒ½æ²¡æœ‰æ•°æ®ï¼Œè¯´æ˜ä»·æ ¼å¼‚æ­¥åˆ·æ–°æœåŠ¡è¿˜æ²¡åˆ·æ–°ï¼Œæ­¤æ—¶ä¸åº”è¯¥æ‰§è¡ŒåŒæ­¥
            if not gainers and not losers:
                logger.warning("[ClickHouse] âš ï¸ æ¶¨å¹…æ¦œå’Œè·Œå¹…æ¦œéƒ½æ²¡æœ‰æ•°æ®ï¼ˆsideå­—æ®µä¸ºç©ºï¼‰ï¼Œè·³è¿‡åŒæ­¥æ“ä½œ")
                logger.warning("[ClickHouse] âš ï¸ è¿™å¯èƒ½æ˜¯å› ä¸ºä»·æ ¼å¼‚æ­¥åˆ·æ–°æœåŠ¡è¿˜æ²¡æœ‰åˆ·æ–°open_priceï¼Œå¯¼è‡´sideå­—æ®µä¸ºç©ºå­—ç¬¦ä¸²")
                return
            
            # å‡†å¤‡æ’å…¥æ•°æ®
            logger.info("[ClickHouse] ğŸ“ å‡†å¤‡æ’å…¥æ•°æ®...")
            all_rows = []
            column_names = [
                "event_time", "symbol", "price_change", "price_change_percent", "side",
                "change_percent_text", "average_price", "last_price", "last_trade_volume",
                "open_price", "high_price", "low_price", "base_volume", "quote_volume",
                "stats_open_time", "stats_close_time", "first_trade_id", "last_trade_id",
                "trade_count", "ingestion_time", "rank", "create_datetime"
            ]

            # æœ¬æ¬¡åŒæ­¥æ‰¹æ¬¡çš„å”¯ä¸€æ—¶é—´æˆ³ï¼ˆæ•´æ‰¹æ’å…¥ä½¿ç”¨ç›¸åŒçš„ create_datetimeï¼‰
            batch_time = datetime.now(timezone.utc)
            
            # æ·»åŠ æ¶¨å¹…æ¦œæ•°æ®ï¼ˆå¸¦æ’åï¼‰
            logger.info("[ClickHouse] ğŸ“Š å¤„ç†æ¶¨å¹…æ¦œæ•°æ®...")
            for idx, row in enumerate(gainers, 1):
                symbol = row.get("symbol", "")
                base_volume_raw = row.get("base_volume")
                quote_volume_raw = row.get("quote_volume")
                
                # è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥åŸå§‹æ•°æ®
                if idx <= 3:  # åªè®°å½•å‰3æ¡ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                    logger.debug(
                        f"[ClickHouse] [æ¶¨å¹…æ¦œ #{idx}] Symbol: {symbol}, "
                        f"base_volume_raw: {base_volume_raw}, quote_volume_raw: {quote_volume_raw}"
                    )
                
                row_data = [
                    _to_datetime(row.get("event_time")),  # DateTime
                    _normalize_field_value(row.get("symbol"), "String", "symbol"),  # String
                    _normalize_field_value(row.get("price_change"), "Float64", "price_change"),  # Float64
                    _normalize_field_value(row.get("price_change_percent"), "Float64", "price_change_percent"),  # Float64
                    _normalize_field_value(row.get("side", "gainer"), "String", "side"),  # String
                    _normalize_field_value(row.get("change_percent_text"), "String", "change_percent_text"),  # String
                    _normalize_field_value(row.get("average_price"), "Float64", "average_price"),  # Float64
                    _normalize_field_value(row.get("last_price"), "Float64", "last_price"),  # Float64
                    _normalize_field_value(row.get("last_trade_volume"), "Float64", "last_trade_volume"),  # Float64
                    _normalize_field_value(row.get("open_price"), "Float64", "open_price"),  # Float64
                    _normalize_field_value(row.get("high_price"), "Float64", "high_price"),  # Float64
                    _normalize_field_value(row.get("low_price"), "Float64", "low_price"),  # Float64
                    _normalize_field_value(base_volume_raw, "Float64", "base_volume"),  # Float64
                    _normalize_field_value(quote_volume_raw, "Float64", "quote_volume"),  # Float64
                    _to_datetime(row.get("stats_open_time")),  # DateTime
                    _to_datetime(row.get("stats_close_time")),  # DateTime
                    _normalize_field_value(row.get("first_trade_id"), "UInt64", "first_trade_id"),  # UInt64
                    _normalize_field_value(row.get("last_trade_id"), "UInt64", "last_trade_id"),  # UInt64
                    _normalize_field_value(row.get("trade_count"), "UInt64", "trade_count"),  # UInt64
                    _to_datetime(row.get("ingestion_time")),  # DateTime
                    _normalize_field_value(idx, "UInt8", "rank"),  # UInt8 (rank)
                    batch_time,  # create_datetimeï¼ŒåŒä¸€æ‰¹æ¬¡ä½¿ç”¨ç›¸åŒæ—¶é—´
                ]
                all_rows.append(row_data)
            logger.info("[ClickHouse] âœ… æ¶¨å¹…æ¦œæ•°æ®å¤„ç†å®Œæˆï¼Œå…± %s æ¡", len(gainers))
            
            # æ·»åŠ è·Œå¹…æ¦œæ•°æ®ï¼ˆå¸¦æ’åï¼‰
            logger.info("[ClickHouse] ğŸ“Š å¤„ç†è·Œå¹…æ¦œæ•°æ®...")
            for idx, row in enumerate(losers, 1):
                symbol = row.get("symbol", "")
                base_volume_raw = row.get("base_volume")
                quote_volume_raw = row.get("quote_volume")
                
                # è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥åŸå§‹æ•°æ®
                if idx <= 3:  # åªè®°å½•å‰3æ¡ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                    logger.debug(
                        f"[ClickHouse] [è·Œå¹…æ¦œ #{idx}] Symbol: {symbol}, "
                        f"base_volume_raw: {base_volume_raw}, quote_volume_raw: {quote_volume_raw}"
                    )
                
                row_data = [
                    _to_datetime(row.get("event_time")),  # DateTime
                    _normalize_field_value(row.get("symbol"), "String", "symbol"),  # String
                    _normalize_field_value(row.get("price_change"), "Float64", "price_change"),  # Float64
                    _normalize_field_value(row.get("price_change_percent"), "Float64", "price_change_percent"),  # Float64
                    _normalize_field_value(row.get("side", "loser"), "String", "side"),  # String
                    _normalize_field_value(row.get("change_percent_text"), "String", "change_percent_text"),  # String
                    _normalize_field_value(row.get("average_price"), "Float64", "average_price"),  # Float64
                    _normalize_field_value(row.get("last_price"), "Float64", "last_price"),  # Float64
                    _normalize_field_value(row.get("last_trade_volume"), "Float64", "last_trade_volume"),  # Float64
                    _normalize_field_value(row.get("open_price"), "Float64", "open_price"),  # Float64
                    _normalize_field_value(row.get("high_price"), "Float64", "high_price"),  # Float64
                    _normalize_field_value(row.get("low_price"), "Float64", "low_price"),  # Float64
                    _normalize_field_value(base_volume_raw, "Float64", "base_volume"),  # Float64
                    _normalize_field_value(quote_volume_raw, "Float64", "quote_volume"),  # Float64
                    _to_datetime(row.get("stats_open_time")),  # DateTime
                    _to_datetime(row.get("stats_close_time")),  # DateTime
                    _normalize_field_value(row.get("first_trade_id"), "UInt64", "first_trade_id"),  # UInt64
                    _normalize_field_value(row.get("last_trade_id"), "UInt64", "last_trade_id"),  # UInt64
                    _normalize_field_value(row.get("trade_count"), "UInt64", "trade_count"),  # UInt64
                    _to_datetime(row.get("ingestion_time")),  # DateTime
                    _normalize_field_value(idx, "UInt8", "rank"),  # UInt8 (rank)
                    batch_time,  # create_datetimeï¼ŒåŒä¸€æ‰¹æ¬¡ä½¿ç”¨ç›¸åŒæ—¶é—´
                ]
                all_rows.append(row_data)
            logger.info("[ClickHouse] âœ… è·Œå¹…æ¦œæ•°æ®å¤„ç†å®Œæˆï¼Œå…± %s æ¡", len(losers))
            
            if all_rows:
                logger.info("[ClickHouse] ğŸ’¾ å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®åˆ°ClickHouseï¼Œå…± %s æ¡...", len(all_rows))

                # ä½¿ç”¨é”é˜²æ­¢å¹¶å‘æ‰§è¡Œæ’å…¥ï¼Œé¿å…å¹¶å‘æ‰¹æ¬¡äº¤ç»‡
                with ClickHouseDatabase._sync_leaderboard_lock:
                    # ç›´æ¥ä½¿ç”¨ ClickHouse æ‰¹é‡æ’å…¥ï¼Œä¸å†ä½¿ç”¨ä¸´æ—¶è¡¨/å…¨é‡æ›¿æ¢æ–¹æ¡ˆ
                    self.insert_rows(self.leaderboard_table, all_rows, column_names)
                    logger.info(
                        "[ClickHouse] âœ… æ‰¹é‡æ’å…¥å®Œæˆï¼Œæœ¬æ¬¡æ‰¹æ¬¡æ—¶é—´æˆ³: %s, æ¶¨å¹…: %d æ¡, è·Œå¹…: %d æ¡",
                        batch_time.isoformat(),
                        len(gainers),
                        len(losers),
                    )
            else:
                logger.warning("[ClickHouse] âš ï¸  æ²¡æœ‰æ¶¨è·Œå¹…æ¦œæ•°æ®å¯åŒæ­¥")
                
        except Exception as exc:
            logger.error("[ClickHouse] âŒ æ¶¨è·Œå¹…æ¦œåŒæ­¥å¤±è´¥: %s", exc, exc_info=True)

    def get_leaderboard(self, limit: int = 10) -> Dict[str, List[Dict]]:
        """Get leaderboard data from futures_leaderboard table.
        
        Args:
            limit: Number of top items to return for each side
            
        Returns:
            Dictionary with 'gainers' and 'losers' lists
        """
        try:
            # ä¸€æ¡ SQLï¼šå…ˆé”å®šæœ€æ–°æ‰¹æ¬¡çš„ create_datetimeï¼Œå†å–è¯¥æ‰¹æ¬¡æ‰€æœ‰æ¶¨è·Œæ•°æ®
            query = f"""
            SELECT
                symbol,
                last_price,
                price_change_percent,
                side,
                change_percent_text,
                quote_volume,
                rank,
                create_datetime
            FROM {self.leaderboard_table}
            WHERE create_datetime = (
                SELECT max(create_datetime) FROM {self.leaderboard_table}
            )
              AND side IN ('gainer', 'loser')
            """

            def _execute_query(client):
                return client.query(query)

            result = self._with_connection(_execute_query)
            rows = result.result_rows

            if not rows:
                logger.warning("[ClickHouse] get_leaderboard: futures_leaderboard ä¸­æ²¡æœ‰æ•°æ®")
                return {'gainers': [], 'losers': []}

            gainers: List[Dict] = []
            losers: List[Dict] = []

            # å…ˆæŒ‰ side åˆ†ç±»ï¼Œå†åœ¨å†…å­˜ä¸­æ’åº + æˆªæ–­å‰ N
            for row in rows:
                try:
                    side = str(row[3]) if row[3] else ''
                    item = {
                        'symbol': str(row[0]) if row[0] else '',
                        'price': float(row[1]) if row[1] is not None else 0.0,
                        'change_percent': float(row[2]) if row[2] is not None else 0.0,
                        'side': side or ('gainer' if (row[2] or 0) >= 0 else 'loser'),
                        'change_percent_text': str(row[4]) if row[4] else '',
                        'quote_volume': float(row[5]) if len(row) > 5 and row[5] is not None else 0.0,
                        'rank': int(row[6]) if len(row) > 6 and row[6] is not None else 0,
                    }

                    if item['side'] == 'gainer':
                        gainers.append(item)
                    elif item['side'] == 'loser':
                        losers.append(item)
                except (TypeError, ValueError, IndexError) as e:
                    logger.warning("[ClickHouse] Failed to parse leaderboard row: %s, error: %s", row, e)
                    continue

            # åœ¨å†…å­˜ä¸­æŒ‰æ¶¨è·Œå¹…æ’åºå¹¶æˆªå–å‰ N å
            gainers.sort(key=lambda x: x.get('change_percent', 0.0), reverse=True)
            losers.sort(key=lambda x: x.get('change_percent', 0.0))  # è·Œå¹…è¶Šå°ï¼ˆæ›´è´Ÿï¼‰æ’è¶Šå‰

            gainers = gainers[: max(0, int(limit or 0))] if limit else gainers
            losers = losers[: max(0, int(limit or 0))] if limit else losers

            return {
                'gainers': gainers,
                'losers': losers,
            }
        except Exception as exc:
            logger.error("[ClickHouse] Failed to get leaderboard: %s", exc, exc_info=True)
            return {'gainers': [], 'losers': []}

    def get_leaderboard_symbols(self) -> List[str]:
        """Get distinct symbols from leaderboard table."""
        try:
            query = f"""
            SELECT symbol
            FROM {self.leaderboard_table}
            WHERE symbol != ''
            GROUP BY symbol
            """
            
            def _execute_query(client):
                return client.query(query)
            
            result = self._with_connection(_execute_query)
            symbols = [row[0] for row in result.result_rows if row[0]]
            return symbols
        except Exception as exc:
            logger.error("[ClickHouse] Failed to get leaderboard symbols: %s", exc, exc_info=True)
            return []

    def ensure_market_klines_table(self) -> None:
        """Create per-interval market_klines tables if they do not exist.

        æ‹†åˆ†åŸæ¥çš„å•è¡¨ market_klines ä¸º 7 å¼ æŒ‰ interval åˆ’åˆ†çš„è¡¨ï¼š
        - market_klines_1w, market_klines_1d, market_klines_4h, market_klines_1h,
          market_klines_15m, market_klines_5m, market_klines_1m
        """
        for interval, table_name in self.market_klines_tables.items():
            ddl = f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                event_time DateTime,
                symbol String,
                contract_type String,
                kline_start_time DateTime,
                kline_end_time DateTime,
                interval String,
                first_trade_id UInt64,
                last_trade_id UInt64,
                open_price Float64,
                close_price Float64,
                high_price Float64,
                low_price Float64,
                base_volume Float64,
                trade_count UInt64,
                is_closed UInt8,
                quote_volume Float64,
                taker_buy_base_volume Float64,
                taker_buy_quote_volume Float64,
                create_time DateTime DEFAULT now()
            )
            ENGINE = MergeTree
            ORDER BY (symbol, interval, kline_end_time, event_time)
            TTL kline_end_time + INTERVAL 2 DAY
            """
            self.command(ddl)
            logger.info("[ClickHouse] Ensured kline table %s (interval=%s) exists", table_name, interval)

    def insert_market_klines(self, rows: Iterable[Dict[str, Any]]) -> None:
        """Insert kline data into per-interval market_klines tables."""
        rows = list(rows)
        if not rows:
            return

        column_names = [
            "event_time",
            "symbol",
            "contract_type",
            "kline_start_time",
            "kline_end_time",
            "interval",
            "first_trade_id",
            "last_trade_id",
            "open_price",
            "close_price",
            "high_price",
            "low_price",
            "base_volume",
            "trade_count",
            "is_closed",
            "quote_volume",
            "taker_buy_base_volume",
            "taker_buy_quote_volume",
        ]

        # æŒ‰ interval å½’ç±»åˆ°å¯¹åº”çš„è¡¨
        bucketed: Dict[str, List[Tuple[Any, ...]]] = {}
        for row in rows:
            interval = (row.get("interval") or "").strip()
            table_name = self.market_klines_tables.get(interval)
            if not table_name:
                logger.debug(
                    "[ClickHouse] Skip kline row with unsupported interval: %s (row=%s)",
                    interval,
                    row,
                )
                continue

            values = (
                row.get("event_time"),
                row.get("symbol", ""),
                row.get("contract_type", ""),
                row.get("kline_start_time"),
                row.get("kline_end_time"),
                interval,
                row.get("first_trade_id", 0),
                row.get("last_trade_id", 0),
                row.get("open_price", 0.0),
                row.get("close_price", 0.0),
                row.get("high_price", 0.0),
                row.get("low_price", 0.0),
                row.get("base_volume", 0.0),
                row.get("trade_count", 0),
                row.get("is_closed", 0),
                row.get("quote_volume", 0.0),
                row.get("taker_buy_base_volume", 0.0),
                row.get("taker_buy_quote_volume", 0.0),
            )
            bucketed.setdefault(table_name, []).append(values)

        # æ‰¹é‡å†™å…¥å„ interval å¯¹åº”çš„è¡¨
        for table_name, payload in bucketed.items():
            if not payload:
                continue
            self.insert_rows(table_name, payload, column_names)
            logger.debug(
                "[ClickHouse] Inserted %s kline rows into %s",
                len(payload),
                table_name,
            )

    def cleanup_old_klines(self, days: int = 2) -> int:
        """Delete klines older than specified days. Returns number of deleted rows."""
        try:
            # ClickHouse DELETE requires mutations, use ALTER TABLE DELETE
            for interval, table_name in self.market_klines_tables.items():
                delete_sql = f"""
                ALTER TABLE {table_name}
                DELETE WHERE kline_end_time < now() - INTERVAL {days} DAY
                """
                self.command(delete_sql)
                logger.info(
                    "[ClickHouse] Initiated cleanup of klines older than %s days for %s (interval=%s)",
                    days,
                    table_name,
                    interval,
                )
            # Note: ClickHouse DELETE is asynchronous, we can't get exact count immediately
            return 0  # ClickHouse doesn't return count for async DELETE
        except Exception as exc:
            logger.error("[ClickHouse] Failed to cleanup old klines: %s", exc, exc_info=True)
            return 0

    def cleanup_old_leaderboard(self, minutes: int = 10) -> int:
        """Delete leaderboard rows older than specified minutes based on create_datetime.

        Args:
            minutes: ä¿ç•™æ—¶é—´çª—å£ï¼ˆåˆ†é’Ÿï¼‰ï¼Œåˆ é™¤ create_datetime æ—©äºå½“å‰æ—¶é—´è¯¥åˆ†é’Ÿæ•°ä¹‹å‰çš„æ•°æ®

        Returns:
            å·²æäº¤åˆ é™¤ä»»åŠ¡çš„è¡Œæ•°ï¼ˆClickHouse å¼‚æ­¥ DELETEï¼Œè¿”å›0è¡¨ç¤ºæœªçŸ¥ï¼‰
        """
        try:
            delete_sql = f"""
            ALTER TABLE {self.leaderboard_table}
            DELETE WHERE create_datetime < now() - INTERVAL {minutes} MINUTE
            """
            self.command(delete_sql)
            logger.info(
                "[ClickHouse] Initiated cleanup of leaderboard rows older than %s minutes",
                minutes,
            )
            # ClickHouse çš„ ALTER DELETE æ˜¯å¼‚æ­¥çš„ï¼Œè¿™é‡Œè¿”å›0è¡¨ç¤ºå·²æäº¤
            return 0
        except Exception as exc:
            logger.error("[ClickHouse] Failed to cleanup old leaderboard rows: %s", exc, exc_info=True)
            return 0


def _to_datetime(value: Any) -> datetime:
    """å°†å€¼è½¬æ¢ä¸ºdatetimeå¯¹è±¡ï¼Œå¦‚æœæ— æ³•è½¬æ¢åˆ™è¿”å›å½“å‰UTCæ—¶é—´
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°æ°¸è¿œä¸ä¼šè¿”å›Noneï¼Œç¡®ä¿DateTimeå­—æ®µå§‹ç»ˆæœ‰å€¼
    """
    if isinstance(value, datetime):
        return value
    if value is None:
        return datetime.now(timezone.utc)
    try:
        numeric = float(value)
    except (TypeError, ValueError):
        return datetime.now(timezone.utc)

    seconds = numeric / 1000.0
    return datetime.fromtimestamp(seconds, tz=timezone.utc)


def _normalize_field_value(value: Any, field_type: str, field_name: str = "") -> Any:
    """è§„èŒƒåŒ–å­—æ®µå€¼ï¼Œç¡®ä¿ç¬¦åˆClickHouseå­—æ®µç±»å‹è¦æ±‚
    
    Args:
        value: åŸå§‹å€¼
        field_type: å­—æ®µç±»å‹ ('Float64', 'UInt64', 'UInt8', 'String', 'DateTime')
        field_name: å­—æ®µåç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
    Returns:
        è§„èŒƒåŒ–åçš„å€¼ï¼Œç¡®ä¿ä¸ä¸ºNoneï¼ˆé™¤éå­—æ®µç±»å‹å…è®¸ï¼‰
    """
    if value is None:
        if field_type == 'Float64':
            return 0.0
        elif field_type == 'UInt64':
            return 0
        elif field_type == 'UInt8':
            return 0
        elif field_type == 'String':
            return ""
        elif field_type == 'DateTime':
            return datetime.now(timezone.utc)
        else:
            logger.warning(f"[ClickHouse] Unknown field type {field_type} for field {field_name}, using None")
            return None
    
    # ç±»å‹è½¬æ¢
    try:
        if field_type == 'Float64':
            return float(value)
        elif field_type == 'UInt64':
            return int(value)
        elif field_type == 'UInt8':
            return int(value)
        elif field_type == 'String':
            return str(value) if value else ""
        elif field_type == 'DateTime':
            return _to_datetime(value)
    except (TypeError, ValueError) as e:
        logger.warning(f"[ClickHouse] Failed to convert field {field_name} ({field_type}): {e}, using default")
        # è¿”å›é»˜è®¤å€¼
        if field_type == 'Float64':
            return 0.0
        elif field_type == 'UInt64':
            return 0
        elif field_type == 'UInt8':
            return 0
        elif field_type == 'String':
            return ""
        elif field_type == 'DateTime':
            return datetime.now(timezone.utc)
    
    return value


def _derive_side(percent: Any) -> str:
    try:
        value = float(percent)
    except (TypeError, ValueError):
        value = 0.0
    return "loser" if value < 0 else "gainer"


def _format_percent_text(percent: Any) -> str:
    try:
        value = float(percent)
    except (TypeError, ValueError):
        value = 0.0
    return f"{value:.2f}%"
